{
  "codewiki.cli.adapters.doc_generator.CLIDocumentationGenerator": {
    "id": "codewiki.cli.adapters.doc_generator.CLIDocumentationGenerator",
    "name": "CLIDocumentationGenerator",
    "component_type": "class",
    "file_path": "/Users/wufagang/project/aiopen/CodeWiki/codewiki/cli/adapters/doc_generator.py",
    "relative_path": "codewiki/cli/adapters/doc_generator.py",
    "depends_on": [
      "codewiki.cli.models.job.LLMConfig",
      "codewiki.cli.utils.fs.ensure_directory",
      "codewiki.src.be.cluster_modules.cluster_modules",
      "codewiki.src.be.dependency_analyzer.utils.logging_config.ColoredFormatter",
      "codewiki.cli.html_generator.HTMLGenerator",
      "codewiki.cli.models.job.DocumentationJob",
      "codewiki.cli.utils.errors.APIError",
      "codewiki.cli.utils.progress.ProgressTracker",
      "codewiki.src.be.documentation_generator.DocumentationGenerator",
      "codewiki.src.config.set_cli_context"
    ],
    "source_code": "class CLIDocumentationGenerator:\n    \"\"\"\n    CLI adapter for documentation generation with progress reporting.\n    \n    This class wraps the backend documentation generator and adds\n    CLI-specific features like progress tracking and error handling.\n    \"\"\"\n    \n    def __init__(\n        self,\n        repo_path: Path,\n        output_dir: Path,\n        config: Dict[str, Any],\n        verbose: bool = False,\n        generate_html: bool = False\n    ):\n        \"\"\"\n        Initialize the CLI documentation generator.\n        \n        Args:\n            repo_path: Repository path\n            output_dir: Output directory\n            config: LLM configuration\n            verbose: Enable verbose output\n            generate_html: Whether to generate HTML viewer\n        \"\"\"\n        self.repo_path = repo_path\n        self.output_dir = output_dir\n        self.config = config\n        self.verbose = verbose\n        self.generate_html = generate_html\n        self.progress_tracker = ProgressTracker(total_stages=5, verbose=verbose)\n        self.job = DocumentationJob()\n        \n        # Setup job metadata\n        self.job.repository_path = str(repo_path)\n        self.job.repository_name = repo_path.name\n        self.job.output_directory = str(output_dir)\n        self.job.llm_config = LLMConfig(\n            main_model=config.get('main_model', ''),\n            cluster_model=config.get('cluster_model', ''),\n            base_url=config.get('base_url', '')\n        )\n        \n        # Configure backend logging\n        self._configure_backend_logging()\n    \n    def _configure_backend_logging(self):\n        \"\"\"Configure backend logger for CLI use with colored output.\"\"\"\n        from codewiki.src.be.dependency_analyzer.utils.logging_config import ColoredFormatter\n        \n        # Get backend logger (parent of all backend modules)\n        backend_logger = logging.getLogger('codewiki.src.be')\n        \n        # Remove existing handlers to avoid duplicates\n        backend_logger.handlers.clear()\n        \n        if self.verbose:\n            # In verbose mode, show INFO and above\n            backend_logger.setLevel(logging.INFO)\n            \n            # Create console handler with formatting\n            console_handler = logging.StreamHandler(sys.stdout)\n            console_handler.setLevel(logging.INFO)\n            \n            # Use colored formatter for better readability\n            colored_formatter = ColoredFormatter()\n            console_handler.setFormatter(colored_formatter)\n            \n            # Add handler to logger\n            backend_logger.addHandler(console_handler)\n        else:\n            # In non-verbose mode, suppress backend logs (use WARNING level to hide INFO/DEBUG)\n            backend_logger.setLevel(logging.WARNING)\n            \n            # Create console handler for warnings and errors only\n            console_handler = logging.StreamHandler(sys.stderr)\n            console_handler.setLevel(logging.WARNING)\n            \n            # Use colored formatter even for warnings/errors\n            colored_formatter = ColoredFormatter()\n            console_handler.setFormatter(colored_formatter)\n            \n            backend_logger.addHandler(console_handler)\n        \n        # Prevent propagation to root logger to avoid duplicate messages\n        backend_logger.propagate = False\n    \n    def generate(self) -> DocumentationJob:\n        \"\"\"\n        Generate documentation with progress tracking.\n        \n        Returns:\n            Completed DocumentationJob\n            \n        Raises:\n            APIError: If LLM API call fails\n        \"\"\"\n        self.job.start()\n        start_time = time.time()\n        \n        try:\n            # Set CLI context for backend\n            set_cli_context(True)\n            \n            # Create backend config with CLI settings\n            backend_config = BackendConfig.from_cli(\n                repo_path=str(self.repo_path),\n                output_dir=str(self.output_dir),\n                llm_base_url=self.config.get('base_url'),\n                llm_api_key=self.config.get('api_key'),\n                main_model=self.config.get('main_model'),\n                cluster_model=self.config.get('cluster_model'),\n                fallback_model=self.config.get('fallback_model')\n            )\n            \n            # Run backend documentation generation\n            asyncio.run(self._run_backend_generation(backend_config))\n            \n            # Stage 4: HTML Generation (optional)\n            if self.generate_html:\n                self._run_html_generation()\n            \n            # Stage 5: Finalization (metadata already created by backend)\n            self._finalize_job()\n            \n            # Complete job\n            generation_time = time.time() - start_time\n            self.job.complete()\n            \n            return self.job\n            \n        except APIError as e:\n            self.job.fail(str(e))\n            raise\n        except Exception as e:\n            self.job.fail(str(e))\n            raise\n    \n    async def _run_backend_generation(self, backend_config: BackendConfig):\n        \"\"\"Run the backend documentation generation with progress tracking.\"\"\"\n        \n        # Stage 1: Dependency Analysis\n        self.progress_tracker.start_stage(1, \"Dependency Analysis\")\n        if self.verbose:\n            self.progress_tracker.update_stage(0.2, \"Initializing dependency analyzer...\")\n        \n        # Create documentation generator\n        doc_generator = DocumentationGenerator(backend_config)\n        \n        if self.verbose:\n            self.progress_tracker.update_stage(0.5, \"Parsing source files...\")\n        \n        # Build dependency graph\n        try:\n            components, leaf_nodes = doc_generator.graph_builder.build_dependency_graph()\n            self.job.statistics.total_files_analyzed = len(components)\n            self.job.statistics.leaf_nodes = len(leaf_nodes)\n            \n            if self.verbose:\n                self.progress_tracker.update_stage(1.0, f\"Found {len(leaf_nodes)} leaf nodes\")\n        except Exception as e:\n            raise APIError(f\"Dependency analysis failed: {e}\")\n        \n        self.progress_tracker.complete_stage()\n        \n        # Stage 2: Module Clustering\n        self.progress_tracker.start_stage(2, \"Module Clustering\")\n        if self.verbose:\n            self.progress_tracker.update_stage(0.5, \"Clustering modules with LLM...\")\n        \n        # Import clustering function\n        from codewiki.src.be.cluster_modules import cluster_modules\n        from codewiki.src.utils import file_manager\n        from codewiki.src.config import FIRST_MODULE_TREE_FILENAME, MODULE_TREE_FILENAME\n        \n        working_dir = str(self.output_dir.absolute())\n        file_manager.ensure_directory(working_dir)\n        first_module_tree_path = os.path.join(working_dir, FIRST_MODULE_TREE_FILENAME)\n        module_tree_path = os.path.join(working_dir, MODULE_TREE_FILENAME)\n        \n        try:\n            if os.path.exists(first_module_tree_path):\n                module_tree = file_manager.load_json(first_module_tree_path)\n            else:\n                module_tree = cluster_modules(leaf_nodes, components, backend_config)\n                file_manager.save_json(module_tree, first_module_tree_path)\n            \n            file_manager.save_json(module_tree, module_tree_path)\n            self.job.module_count = len(module_tree)\n            \n            if self.verbose:\n                self.progress_tracker.update_stage(1.0, f\"Created {len(module_tree)} modules\")\n        except Exception as e:\n            raise APIError(f\"Module clustering failed: {e}\")\n        \n        self.progress_tracker.complete_stage()\n        \n        # Stage 3: Documentation Generation\n        self.progress_tracker.start_stage(3, \"Documentation Generation\")\n        if self.verbose:\n            self.progress_tracker.update_stage(0.1, \"Generating module documentation...\")\n        \n        try:\n            # Run the actual documentation generation\n            await doc_generator.generate_module_documentation(components, leaf_nodes)\n            \n            if self.verbose:\n                self.progress_tracker.update_stage(0.9, \"Creating repository overview...\")\n            \n            # Create metadata\n            doc_generator.create_documentation_metadata(working_dir, components, len(leaf_nodes))\n            \n            # Collect generated files\n            for file_path in os.listdir(working_dir):\n                if file_path.endswith('.md') or file_path.endswith('.json'):\n                    self.job.files_generated.append(file_path)\n            \n        except Exception as e:\n            raise APIError(f\"Documentation generation failed: {e}\")\n        \n        self.progress_tracker.complete_stage()\n    \n    def _run_html_generation(self):\n        \"\"\"Run HTML generation stage.\"\"\"\n        self.progress_tracker.start_stage(4, \"HTML Generation\")\n        \n        from codewiki.cli.html_generator import HTMLGenerator\n        \n        # Generate HTML\n        html_generator = HTMLGenerator()\n        \n        if self.verbose:\n            self.progress_tracker.update_stage(0.3, \"Loading module tree and metadata...\")\n        \n        repo_info = html_generator.detect_repository_info(self.repo_path)\n        \n        # Generate HTML with auto-loading of module_tree and metadata from docs_dir\n        output_path = self.output_dir / \"index.html\"\n        html_generator.generate(\n            output_path=output_path,\n            title=repo_info['name'],\n            repository_url=repo_info['url'],\n            github_pages_url=repo_info['github_pages_url'],\n            docs_dir=self.output_dir  # Auto-load module_tree and metadata from here\n        )\n        \n        self.job.files_generated.append(\"index.html\")\n        \n        if self.verbose:\n            self.progress_tracker.update_stage(1.0, \"Generated index.html\")\n        \n        self.progress_tracker.complete_stage()\n    \n    def _finalize_job(self):\n        \"\"\"Finalize the job (metadata already created by backend).\"\"\"\n        # Just verify metadata exists\n        metadata_path = self.output_dir / \"metadata.json\"\n        if not metadata_path.exists():\n            # Create our own if backend didn't\n            with open(metadata_path, 'w') as f:\n                f.write(self.job.to_json())",
    "start_line": 26,
    "end_line": 287,
    "has_docstring": true,
    "docstring": "CLI adapter for documentation generation with progress reporting.\n\nThis class wraps the backend documentation generator and adds\nCLI-specific features like progress tracking and error handling.",
    "parameters": null,
    "node_type": "class",
    "base_classes": null,
    "class_name": null,
    "display_name": "class CLIDocumentationGenerator",
    "component_id": "codewiki.cli.adapters.doc_generator.CLIDocumentationGenerator"
  },
  "codewiki.cli.commands.config.config_group": {
    "id": "codewiki.cli.commands.config.config_group",
    "name": "config_group",
    "component_type": "function",
    "file_path": "/Users/wufagang/project/aiopen/CodeWiki/codewiki/cli/commands/config.py",
    "relative_path": "codewiki/cli/commands/config.py",
    "depends_on": [],
    "source_code": "def config_group():\n    \"\"\"Manage CodeWiki configuration (API credentials and settings).\"\"\"\n    pass",
    "start_line": 27,
    "end_line": 29,
    "has_docstring": true,
    "docstring": "Manage CodeWiki configuration (API credentials and settings).",
    "parameters": [],
    "node_type": "function",
    "base_classes": null,
    "class_name": null,
    "display_name": "function config_group",
    "component_id": "codewiki.cli.commands.config.config_group"
  },
  "codewiki.cli.commands.config.config_set": {
    "id": "codewiki.cli.commands.config.config_set",
    "name": "config_set",
    "component_type": "function",
    "file_path": "/Users/wufagang/project/aiopen/CodeWiki/codewiki/cli/commands/config.py",
    "relative_path": "codewiki/cli/commands/config.py",
    "depends_on": [
      "codewiki.cli.utils.validation.validate_api_key",
      "codewiki.cli.utils.validation.validate_url",
      "codewiki.cli.config_manager.ConfigManager",
      "codewiki.cli.utils.errors.handle_error",
      "codewiki.cli.utils.validation.validate_model_name",
      "codewiki.cli.utils.validation.is_top_tier_model"
    ],
    "source_code": "def config_set(\n    api_key: Optional[str],\n    base_url: Optional[str],\n    main_model: Optional[str],\n    cluster_model: Optional[str],\n    fallback_model: Optional[str]\n):\n    \"\"\"\n    Set configuration values for CodeWiki.\n    \n    API keys are stored securely in your system keychain:\n      • macOS: Keychain Access\n      • Windows: Credential Manager  \n      • Linux: Secret Service (GNOME Keyring, KWallet)\n    \n    Examples:\n    \n    \\b\n    # Set all configuration\n    $ codewiki config set --api-key sk-abc123 --base-url https://api.anthropic.com \\\\\n        --main-model claude-sonnet-4 --cluster-model claude-sonnet-4 --fallback-model glm-4p5\n    \n    \\b\n    # Update only API key\n    $ codewiki config set --api-key sk-new-key\n    \"\"\"\n    try:\n        # Check if at least one option is provided\n        if not any([api_key, base_url, main_model, cluster_model, fallback_model]):\n            click.echo(\"No options provided. Use --help for usage information.\")\n            sys.exit(EXIT_CONFIG_ERROR)\n        \n        # Validate inputs before saving\n        validated_data = {}\n        \n        if api_key:\n            validated_data['api_key'] = validate_api_key(api_key)\n        \n        if base_url:\n            validated_data['base_url'] = validate_url(base_url)\n        \n        if main_model:\n            validated_data['main_model'] = validate_model_name(main_model)\n        \n        if cluster_model:\n            validated_data['cluster_model'] = validate_model_name(cluster_model)\n        \n        if fallback_model:\n            validated_data['fallback_model'] = validate_model_name(fallback_model)\n        \n        # Create config manager and save\n        manager = ConfigManager()\n        manager.load()  # Load existing config if present\n        \n        manager.save(\n            api_key=validated_data.get('api_key'),\n            base_url=validated_data.get('base_url'),\n            main_model=validated_data.get('main_model'),\n            cluster_model=validated_data.get('cluster_model'),\n            fallback_model=validated_data.get('fallback_model')\n        )\n        \n        # Display success messages\n        click.echo()\n        if api_key:\n            if manager.keyring_available:\n                click.secho(\"✓ API key saved to system keychain\", fg=\"green\")\n            else:\n                click.secho(\n                    \"⚠️  System keychain unavailable. API key stored in encrypted file.\",\n                    fg=\"yellow\"\n                )\n        \n        if base_url:\n            click.secho(f\"✓ Base URL: {base_url}\", fg=\"green\")\n        \n        if main_model:\n            click.secho(f\"✓ Main model: {main_model}\", fg=\"green\")\n        \n        if cluster_model:\n            click.secho(f\"✓ Cluster model: {cluster_model}\", fg=\"green\")\n            \n            # Warn if not using top-tier model for clustering\n            if not is_top_tier_model(cluster_model):\n                click.secho(\n                    \"\\n⚠️  Cluster model is not a top-tier LLM. \"\n                    \"Documentation quality may be suboptimal.\",\n                    fg=\"yellow\"\n                )\n                click.echo(\n                    \"   Recommended models: claude-opus, claude-sonnet-4, gpt-4, gpt-4-turbo\"\n                )\n        \n        if fallback_model:\n            click.secho(f\"✓ Fallback model: {fallback_model}\", fg=\"green\")\n        \n        click.echo(\"\\n\" + click.style(\"Configuration updated successfully.\", fg=\"green\", bold=True))\n        \n    except ConfigurationError as e:\n        click.secho(f\"\\n✗ Configuration error: {e.message}\", fg=\"red\", err=True)\n        sys.exit(e.exit_code)\n    except Exception as e:\n        sys.exit(handle_error(e))",
    "start_line": 58,
    "end_line": 160,
    "has_docstring": true,
    "docstring": "Set configuration values for CodeWiki.\n\nAPI keys are stored securely in your system keychain:\n  • macOS: Keychain Access\n  • Windows: Credential Manager  \n  • Linux: Secret Service (GNOME Keyring, KWallet)\n\nExamples:\n\n\b\n# Set all configuration\n$ codewiki config set --api-key sk-abc123 --base-url https://api.anthropic.com \\\n    --main-model claude-sonnet-4 --cluster-model claude-sonnet-4 --fallback-model glm-4p5\n\n\b\n# Update only API key\n$ codewiki config set --api-key sk-new-key",
    "parameters": [
      "api_key",
      "base_url",
      "main_model",
      "cluster_model",
      "fallback_model"
    ],
    "node_type": "function",
    "base_classes": null,
    "class_name": null,
    "display_name": "function config_set",
    "component_id": "codewiki.cli.commands.config.config_set"
  },
  "codewiki.cli.commands.config.config_show": {
    "id": "codewiki.cli.commands.config.config_show",
    "name": "config_show",
    "component_type": "function",
    "file_path": "/Users/wufagang/project/aiopen/CodeWiki/codewiki/cli/commands/config.py",
    "relative_path": "codewiki/cli/commands/config.py",
    "depends_on": [
      "codewiki.cli.config_manager.ConfigManager",
      "codewiki.cli.utils.validation.mask_api_key",
      "codewiki.cli.utils.errors.handle_error"
    ],
    "source_code": "def config_show(output_json: bool):\n    \"\"\"\n    Display current configuration.\n    \n    API keys are masked for security (showing only first and last 4 characters).\n    \n    Examples:\n    \n    \\b\n    # Display configuration\n    $ codewiki config show\n    \n    \\b\n    # Display as JSON\n    $ codewiki config show --json\n    \"\"\"\n    try:\n        manager = ConfigManager()\n        \n        if not manager.load():\n            click.secho(\"\\n✗ Configuration not found.\", fg=\"red\", err=True)\n            click.echo(\"\\nPlease run 'codewiki config set' to configure your API credentials:\")\n            click.echo(\"  codewiki config set --api-key <key> --base-url <url> \\\\\")\n            click.echo(\"    --main-model <model> --cluster-model <model> --fallback-model <model>\")\n            click.echo(\"\\nFor more help: codewiki config set --help\")\n            sys.exit(EXIT_CONFIG_ERROR)\n        \n        config = manager.get_config()\n        api_key = manager.get_api_key()\n        \n        if output_json:\n            # JSON output\n            output = {\n                \"api_key\": mask_api_key(api_key) if api_key else \"Not set\",\n                \"api_key_storage\": \"keychain\" if manager.keyring_available else \"encrypted_file\",\n                \"base_url\": config.base_url if config else \"\",\n                \"main_model\": config.main_model if config else \"\",\n                \"cluster_model\": config.cluster_model if config else \"\",\n                \"fallback_model\": config.fallback_model if config else \"glm-4p5\",\n                \"default_output\": config.default_output if config else \"docs\",\n                \"config_file\": str(manager.config_file_path)\n            }\n            click.echo(json.dumps(output, indent=2))\n        else:\n            # Human-readable output\n            click.echo()\n            click.secho(\"CodeWiki Configuration\", fg=\"blue\", bold=True)\n            click.echo(\"━\" * 40)\n            click.echo()\n            \n            click.secho(\"Credentials\", fg=\"cyan\", bold=True)\n            if api_key:\n                storage = \"system keychain\" if manager.keyring_available else \"encrypted file\"\n                click.echo(f\"  API Key:          {mask_api_key(api_key)} (in {storage})\")\n            else:\n                click.secho(\"  API Key:          Not set\", fg=\"yellow\")\n            \n            click.echo()\n            click.secho(\"API Settings\", fg=\"cyan\", bold=True)\n            if config:\n                click.echo(f\"  Base URL:         {config.base_url or 'Not set'}\")\n                click.echo(f\"  Main Model:       {config.main_model or 'Not set'}\")\n                click.echo(f\"  Cluster Model:    {config.cluster_model or 'Not set'}\")\n                click.echo(f\"  Fallback Model:   {config.fallback_model or 'Not set'}\")\n            else:\n                click.secho(\"  Not configured\", fg=\"yellow\")\n            \n            click.echo()\n            click.secho(\"Output Settings\", fg=\"cyan\", bold=True)\n            if config:\n                click.echo(f\"  Default Output:   {config.default_output}\")\n            \n            click.echo()\n            click.echo(f\"Configuration file: {manager.config_file_path}\")\n            click.echo()\n        \n    except Exception as e:\n        sys.exit(handle_error(e))",
    "start_line": 170,
    "end_line": 247,
    "has_docstring": true,
    "docstring": "Display current configuration.\n\nAPI keys are masked for security (showing only first and last 4 characters).\n\nExamples:\n\n\b\n# Display configuration\n$ codewiki config show\n\n\b\n# Display as JSON\n$ codewiki config show --json",
    "parameters": [
      "output_json"
    ],
    "node_type": "function",
    "base_classes": null,
    "class_name": null,
    "display_name": "function config_show",
    "component_id": "codewiki.cli.commands.config.config_show"
  },
  "codewiki.cli.commands.config.config_validate": {
    "id": "codewiki.cli.commands.config.config_validate",
    "name": "config_validate",
    "component_type": "function",
    "file_path": "/Users/wufagang/project/aiopen/CodeWiki/codewiki/cli/commands/config.py",
    "relative_path": "codewiki/cli/commands/config.py",
    "depends_on": [
      "codewiki.cli.config_manager.ConfigManager",
      "codewiki.cli.utils.errors.handle_error",
      "codewiki.cli.utils.validation.is_top_tier_model",
      "codewiki.cli.utils.validation.validate_url"
    ],
    "source_code": "def config_validate(quick: bool, verbose: bool):\n    \"\"\"\n    Validate configuration and test LLM API connectivity.\n    \n    Checks:\n      • Configuration file exists and is valid\n      • API key is present\n      • API settings are correctly formatted\n      • (Optional) API connectivity test\n    \n    Examples:\n    \n    \\b\n    # Full validation with API test\n    $ codewiki config validate\n    \n    \\b\n    # Quick validation (config only)\n    $ codewiki config validate --quick\n    \n    \\b\n    # Verbose output\n    $ codewiki config validate --verbose\n    \"\"\"\n    try:\n        click.echo()\n        click.secho(\"Validating configuration...\", fg=\"blue\", bold=True)\n        click.echo()\n        \n        manager = ConfigManager()\n        \n        # Step 1: Check config file\n        if verbose:\n            click.echo(\"[1/5] Checking configuration file...\")\n            click.echo(f\"      Path: {manager.config_file_path}\")\n        \n        if not manager.load():\n            click.secho(\"✗ Configuration file not found\", fg=\"red\")\n            click.echo()\n            click.echo(\"Error: Configuration is incomplete. Run 'codewiki config set --help' for setup instructions.\")\n            sys.exit(EXIT_CONFIG_ERROR)\n        \n        if verbose:\n            click.secho(\"      ✓ File exists\", fg=\"green\")\n            click.secho(\"      ✓ Valid JSON format\", fg=\"green\")\n        else:\n            click.secho(\"✓ Configuration file exists\", fg=\"green\")\n        \n        # Step 2: Check API key\n        if verbose:\n            click.echo()\n            click.echo(\"[2/5] Checking API key...\")\n            storage = \"system keychain\" if manager.keyring_available else \"encrypted file\"\n            click.echo(f\"      Storage: {storage}\")\n        \n        api_key = manager.get_api_key()\n        if not api_key:\n            click.secho(\"✗ API key missing\", fg=\"red\")\n            click.echo()\n            click.echo(\"Error: API key not set. Run 'codewiki config set --api-key <key>'\")\n            sys.exit(EXIT_CONFIG_ERROR)\n        \n        if verbose:\n            click.secho(f\"      ✓ API key retrieved\", fg=\"green\")\n            click.secho(f\"      ✓ Length: {len(api_key)} characters\", fg=\"green\")\n        else:\n            click.secho(\"✓ API key present (stored in keychain)\", fg=\"green\")\n        \n        # Step 3: Check base URL\n        config = manager.get_config()\n        if verbose:\n            click.echo()\n            click.echo(\"[3/5] Checking base URL...\")\n            click.echo(f\"      URL: {config.base_url}\")\n        \n        if not config.base_url:\n            click.secho(\"✗ Base URL not set\", fg=\"red\")\n            sys.exit(EXIT_CONFIG_ERROR)\n        \n        try:\n            validate_url(config.base_url)\n            if verbose:\n                click.secho(\"      ✓ Valid HTTPS URL\", fg=\"green\")\n            else:\n                click.secho(f\"✓ Base URL valid: {config.base_url}\", fg=\"green\")\n        except ConfigurationError as e:\n            click.secho(f\"✗ Invalid base URL: {e.message}\", fg=\"red\")\n            sys.exit(EXIT_CONFIG_ERROR)\n        \n        # Step 4: Check models\n        if verbose:\n            click.echo()\n            click.echo(\"[4/5] Checking model configuration...\")\n            click.echo(f\"      Main model: {config.main_model}\")\n            click.echo(f\"      Cluster model: {config.cluster_model}\")\n            click.echo(f\"      Fallback model: {config.fallback_model}\")\n        \n        if not config.main_model or not config.cluster_model or not config.fallback_model:\n            click.secho(\"✗ Models not configured\", fg=\"red\")\n            sys.exit(EXIT_CONFIG_ERROR)\n        \n        if verbose:\n            click.secho(\"      ✓ Models configured\", fg=\"green\")\n        else:\n            click.secho(f\"✓ Main model configured: {config.main_model}\", fg=\"green\")\n            click.secho(f\"✓ Cluster model configured: {config.cluster_model}\", fg=\"green\")\n            click.secho(f\"✓ Fallback model configured: {config.fallback_model}\", fg=\"green\")\n        \n        # Warn about non-top-tier cluster model\n        if not is_top_tier_model(config.cluster_model):\n            click.secho(\n                \"⚠️  Cluster model is not top-tier. Consider using claude-sonnet-4 or gpt-4.\",\n                fg=\"yellow\"\n            )\n        \n        # Step 5: API connectivity test (unless --quick)\n        if not quick:\n            try:\n                from openai import OpenAI\n                client = OpenAI(api_key=api_key, base_url=config.base_url)\n                response = client.models.list()\n                click.secho(\"✓ API connectivity test successful\", fg=\"green\")\n            except Exception as e:\n                click.secho(\"✗ API connectivity test failed\", fg=\"red\")\n                sys.exit(EXIT_CONFIG_ERROR)\n        \n        # Success\n        click.echo()\n        click.secho(\"✓ Configuration is valid!\", fg=\"green\", bold=True)\n        click.echo()\n        \n    except ConfigurationError as e:\n        click.secho(f\"\\n✗ Configuration error: {e.message}\", fg=\"red\", err=True)\n        sys.exit(e.exit_code)\n    except Exception as e:\n        sys.exit(handle_error(e, verbose=verbose))",
    "start_line": 262,
    "end_line": 397,
    "has_docstring": true,
    "docstring": "Validate configuration and test LLM API connectivity.\n\nChecks:\n  • Configuration file exists and is valid\n  • API key is present\n  • API settings are correctly formatted\n  • (Optional) API connectivity test\n\nExamples:\n\n\b\n# Full validation with API test\n$ codewiki config validate\n\n\b\n# Quick validation (config only)\n$ codewiki config validate --quick\n\n\b\n# Verbose output\n$ codewiki config validate --verbose",
    "parameters": [
      "quick",
      "verbose"
    ],
    "node_type": "function",
    "base_classes": null,
    "class_name": null,
    "display_name": "function config_validate",
    "component_id": "codewiki.cli.commands.config.config_validate"
  },
  "codewiki.cli.commands.generate.generate_command": {
    "id": "codewiki.cli.commands.generate.generate_command",
    "name": "generate_command",
    "component_type": "function",
    "file_path": "/Users/wufagang/project/aiopen/CodeWiki/codewiki/cli/commands/generate.py",
    "relative_path": "codewiki/cli/commands/generate.py",
    "depends_on": [
      "codewiki.cli.utils.errors.RepositoryError",
      "codewiki.cli.utils.errors.warning",
      "codewiki.cli.utils.repo_validator.is_git_repository",
      "codewiki.cli.utils.repo_validator.get_git_commit_hash",
      "codewiki.cli.utils.repo_validator.validate_repository",
      "codewiki.cli.config_manager.ConfigManager",
      "codewiki.cli.utils.errors.ConfigurationError",
      "codewiki.cli.utils.errors.handle_error",
      "codewiki.cli.utils.errors.info",
      "codewiki.cli.adapters.doc_generator.CLIDocumentationGenerator",
      "codewiki.cli.utils.repo_validator.check_writable_output",
      "codewiki.cli.utils.repo_validator.get_git_branch",
      "codewiki.cli.models.job.GenerationOptions",
      "codewiki.cli.utils.logging.create_logger",
      "codewiki.cli.utils.instructions.display_post_generation_instructions",
      "codewiki.cli.git_manager.GitManager",
      "codewiki.cli.utils.errors.success"
    ],
    "source_code": "def generate_command(\n    ctx,\n    output: str,\n    create_branch: bool,\n    github_pages: bool,\n    no_cache: bool,\n    verbose: bool\n):\n    \"\"\"\n    Generate comprehensive documentation for a code repository.\n    \n    Analyzes the current repository and generates documentation using LLM-powered\n    analysis. Documentation is output to ./docs/ by default.\n    \n    Examples:\n    \n    \\b\n    # Basic generation\n    $ codewiki generate\n    \n    \\b\n    # With git branch creation and GitHub Pages\n    $ codewiki generate --create-branch --github-pages\n    \n    \\b\n    # Force full regeneration\n    $ codewiki generate --no-cache\n    \"\"\"\n    logger = create_logger(verbose=verbose)\n    start_time = time.time()\n    \n    # Suppress httpx INFO logs\n    logging.getLogger(\"httpx\").setLevel(logging.WARNING)\n    \n    try:\n        # Pre-generation checks\n        logger.step(\"Validating configuration...\", 1, 4)\n        \n        # Load configuration\n        config_manager = ConfigManager()\n        if not config_manager.load():\n            raise ConfigurationError(\n                \"Configuration not found or invalid.\\n\\n\"\n                \"Please run 'codewiki config set' to configure your LLM API credentials:\\n\"\n                \"  codewiki config set --api-key <your-api-key> --base-url <api-url> \\\\\\n\"\n                \"    --main-model <model> --cluster-model <model>\\n\\n\"\n                \"For more help: codewiki config --help\"\n            )\n        \n        if not config_manager.is_configured():\n            raise ConfigurationError(\n                \"Configuration is incomplete. Please run 'codewiki config validate'\"\n            )\n        \n        config = config_manager.get_config()\n        api_key = config_manager.get_api_key()\n        \n        logger.success(\"Configuration valid\")\n        \n        # Validate repository\n        logger.step(\"Validating repository...\", 2, 4)\n        \n        repo_path = Path.cwd()\n        repo_path, languages = validate_repository(repo_path)\n        \n        logger.success(f\"Repository valid: {repo_path.name}\")\n        if verbose:\n            logger.debug(f\"Detected languages: {', '.join(f'{lang} ({count} files)' for lang, count in languages)}\")\n        \n        # Check git repository\n        if not is_git_repository(repo_path):\n            if create_branch:\n                raise RepositoryError(\n                    \"Not a git repository.\\n\\n\"\n                    \"The --create-branch flag requires a git repository.\\n\\n\"\n                    \"To initialize a git repository: git init\"\n                )\n            else:\n                logger.warning(\"Not a git repository. Git features unavailable.\")\n        \n        # Validate output directory\n        output_dir = Path(output).expanduser().resolve()\n        check_writable_output(output_dir.parent)\n        \n        logger.success(f\"Output directory: {output_dir}\")\n        \n        # Check for existing documentation\n        if output_dir.exists() and list(output_dir.glob(\"*.md\")):\n            if not click.confirm(\n                f\"\\n{output_dir} already contains documentation. Overwrite?\",\n                default=True\n            ):\n                logger.info(\"Generation cancelled by user.\")\n                sys.exit(EXIT_SUCCESS)\n        \n        # Git branch creation (if requested)\n        branch_name = None\n        if create_branch:\n            logger.step(\"Creating git branch...\", 3, 4)\n            \n            from codewiki.cli.git_manager import GitManager\n            \n            git_manager = GitManager(repo_path)\n            \n            # Check clean working directory\n            is_clean, status_msg = git_manager.check_clean_working_directory()\n            if not is_clean:\n                raise RepositoryError(\n                    \"Working directory has uncommitted changes.\\n\\n\"\n                    f\"{status_msg}\\n\\n\"\n                    \"Cannot create documentation branch with uncommitted changes.\\n\"\n                    \"Please commit or stash your changes first:\\n\"\n                    \"  git add -A && git commit -m \\\"Your message\\\"\\n\"\n                    \"  # or\\n\"\n                    \"  git stash\"\n                )\n            \n            # Create branch\n            branch_name = git_manager.create_documentation_branch()\n            logger.success(f\"Created branch: {branch_name}\")\n        \n        # Generate documentation\n        logger.step(\"Generating documentation...\", 4, 4)\n        click.echo()\n        \n        # Create generation options\n        generation_options = GenerationOptions(\n            create_branch=create_branch,\n            github_pages=github_pages,\n            no_cache=no_cache,\n            custom_output=output if output != \"docs\" else None\n        )\n        \n        # Create generator\n        generator = CLIDocumentationGenerator(\n            repo_path=repo_path,\n            output_dir=output_dir,\n            config={\n                'main_model': config.main_model,\n                'cluster_model': config.cluster_model,\n                'fallback_model': config.fallback_model,\n                'base_url': config.base_url,\n                'api_key': api_key,\n            },\n            verbose=verbose,\n            generate_html=github_pages\n        )\n        \n        # Run generation\n        job = generator.generate()\n        \n        # Post-generation\n        generation_time = time.time() - start_time\n        \n        # Get repository info\n        repo_url = None\n        commit_hash = get_git_commit_hash(repo_path)\n        current_branch = get_git_branch(repo_path)\n        \n        if is_git_repository(repo_path):\n            try:\n                import git\n                repo = git.Repo(repo_path)\n                if repo.remotes:\n                    repo_url = repo.remotes.origin.url\n            except:\n                pass\n        \n        # Display instructions\n        display_post_generation_instructions(\n            output_dir=output_dir,\n            repo_name=repo_path.name,\n            repo_url=repo_url,\n            branch_name=branch_name,\n            github_pages=github_pages,\n            files_generated=job.files_generated,\n            statistics={\n                'module_count': job.module_count,\n                'total_files_analyzed': job.statistics.total_files_analyzed,\n                'generation_time': generation_time,\n                'total_tokens_used': job.statistics.total_tokens_used,\n            }\n        )\n        \n    except ConfigurationError as e:\n        logger.error(e.message)\n        logger.error(f\"Traceback: {traceback.format_exc()}\")\n        sys.exit(e.exit_code)\n    except RepositoryError as e:\n        logger.error(e.message)\n        logger.error(f\"Traceback: {traceback.format_exc()}\")\n        sys.exit(e.exit_code)\n    except APIError as e:\n        logger.error(e.message)\n        logger.error(f\"Traceback: {traceback.format_exc()}\")\n        sys.exit(e.exit_code)\n    except KeyboardInterrupt:\n        click.echo(\"\\n\\nInterrupted by user\")\n        sys.exit(130)\n    except Exception as e:\n        sys.exit(handle_error(e, verbose=verbose))",
    "start_line": 64,
    "end_line": 264,
    "has_docstring": true,
    "docstring": "Generate comprehensive documentation for a code repository.\n\nAnalyzes the current repository and generates documentation using LLM-powered\nanalysis. Documentation is output to ./docs/ by default.\n\nExamples:\n\n\b\n# Basic generation\n$ codewiki generate\n\n\b\n# With git branch creation and GitHub Pages\n$ codewiki generate --create-branch --github-pages\n\n\b\n# Force full regeneration\n$ codewiki generate --no-cache",
    "parameters": [
      "ctx",
      "output",
      "create_branch",
      "github_pages",
      "no_cache",
      "verbose"
    ],
    "node_type": "function",
    "base_classes": null,
    "class_name": null,
    "display_name": "function generate_command",
    "component_id": "codewiki.cli.commands.generate.generate_command"
  },
  "codewiki.cli.config_manager.ConfigManager": {
    "id": "codewiki.cli.config_manager.ConfigManager",
    "name": "ConfigManager",
    "component_type": "class",
    "file_path": "/Users/wufagang/project/aiopen/CodeWiki/codewiki/cli/config_manager.py",
    "relative_path": "codewiki/cli/config_manager.py",
    "depends_on": [
      "codewiki.cli.utils.fs.safe_read",
      "codewiki.cli.utils.fs.ensure_directory",
      "codewiki.cli.models.config.Configuration",
      "codewiki.cli.utils.fs.safe_write",
      "codewiki.cli.utils.errors.ConfigurationError"
    ],
    "source_code": "class ConfigManager:\n    \"\"\"\n    Manages CodeWiki configuration with secure keyring storage for API keys.\n    \n    Storage:\n        - API key: System keychain via keyring (macOS Keychain, Windows Credential Manager, \n                  Linux Secret Service)\n        - Other settings: ~/.codewiki/config.json\n    \"\"\"\n    \n    def __init__(self):\n        \"\"\"Initialize the configuration manager.\"\"\"\n        self._api_key: Optional[str] = None\n        self._config: Optional[Configuration] = None\n        self._keyring_available = self._check_keyring_available()\n    \n    def _check_keyring_available(self) -> bool:\n        \"\"\"Check if system keyring is available.\"\"\"\n        try:\n            # Try to get/set a test value\n            keyring.get_password(KEYRING_SERVICE, \"__test__\")\n            return True\n        except KeyringError:\n            return False\n    \n    def load(self) -> bool:\n        \"\"\"\n        Load configuration from file and keyring.\n        \n        Returns:\n            True if configuration exists, False otherwise\n        \"\"\"\n        # Load from JSON file\n        if not CONFIG_FILE.exists():\n            return False\n        \n        try:\n            content = safe_read(CONFIG_FILE)\n            data = json.loads(content)\n            \n            # Validate version\n            if data.get('version') != CONFIG_VERSION:\n                # Could implement migration here\n                pass\n            \n            self._config = Configuration.from_dict(data)\n            \n            # Load API key from keyring\n            try:\n                self._api_key = keyring.get_password(KEYRING_SERVICE, KEYRING_API_KEY_ACCOUNT)\n            except KeyringError:\n                # Keyring unavailable, API key will be None\n                pass\n            \n            return True\n        except (json.JSONDecodeError, FileSystemError) as e:\n            raise ConfigurationError(f\"Failed to load configuration: {e}\")\n    \n    def save(\n        self, \n        api_key: Optional[str] = None,\n        base_url: Optional[str] = None,\n        main_model: Optional[str] = None,\n        cluster_model: Optional[str] = None,\n        fallback_model: Optional[str] = None,\n        default_output: Optional[str] = None\n    ):\n        \"\"\"\n        Save configuration to file and keyring.\n        \n        Args:\n            api_key: API key (stored in keyring)\n            base_url: LLM API base URL\n            main_model: Primary model\n            cluster_model: Clustering model\n            fallback_model: Fallback model\n            default_output: Default output directory\n        \"\"\"\n        # Ensure config directory exists\n        try:\n            ensure_directory(CONFIG_DIR)\n        except FileSystemError as e:\n            raise ConfigurationError(f\"Cannot create config directory: {e}\")\n        \n        # Load existing config or create new\n        if self._config is None:\n            if CONFIG_FILE.exists():\n                self.load()\n            else:\n                self._config = Configuration(\n                    base_url=\"\",\n                    main_model=\"\",\n                    cluster_model=\"\",\n                    fallback_model=\"glm-4p5\",\n                    default_output=\"docs\"\n                )\n        \n        # Update fields if provided\n        if base_url is not None:\n            self._config.base_url = base_url\n        if main_model is not None:\n            self._config.main_model = main_model\n        if cluster_model is not None:\n            self._config.cluster_model = cluster_model\n        if fallback_model is not None:\n            self._config.fallback_model = fallback_model\n        if default_output is not None:\n            self._config.default_output = default_output\n        \n        # Validate configuration\n        self._config.validate()\n        \n        # Save API key to keyring\n        if api_key is not None:\n            self._api_key = api_key\n            try:\n                keyring.set_password(KEYRING_SERVICE, KEYRING_API_KEY_ACCOUNT, api_key)\n            except KeyringError as e:\n                # Fallback: warn about keyring unavailability\n                raise ConfigurationError(\n                    f\"System keychain unavailable: {e}\\n\"\n                    f\"Please ensure your system keychain is properly configured.\"\n                )\n        \n        # Save non-sensitive config to JSON\n        config_data = {\n            \"version\": CONFIG_VERSION,\n            **self._config.to_dict()\n        }\n        \n        try:\n            safe_write(CONFIG_FILE, json.dumps(config_data, indent=2))\n        except FileSystemError as e:\n            raise ConfigurationError(f\"Failed to save configuration: {e}\")\n    \n    def get_api_key(self) -> Optional[str]:\n        \"\"\"\n        Get API key from keyring.\n        \n        Returns:\n            API key or None if not set\n        \"\"\"\n        if self._api_key is None:\n            try:\n                self._api_key = keyring.get_password(KEYRING_SERVICE, KEYRING_API_KEY_ACCOUNT)\n            except KeyringError:\n                pass\n        \n        return self._api_key\n    \n    def get_config(self) -> Optional[Configuration]:\n        \"\"\"\n        Get current configuration.\n        \n        Returns:\n            Configuration object or None if not loaded\n        \"\"\"\n        return self._config\n    \n    def is_configured(self) -> bool:\n        \"\"\"\n        Check if configuration is complete and valid.\n        \n        Returns:\n            True if configured, False otherwise\n        \"\"\"\n        if self._config is None:\n            return False\n        \n        # Check if API key is set\n        if self.get_api_key() is None:\n            return False\n        \n        # Check if config is complete\n        return self._config.is_complete()\n    \n    def delete_api_key(self):\n        \"\"\"Delete API key from keyring.\"\"\"\n        try:\n            keyring.delete_password(KEYRING_SERVICE, KEYRING_API_KEY_ACCOUNT)\n            self._api_key = None\n        except KeyringError:\n            pass\n    \n    def clear(self):\n        \"\"\"Clear all configuration (file and keyring).\"\"\"\n        # Delete API key from keyring\n        self.delete_api_key()\n        \n        # Delete config file\n        if CONFIG_FILE.exists():\n            CONFIG_FILE.unlink()\n        \n        self._config = None\n        self._api_key = None\n    \n    @property\n    def keyring_available(self) -> bool:\n        \"\"\"Check if keyring is available.\"\"\"\n        return self._keyring_available\n    \n    @property\n    def config_file_path(self) -> Path:\n        \"\"\"Get configuration file path.\"\"\"\n        return CONFIG_FILE",
    "start_line": 26,
    "end_line": 230,
    "has_docstring": true,
    "docstring": "Manages CodeWiki configuration with secure keyring storage for API keys.\n\nStorage:\n    - API key: System keychain via keyring (macOS Keychain, Windows Credential Manager, \n              Linux Secret Service)\n    - Other settings: ~/.codewiki/config.json",
    "parameters": null,
    "node_type": "class",
    "base_classes": null,
    "class_name": null,
    "display_name": "class ConfigManager",
    "component_id": "codewiki.cli.config_manager.ConfigManager"
  },
  "codewiki.cli.git_manager.GitManager": {
    "id": "codewiki.cli.git_manager.GitManager",
    "name": "GitManager",
    "component_type": "class",
    "file_path": "/Users/wufagang/project/aiopen/CodeWiki/codewiki/cli/git_manager.py",
    "relative_path": "codewiki/cli/git_manager.py",
    "depends_on": [
      "codewiki.cli.utils.errors.RepositoryError"
    ],
    "source_code": "class GitManager:\n    \"\"\"\n    Manages git operations for documentation generation.\n    \n    Handles:\n    - Status checking\n    - Branch creation\n    - Committing documentation\n    - Remote detection\n    \"\"\"\n    \n    def __init__(self, repo_path: Path):\n        \"\"\"\n        Initialize git manager.\n        \n        Args:\n            repo_path: Path to git repository\n            \n        Raises:\n            RepositoryError: If not a valid git repository\n        \"\"\"\n        self.repo_path = Path(repo_path).expanduser().resolve()\n        \n        try:\n            self.repo = git.Repo(repo_path, search_parent_directories=True)\n        except git.InvalidGitRepositoryError:\n            raise RepositoryError(\n                f\"Not a git repository: {repo_path}\\n\\n\"\n                \"To initialize a git repository: git init\"\n            )\n    \n    def check_clean_working_directory(self) -> Tuple[bool, str]:\n        \"\"\"\n        Check if working directory is clean (no uncommitted changes).\n        \n        Returns:\n            Tuple of (is_clean, status_message)\n        \"\"\"\n        if self.repo.is_dirty(untracked_files=True):\n            status_lines = []\n            \n            # Changed files\n            changed = [item.a_path for item in self.repo.index.diff(None)]\n            if changed:\n                status_lines.append(f\"Modified: {', '.join(changed[:3])}\")\n                if len(changed) > 3:\n                    status_lines.append(f\"... and {len(changed) - 3} more\")\n            \n            # Untracked files\n            untracked = self.repo.untracked_files\n            if untracked:\n                status_lines.append(f\"Untracked: {', '.join(untracked[:3])}\")\n                if len(untracked) > 3:\n                    status_lines.append(f\"... and {len(untracked) - 3} more\")\n            \n            return False, \"\\n\".join(status_lines)\n        \n        return True, \"Working directory is clean\"\n    \n    def create_documentation_branch(self, force: bool = False) -> str:\n        \"\"\"\n        Create a new documentation branch with timestamp.\n        \n        Args:\n            force: Force creation even if dirty working directory\n            \n        Returns:\n            Branch name\n            \n        Raises:\n            RepositoryError: If working directory is dirty (unless force=True)\n        \"\"\"\n        # Check working directory\n        if not force:\n            is_clean, status_msg = self.check_clean_working_directory()\n            if not is_clean:\n                raise RepositoryError(\n                    \"Working directory has uncommitted changes.\\n\\n\"\n                    f\"{status_msg}\\n\\n\"\n                    \"Cannot create documentation branch with uncommitted changes.\\n\"\n                    \"Please commit or stash your changes first:\\n\"\n                    \"  git status\\n\"\n                    \"  git add -A && git commit -m \\\"Your message\\\"\\n\"\n                    \"  # or\\n\"\n                    \"  git stash\\n\\n\"\n                    \"Then re-run: codewiki generate --create-branch\"\n                )\n        \n        # Generate branch name with timestamp\n        timestamp = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n        branch_name = f\"docs/codewiki-{timestamp}\"\n        \n        # Check if branch already exists (shouldn't happen with timestamp)\n        existing_branches = [b.name for b in self.repo.branches]\n        if branch_name in existing_branches:\n            # Append counter\n            counter = 1\n            while f\"{branch_name}-{counter}\" in existing_branches:\n                counter += 1\n            branch_name = f\"{branch_name}-{counter}\"\n        \n        try:\n            # Create and checkout new branch\n            new_branch = self.repo.create_head(branch_name)\n            new_branch.checkout()\n            return branch_name\n        except GitCommandError as e:\n            raise RepositoryError(f\"Failed to create branch: {e}\")\n    \n    def commit_documentation(\n        self,\n        docs_path: Path,\n        message: Optional[str] = None\n    ) -> str:\n        \"\"\"\n        Commit generated documentation.\n        \n        Args:\n            docs_path: Path to documentation directory\n            message: Commit message (optional)\n            \n        Returns:\n            Commit hash\n            \n        Raises:\n            RepositoryError: If commit fails\n        \"\"\"\n        if message is None:\n            message = \"Add generated documentation\\n\\nGenerated by CodeWiki CLI\"\n        \n        try:\n            # Add documentation files\n            self.repo.index.add([str(docs_path)])\n            \n            # Commit\n            commit = self.repo.index.commit(message)\n            \n            return commit.hexsha\n        except GitCommandError as e:\n            raise RepositoryError(f\"Failed to commit documentation: {e}\")\n    \n    def get_remote_url(self, remote_name: str = \"origin\") -> Optional[str]:\n        \"\"\"\n        Get remote repository URL.\n        \n        Args:\n            remote_name: Name of remote (default: origin)\n            \n        Returns:\n            Remote URL or None if no remote\n        \"\"\"\n        try:\n            remote = self.repo.remote(remote_name)\n            return remote.url\n        except ValueError:\n            return None\n    \n    def get_current_branch(self) -> str:\n        \"\"\"\n        Get current branch name.\n        \n        Returns:\n            Branch name\n        \"\"\"\n        try:\n            return self.repo.active_branch.name\n        except TypeError:\n            # Detached HEAD\n            return \"HEAD\"\n    \n    def get_commit_hash(self) -> str:\n        \"\"\"\n        Get current commit hash.\n        \n        Returns:\n            Commit hash\n        \"\"\"\n        return self.repo.head.commit.hexsha\n    \n    def branch_exists(self, branch_name: str) -> bool:\n        \"\"\"\n        Check if a branch exists.\n        \n        Args:\n            branch_name: Branch name to check\n            \n        Returns:\n            True if exists, False otherwise\n        \"\"\"\n        return branch_name in [b.name for b in self.repo.branches]\n    \n    def get_github_pr_url(self, branch_name: str) -> Optional[str]:\n        \"\"\"\n        Get GitHub PR creation URL for a branch.\n        \n        Args:\n            branch_name: Branch name\n            \n        Returns:\n            PR URL or None if not a GitHub repo\n        \"\"\"\n        remote_url = self.get_remote_url()\n        if not remote_url or \"github.com\" not in remote_url:\n            return None\n        \n        # Clean URL\n        base_url = remote_url.rstrip('/').replace('.git', '')\n        \n        # Convert SSH to HTTPS\n        if base_url.startswith('git@github.com:'):\n            base_url = base_url.replace('git@github.com:', 'https://github.com/')\n        \n        return f\"{base_url}/compare/{branch_name}\"",
    "start_line": 14,
    "end_line": 226,
    "has_docstring": true,
    "docstring": "Manages git operations for documentation generation.\n\nHandles:\n- Status checking\n- Branch creation\n- Committing documentation\n- Remote detection",
    "parameters": null,
    "node_type": "class",
    "base_classes": null,
    "class_name": null,
    "display_name": "class GitManager",
    "component_id": "codewiki.cli.git_manager.GitManager"
  },
  "codewiki.cli.html_generator.HTMLGenerator": {
    "id": "codewiki.cli.html_generator.HTMLGenerator",
    "name": "HTMLGenerator",
    "component_type": "class",
    "file_path": "/Users/wufagang/project/aiopen/CodeWiki/codewiki/cli/html_generator.py",
    "relative_path": "codewiki/cli/html_generator.py",
    "depends_on": [
      "codewiki.cli.utils.fs.safe_read",
      "codewiki.cli.utils.errors.FileSystemError",
      "codewiki.cli.utils.fs.safe_write",
      "codewiki.src.fe.visualise_docs.load_module_tree"
    ],
    "source_code": "class HTMLGenerator:\n    \"\"\"\n    Generates static HTML documentation viewer for GitHub Pages.\n    \n    Creates a self-contained index.html with embedded styles, scripts,\n    and configuration for client-side markdown rendering.\n    \"\"\"\n    \n    def __init__(self, template_dir: Optional[Path] = None):\n        \"\"\"\n        Initialize HTML generator.\n        \n        Args:\n            template_dir: Path to template directory (default: package templates)\n        \"\"\"\n        if template_dir is None:\n            # Use package templates\n            template_dir = Path(__file__).parent.parent / \"templates\" / \"github_pages\"\n        \n        self.template_dir = Path(template_dir)\n        \n    \n    def load_module_tree(self, docs_dir: Path) -> Dict[str, Any]:\n        \"\"\"\n        Load module tree from documentation directory.\n        \n        Args:\n            docs_dir: Documentation directory path\n            \n        Returns:\n            Module tree structure\n        \"\"\"\n        module_tree_path = docs_dir / \"module_tree.json\"\n        if not module_tree_path.exists():\n            # Fallback to a simple structure\n            return {\n                \"Overview\": {\n                    \"description\": \"Repository overview\",\n                    \"components\": [],\n                    \"children\": {}\n                }\n            }\n        \n        try:\n            content = safe_read(module_tree_path)\n            return json.loads(content)\n        except Exception as e:\n            raise FileSystemError(f\"Failed to load module tree: {e}\")\n    \n    def load_metadata(self, docs_dir: Path) -> Optional[Dict[str, Any]]:\n        \"\"\"\n        Load metadata from documentation directory.\n        \n        Args:\n            docs_dir: Documentation directory path\n            \n        Returns:\n            Metadata dictionary or None if not found\n        \"\"\"\n        metadata_path = docs_dir / \"metadata.json\"\n        if not metadata_path.exists():\n            return None\n        \n        try:\n            content = safe_read(metadata_path)\n            return json.loads(content)\n        except Exception:\n            # Non-critical, return None\n            return None\n            \n    def generate(\n        self,\n        output_path: Path,\n        title: str,\n        module_tree: Optional[Dict[str, Any]] = None,\n        repository_url: Optional[str] = None,\n        github_pages_url: Optional[str] = None,\n        config: Optional[Dict[str, Any]] = None,\n        docs_dir: Optional[Path] = None,\n        metadata: Optional[Dict[str, Any]] = None\n    ):\n        \"\"\"\n        Generate HTML documentation viewer.\n        \n        Args:\n            output_path: Output file path (index.html)\n            title: Documentation title\n            module_tree: Module tree structure (auto-loaded from docs_dir if not provided)\n            repository_url: GitHub repository URL\n            github_pages_url: Expected GitHub Pages URL\n            config: Additional configuration\n            docs_dir: Documentation directory (for auto-loading module_tree and metadata)\n            metadata: Metadata dictionary (auto-loaded from docs_dir if not provided)\n        \"\"\"\n        # Auto-load module_tree and metadata from docs_dir if not provided\n        if docs_dir:\n            if module_tree is None:\n                module_tree = self.load_module_tree(docs_dir)\n            if metadata is None:\n                metadata = self.load_metadata(docs_dir)\n        \n        # Default values\n        if module_tree is None:\n            module_tree = {}\n        if config is None:\n            config = {}\n        \n        # Load template\n        template_path = self.template_dir / \"viewer_template.html\"\n        if not template_path.exists():\n            raise FileSystemError(f\"Template not found: {template_path}\")\n        \n        template_content = safe_read(template_path)\n        \n        # Build info content HTML\n        info_content = self._build_info_content(metadata)\n        show_info = \"block\" if info_content else \"none\"\n        \n        # Build repository link\n        repo_link = \"\"\n        if repository_url:\n            repo_link = f'<a href=\"{repository_url}\" class=\"repo-link\" target=\"_blank\">🔗 View Repository</a>'\n        \n        # Determine docs base path\n        # For GitHub Pages: relative path to docs folder\n        # For local: relative path to docs folder\n        docs_base_path = \"\"\n        if docs_dir and output_path.parent != docs_dir:\n            # Calculate relative path from output to docs\n            try:\n                docs_base_path = Path(docs_dir.name).as_posix()\n            except Exception:\n                docs_base_path = \".\"\n        \n        # Prepare JSON data for embedding\n        config_json = json.dumps(config, indent=2)\n        module_tree_json = json.dumps(module_tree, indent=2)\n        metadata_json = json.dumps(metadata, indent=2) if metadata else \"null\"\n        \n        # Replace placeholders\n        html_content = template_content\n        replacements = {\n            \"{{TITLE}}\": self._escape_html(title),\n            \"{{REPO_LINK}}\": repo_link,\n            \"{{SHOW_INFO}}\": show_info,\n            \"{{INFO_CONTENT}}\": info_content,\n            \"{{CONFIG_JSON}}\": config_json,\n            \"{{MODULE_TREE_JSON}}\": module_tree_json,\n            \"{{METADATA_JSON}}\": metadata_json,\n            \"{{DOCS_BASE_PATH}}\": docs_base_path,\n        }\n        \n        for placeholder, value in replacements.items():\n            html_content = html_content.replace(placeholder, value)\n        \n        # Write output\n        output_path = Path(output_path)\n        output_path.parent.mkdir(parents=True, exist_ok=True)\n        safe_write(output_path, html_content)\n    \n    def _build_info_content(self, metadata: Optional[Dict[str, Any]]) -> str:\n        \"\"\"\n        Build HTML content for repo info section.\n        \n        Args:\n            metadata: Metadata dictionary\n            \n        Returns:\n            HTML string for info content\n        \"\"\"\n        if not metadata or not metadata.get('generation_info'):\n            return \"\"\n        \n        info = metadata.get('generation_info', {})\n        stats = metadata.get('statistics', {})\n        \n        html_parts = []\n        \n        if info.get('main_model'):\n            html_parts.append(f'<div class=\"info-row\"><strong>Model:</strong> {self._escape_html(info[\"main_model\"])}</div>')\n        \n        if info.get('timestamp'):\n            try:\n                from datetime import datetime\n                timestamp = info['timestamp']\n                # Parse ISO format timestamp\n                if isinstance(timestamp, str):\n                    dt = datetime.fromisoformat(timestamp.replace('Z', '+00:00'))\n                    formatted_date = dt.strftime('%Y-%m-%d')\n                    html_parts.append(f'<div class=\"info-row\"><strong>Generated:</strong> {formatted_date}</div>')\n            except Exception:\n                pass\n        \n        if info.get('commit_id'):\n            commit_short = info['commit_id'][:8]\n            html_parts.append(f'<div class=\"info-row\"><strong>Commit:</strong> {commit_short}</div>')\n        \n        if stats.get('total_components'):\n            components_str = f\"{stats['total_components']:,}\"\n            html_parts.append(f'<div class=\"info-row\"><strong>Components:</strong> {components_str}</div>')\n        \n        if stats.get('max_depth'):\n            html_parts.append(f'<div class=\"info-row\"><strong>Max Depth:</strong> {stats[\"max_depth\"]}</div>')\n        \n        return '\\n                '.join(html_parts)\n    \n    def _escape_html(self, text: str) -> str:\n        \"\"\"\n        Escape HTML special characters.\n        \n        Args:\n            text: Text to escape\n            \n        Returns:\n            Escaped text\n        \"\"\"\n        return (text\n                .replace('&', '&amp;')\n                .replace('<', '&lt;')\n                .replace('>', '&gt;')\n                .replace('\"', '&quot;')\n                .replace(\"'\", '&#39;'))\n       \n\n    \n    def detect_repository_info(self, repo_path: Path) -> Dict[str, Optional[str]]:\n        \"\"\"\n        Detect repository information from git.\n        \n        Args:\n            repo_path: Repository path\n            \n        Returns:\n            Dictionary with 'name', 'url', 'github_pages_url'\n        \"\"\"\n        info = {\n            'name': repo_path.name,\n            'url': None,\n            'github_pages_url': None,\n        }\n        \n        try:\n            import git\n            repo = git.Repo(repo_path)\n            \n            # Get repository name\n            info['name'] = repo_path.name\n            \n            # Get remote URL\n            if repo.remotes:\n                remote_url = repo.remotes.origin.url\n                \n                # Clean URL\n                if remote_url.startswith('git@github.com:'):\n                    remote_url = remote_url.replace('git@github.com:', 'https://github.com/')\n                \n                remote_url = remote_url.rstrip('/').replace('.git', '')\n                info['url'] = remote_url\n                \n                # Compute GitHub Pages URL\n                if 'github.com' in remote_url:\n                    parts = remote_url.split('/')\n                    if len(parts) >= 2:\n                        owner = parts[-2]\n                        repo = parts[-1]\n                        info['github_pages_url'] = f\"https://{owner}.github.io/{repo}/\"\n        \n        except Exception:\n            pass\n        \n        return info",
    "start_line": 13,
    "end_line": 283,
    "has_docstring": true,
    "docstring": "Generates static HTML documentation viewer for GitHub Pages.\n\nCreates a self-contained index.html with embedded styles, scripts,\nand configuration for client-side markdown rendering.",
    "parameters": null,
    "node_type": "class",
    "base_classes": null,
    "class_name": null,
    "display_name": "class HTMLGenerator",
    "component_id": "codewiki.cli.html_generator.HTMLGenerator"
  },
  "codewiki.cli.main.cli": {
    "id": "codewiki.cli.main.cli",
    "name": "cli",
    "component_type": "function",
    "file_path": "/Users/wufagang/project/aiopen/CodeWiki/codewiki/cli/main.py",
    "relative_path": "codewiki/cli/main.py",
    "depends_on": [],
    "source_code": "def cli(ctx):\n    \"\"\"\n    CodeWiki: Transform codebases into comprehensive documentation.\n    \n    Generate AI-powered documentation for your code repositories with support\n    for Python, Java, JavaScript, TypeScript, C, C++, and C#.\n    \"\"\"\n    # Ensure context object exists\n    ctx.ensure_object(dict)",
    "start_line": 15,
    "end_line": 23,
    "has_docstring": true,
    "docstring": "CodeWiki: Transform codebases into comprehensive documentation.\n\nGenerate AI-powered documentation for your code repositories with support\nfor Python, Java, JavaScript, TypeScript, C, C++, and C#.",
    "parameters": [
      "ctx"
    ],
    "node_type": "function",
    "base_classes": null,
    "class_name": null,
    "display_name": "function cli",
    "component_id": "codewiki.cli.main.cli"
  },
  "codewiki.cli.main.version": {
    "id": "codewiki.cli.main.version",
    "name": "version",
    "component_type": "function",
    "file_path": "/Users/wufagang/project/aiopen/CodeWiki/codewiki/cli/main.py",
    "relative_path": "codewiki/cli/main.py",
    "depends_on": [],
    "source_code": "def version():\n    \"\"\"Display version information.\"\"\"\n    click.echo(f\"CodeWiki CLI v{__version__}\")\n    click.echo(\"Python-based documentation generator using AI analysis\")",
    "start_line": 27,
    "end_line": 30,
    "has_docstring": true,
    "docstring": "Display version information.",
    "parameters": [],
    "node_type": "function",
    "base_classes": null,
    "class_name": null,
    "display_name": "function version",
    "component_id": "codewiki.cli.main.version"
  },
  "codewiki.cli.main.main": {
    "id": "codewiki.cli.main.main",
    "name": "main",
    "component_type": "function",
    "file_path": "/Users/wufagang/project/aiopen/CodeWiki/codewiki/cli/main.py",
    "relative_path": "codewiki/cli/main.py",
    "depends_on": [
      "codewiki.cli.main.cli"
    ],
    "source_code": "def main():\n    \"\"\"Entry point for the CLI.\"\"\"\n    try:\n        cli(obj={})\n    except KeyboardInterrupt:\n        click.echo(\"\\n\\nInterrupted by user\", err=True)\n        sys.exit(130)\n    except Exception as e:\n        click.secho(f\"\\n✗ Unexpected error: {e}\", fg=\"red\", err=True)\n        sys.exit(1)",
    "start_line": 42,
    "end_line": 51,
    "has_docstring": true,
    "docstring": "Entry point for the CLI.",
    "parameters": [],
    "node_type": "function",
    "base_classes": null,
    "class_name": null,
    "display_name": "function main",
    "component_id": "codewiki.cli.main.main"
  },
  "codewiki.cli.models.config.Configuration": {
    "id": "codewiki.cli.models.config.Configuration",
    "name": "Configuration",
    "component_type": "class",
    "file_path": "/Users/wufagang/project/aiopen/CodeWiki/codewiki/cli/models/config.py",
    "relative_path": "codewiki/cli/models/config.py",
    "depends_on": [
      "codewiki.cli.utils.validation.validate_model_name",
      "codewiki.cli.utils.validation.validate_url"
    ],
    "source_code": "class Configuration:\n    \"\"\"\n    CodeWiki configuration data model.\n    \n    Attributes:\n        base_url: LLM API base URL\n        main_model: Primary model for documentation generation\n        cluster_model: Model for module clustering\n        fallback_model: Fallback model for documentation generation\n        default_output: Default output directory\n    \"\"\"\n    base_url: str\n    main_model: str\n    cluster_model: str\n    fallback_model: str = \"glm-4p5\"\n    default_output: str = \"docs\"\n    \n    def validate(self):\n        \"\"\"\n        Validate all configuration fields.\n        \n        Raises:\n            ConfigurationError: If validation fails\n        \"\"\"\n        validate_url(self.base_url)\n        validate_model_name(self.main_model)\n        validate_model_name(self.cluster_model)\n        validate_model_name(self.fallback_model)\n    \n    def to_dict(self) -> dict:\n        \"\"\"Convert to dictionary.\"\"\"\n        return asdict(self)\n    \n    @classmethod\n    def from_dict(cls, data: dict) -> 'Configuration':\n        \"\"\"\n        Create Configuration from dictionary.\n        \n        Args:\n            data: Configuration dictionary\n            \n        Returns:\n            Configuration instance\n        \"\"\"\n        return cls(\n            base_url=data.get('base_url', ''),\n            main_model=data.get('main_model', ''),\n            cluster_model=data.get('cluster_model', ''),\n            fallback_model=data.get('fallback_model', 'glm-4p5'),\n            default_output=data.get('default_output', 'docs'),\n        )\n    \n    def is_complete(self) -> bool:\n        \"\"\"Check if all required fields are set.\"\"\"\n        return bool(\n            self.base_url and \n            self.main_model and \n            self.cluster_model and\n            self.fallback_model\n        )\n    \n    def to_backend_config(self, repo_path: str, output_dir: str, api_key: str):\n        \"\"\"\n        Convert CLI Configuration to Backend Config.\n        \n        This method bridges the gap between persistent user settings (CLI Configuration)\n        and runtime job configuration (Backend Config).\n        \n        Args:\n            repo_path: Path to the repository to document\n            output_dir: Output directory for generated documentation\n            api_key: LLM API key (from keyring)\n            \n        Returns:\n            Backend Config instance ready for documentation generation\n        \"\"\"\n        from codewiki.src.config import Config\n        \n        return Config.from_cli(\n            repo_path=repo_path,\n            output_dir=output_dir,\n            llm_base_url=self.base_url,\n            llm_api_key=api_key,\n            main_model=self.main_model,\n            cluster_model=self.cluster_model,\n            fallback_model=self.fallback_model\n        )",
    "start_line": 21,
    "end_line": 107,
    "has_docstring": true,
    "docstring": "CodeWiki configuration data model.\n\nAttributes:\n    base_url: LLM API base URL\n    main_model: Primary model for documentation generation\n    cluster_model: Model for module clustering\n    fallback_model: Fallback model for documentation generation\n    default_output: Default output directory",
    "parameters": null,
    "node_type": "class",
    "base_classes": null,
    "class_name": null,
    "display_name": "class Configuration",
    "component_id": "codewiki.cli.models.config.Configuration"
  },
  "codewiki.cli.models.job.JobStatus": {
    "id": "codewiki.cli.models.job.JobStatus",
    "name": "JobStatus",
    "component_type": "class",
    "file_path": "/Users/wufagang/project/aiopen/CodeWiki/codewiki/cli/models/job.py",
    "relative_path": "codewiki/cli/models/job.py",
    "depends_on": [],
    "source_code": "class JobStatus(str, Enum):\n    \"\"\"Documentation job status.\"\"\"\n    PENDING = \"pending\"\n    RUNNING = \"running\"\n    COMPLETED = \"completed\"\n    FAILED = \"failed\"",
    "start_line": 13,
    "end_line": 18,
    "has_docstring": true,
    "docstring": "Documentation job status.",
    "parameters": null,
    "node_type": "class",
    "base_classes": [
      "str",
      "Enum"
    ],
    "class_name": null,
    "display_name": "class JobStatus",
    "component_id": "codewiki.cli.models.job.JobStatus"
  },
  "codewiki.cli.models.job.GenerationOptions": {
    "id": "codewiki.cli.models.job.GenerationOptions",
    "name": "GenerationOptions",
    "component_type": "class",
    "file_path": "/Users/wufagang/project/aiopen/CodeWiki/codewiki/cli/models/job.py",
    "relative_path": "codewiki/cli/models/job.py",
    "depends_on": [],
    "source_code": "class GenerationOptions:\n    \"\"\"Options for documentation generation.\"\"\"\n    create_branch: bool = False\n    github_pages: bool = False\n    no_cache: bool = False\n    custom_output: Optional[str] = None",
    "start_line": 22,
    "end_line": 27,
    "has_docstring": true,
    "docstring": "Options for documentation generation.",
    "parameters": null,
    "node_type": "class",
    "base_classes": null,
    "class_name": null,
    "display_name": "class GenerationOptions",
    "component_id": "codewiki.cli.models.job.GenerationOptions"
  },
  "codewiki.cli.models.job.JobStatistics": {
    "id": "codewiki.cli.models.job.JobStatistics",
    "name": "JobStatistics",
    "component_type": "class",
    "file_path": "/Users/wufagang/project/aiopen/CodeWiki/codewiki/cli/models/job.py",
    "relative_path": "codewiki/cli/models/job.py",
    "depends_on": [],
    "source_code": "class JobStatistics:\n    \"\"\"Statistics for a documentation job.\"\"\"\n    total_files_analyzed: int = 0\n    leaf_nodes: int = 0\n    max_depth: int = 0\n    total_tokens_used: int = 0",
    "start_line": 31,
    "end_line": 36,
    "has_docstring": true,
    "docstring": "Statistics for a documentation job.",
    "parameters": null,
    "node_type": "class",
    "base_classes": null,
    "class_name": null,
    "display_name": "class JobStatistics",
    "component_id": "codewiki.cli.models.job.JobStatistics"
  },
  "codewiki.cli.models.job.LLMConfig": {
    "id": "codewiki.cli.models.job.LLMConfig",
    "name": "LLMConfig",
    "component_type": "class",
    "file_path": "/Users/wufagang/project/aiopen/CodeWiki/codewiki/cli/models/job.py",
    "relative_path": "codewiki/cli/models/job.py",
    "depends_on": [],
    "source_code": "class LLMConfig:\n    \"\"\"LLM configuration for a job.\"\"\"\n    main_model: str\n    cluster_model: str\n    base_url: str",
    "start_line": 40,
    "end_line": 44,
    "has_docstring": true,
    "docstring": "LLM configuration for a job.",
    "parameters": null,
    "node_type": "class",
    "base_classes": null,
    "class_name": null,
    "display_name": "class LLMConfig",
    "component_id": "codewiki.cli.models.job.LLMConfig"
  },
  "codewiki.cli.models.job.DocumentationJob": {
    "id": "codewiki.cli.models.job.DocumentationJob",
    "name": "DocumentationJob",
    "component_type": "class",
    "file_path": "/Users/wufagang/project/aiopen/CodeWiki/codewiki/cli/models/job.py",
    "relative_path": "codewiki/cli/models/job.py",
    "depends_on": [
      "codewiki.cli.models.job.GenerationOptions",
      "codewiki.cli.models.job.LLMConfig",
      "codewiki.cli.models.job.JobStatus",
      "codewiki.cli.models.job.JobStatistics"
    ],
    "source_code": "class DocumentationJob:\n    \"\"\"\n    Represents a documentation generation job.\n    \n    Attributes:\n        job_id: Unique job identifier\n        repository_path: Absolute path to repository\n        repository_name: Repository name\n        output_directory: Output directory path\n        commit_hash: Git commit SHA\n        branch_name: Git branch name (if applicable)\n        timestamp_start: Job start time\n        timestamp_end: Job end time (if completed)\n        status: Current job status\n        error_message: Error message (if failed)\n        files_generated: List of generated files\n        module_count: Number of modules documented\n        generation_options: Generation options used\n        llm_config: LLM configuration used\n        statistics: Job statistics\n    \"\"\"\n    job_id: str = field(default_factory=lambda: str(uuid.uuid4()))\n    repository_path: str = \"\"\n    repository_name: str = \"\"\n    output_directory: str = \"\"\n    commit_hash: str = \"\"\n    branch_name: Optional[str] = None\n    timestamp_start: str = field(default_factory=lambda: datetime.now().isoformat())\n    timestamp_end: Optional[str] = None\n    status: JobStatus = JobStatus.PENDING\n    error_message: Optional[str] = None\n    files_generated: List[str] = field(default_factory=list)\n    module_count: int = 0\n    generation_options: GenerationOptions = field(default_factory=GenerationOptions)\n    llm_config: Optional[LLMConfig] = None\n    statistics: JobStatistics = field(default_factory=JobStatistics)\n    \n    def start(self):\n        \"\"\"Mark job as started.\"\"\"\n        self.status = JobStatus.RUNNING\n        self.timestamp_start = datetime.now().isoformat()\n    \n    def complete(self):\n        \"\"\"Mark job as completed.\"\"\"\n        self.status = JobStatus.COMPLETED\n        self.timestamp_end = datetime.now().isoformat()\n    \n    def fail(self, error_message: str):\n        \"\"\"Mark job as failed.\"\"\"\n        self.status = JobStatus.FAILED\n        self.error_message = error_message\n        self.timestamp_end = datetime.now().isoformat()\n    \n    def to_dict(self) -> Dict[str, Any]:\n        \"\"\"Convert to dictionary for JSON serialization.\"\"\"\n        data = {\n            \"job_id\": self.job_id,\n            \"repository_path\": self.repository_path,\n            \"repository_name\": self.repository_name,\n            \"output_directory\": self.output_directory,\n            \"commit_hash\": self.commit_hash,\n            \"branch_name\": self.branch_name,\n            \"timestamp_start\": self.timestamp_start,\n            \"timestamp_end\": self.timestamp_end,\n            \"status\": self.status.value if isinstance(self.status, JobStatus) else self.status,\n            \"error_message\": self.error_message,\n            \"files_generated\": self.files_generated,\n            \"module_count\": self.module_count,\n            \"generation_options\": asdict(self.generation_options),\n            \"llm_config\": asdict(self.llm_config) if self.llm_config else None,\n            \"statistics\": asdict(self.statistics),\n        }\n        return data\n    \n    def to_json(self) -> str:\n        \"\"\"Convert to JSON string.\"\"\"\n        return json.dumps(self.to_dict(), indent=2)\n    \n    @classmethod\n    def from_dict(cls, data: Dict[str, Any]) -> 'DocumentationJob':\n        \"\"\"Create from dictionary.\"\"\"\n        job = cls(\n            job_id=data.get('job_id', str(uuid.uuid4())),\n            repository_path=data.get('repository_path', ''),\n            repository_name=data.get('repository_name', ''),\n            output_directory=data.get('output_directory', ''),\n            commit_hash=data.get('commit_hash', ''),\n            branch_name=data.get('branch_name'),\n            timestamp_start=data.get('timestamp_start', datetime.now().isoformat()),\n            timestamp_end=data.get('timestamp_end'),\n            status=JobStatus(data.get('status', 'pending')),\n            error_message=data.get('error_message'),\n            files_generated=data.get('files_generated', []),\n            module_count=data.get('module_count', 0),\n        )\n        \n        # Parse nested objects\n        if 'generation_options' in data:\n            opts = data['generation_options']\n            job.generation_options = GenerationOptions(**opts)\n        \n        if 'llm_config' in data and data['llm_config']:\n            job.llm_config = LLMConfig(**data['llm_config'])\n        \n        if 'statistics' in data:\n            job.statistics = JobStatistics(**data['statistics'])\n        \n        return job",
    "start_line": 48,
    "end_line": 155,
    "has_docstring": true,
    "docstring": "Represents a documentation generation job.\n\nAttributes:\n    job_id: Unique job identifier\n    repository_path: Absolute path to repository\n    repository_name: Repository name\n    output_directory: Output directory path\n    commit_hash: Git commit SHA\n    branch_name: Git branch name (if applicable)\n    timestamp_start: Job start time\n    timestamp_end: Job end time (if completed)\n    status: Current job status\n    error_message: Error message (if failed)\n    files_generated: List of generated files\n    module_count: Number of modules documented\n    generation_options: Generation options used\n    llm_config: LLM configuration used\n    statistics: Job statistics",
    "parameters": null,
    "node_type": "class",
    "base_classes": null,
    "class_name": null,
    "display_name": "class DocumentationJob",
    "component_id": "codewiki.cli.models.job.DocumentationJob"
  },
  "codewiki.cli.utils.api_errors.APIErrorHandler": {
    "id": "codewiki.cli.utils.api_errors.APIErrorHandler",
    "name": "APIErrorHandler",
    "component_type": "class",
    "file_path": "/Users/wufagang/project/aiopen/CodeWiki/codewiki/cli/utils/api_errors.py",
    "relative_path": "codewiki/cli/utils/api_errors.py",
    "depends_on": [
      "codewiki.cli.utils.errors.APIError"
    ],
    "source_code": "class APIErrorHandler:\n    \"\"\"Handler for LLM API errors with fail-fast behavior.\"\"\"\n    \n    @staticmethod\n    def handle_api_error(\n        error: Exception,\n        context: Optional[str] = None,\n        fail_fast: bool = True\n    ) -> APIError:\n        \"\"\"\n        Handle LLM API error and convert to APIError.\n        \n        Args:\n            error: The original exception\n            context: Additional context (e.g., module name)\n            fail_fast: Whether to fail immediately (default: True)\n            \n        Returns:\n            APIError instance\n        \"\"\"\n        error_message = str(error)\n        \n        # Detect specific error types\n        if \"429\" in error_message or \"rate limit\" in error_message.lower():\n            message = (\n                \"LLM API rate limit exceeded.\\n\\n\"\n                \"The API returned a 429 error, indicating too many requests.\\n\\n\"\n                \"Troubleshooting:\\n\"\n                \"  1. Wait a few minutes before retrying\\n\"\n                \"  2. Check your API quota at your provider's dashboard\\n\"\n                \"  3. Consider upgrading your API plan\\n\"\n                \"  4. For large repositories, generate during off-peak hours\"\n            )\n        elif \"401\" in error_message or \"authentication\" in error_message.lower():\n            message = (\n                \"LLM API authentication failed.\\n\\n\"\n                \"Your API key appears to be invalid or expired.\\n\\n\"\n                \"Troubleshooting:\\n\"\n                \"  1. Verify your API key: codewiki config show\\n\"\n                \"  2. Update your API key: codewiki config set --api-key <new-key>\\n\"\n                \"  3. Check that your API key is active in your provider's dashboard\"\n            )\n        elif \"timeout\" in error_message.lower():\n            message = (\n                \"LLM API request timed out.\\n\\n\"\n                \"The API did not respond within the expected time.\\n\\n\"\n                \"Troubleshooting:\\n\"\n                \"  1. Check your internet connection\\n\"\n                \"  2. Verify the API service is operational\\n\"\n                \"  3. Try again in a few moments\\n\"\n                \"  4. If the issue persists, contact your API provider\"\n            )\n        elif \"network\" in error_message.lower() or \"connection\" in error_message.lower():\n            message = (\n                \"Network error while connecting to LLM API.\\n\\n\"\n                \"Could not establish connection to the API.\\n\\n\"\n                \"Troubleshooting:\\n\"\n                \"  1. Check your internet connection\\n\"\n                \"  2. Verify the base URL: codewiki config show\\n\"\n                \"  3. Check if you're behind a proxy or firewall\\n\"\n                \"  4. Try: curl -I <base-url> to test connectivity\"\n            )\n        else:\n            message = (\n                f\"LLM API error: {error_message}\\n\\n\"\n                \"An unexpected error occurred while communicating with the LLM API.\\n\\n\"\n                \"Troubleshooting:\\n\"\n                \"  1. Check your configuration: codewiki config validate\\n\"\n                \"  2. Verify API service status\\n\"\n                \"  3. Review the error message above for specific details\"\n            )\n        \n        if context:\n            message = f\"Context: {context}\\n\\n{message}\"\n        \n        return APIError(message)\n    \n    @staticmethod\n    def display_api_error(error: APIError, module_name: Optional[str] = None):\n        \"\"\"\n        Display API error with formatting.\n        \n        Args:\n            error: The API error\n            module_name: Optional module name for context\n        \"\"\"\n        click.echo()\n        click.secho(\"✗ LLM API Error\", fg=\"red\", bold=True)\n        click.echo()\n        \n        if module_name:\n            click.echo(f\"Module: {module_name}\")\n            click.echo()\n        \n        click.echo(error.message)\n        click.echo()\n        click.secho(\n            \"Documentation generation stopped. No partial results saved.\",\n            fg=\"yellow\"\n        )\n        click.echo()",
    "start_line": 11,
    "end_line": 111,
    "has_docstring": true,
    "docstring": "Handler for LLM API errors with fail-fast behavior.",
    "parameters": null,
    "node_type": "class",
    "base_classes": null,
    "class_name": null,
    "display_name": "class APIErrorHandler",
    "component_id": "codewiki.cli.utils.api_errors.APIErrorHandler"
  },
  "codewiki.cli.utils.api_errors.wrap_api_call": {
    "id": "codewiki.cli.utils.api_errors.wrap_api_call",
    "name": "wrap_api_call",
    "component_type": "function",
    "file_path": "/Users/wufagang/project/aiopen/CodeWiki/codewiki/cli/utils/api_errors.py",
    "relative_path": "codewiki/cli/utils/api_errors.py",
    "depends_on": [],
    "source_code": "def wrap_api_call(func, *args, fail_fast: bool = True, context: Optional[str] = None, **kwargs):\n    \"\"\"\n    Wrap an API call with error handling.\n    \n    Args:\n        func: Function to call\n        *args: Positional arguments\n        fail_fast: Whether to raise on error (default: True)\n        context: Optional context for error message\n        **kwargs: Keyword arguments\n        \n    Returns:\n        Function result\n        \n    Raises:\n        APIError: If API call fails and fail_fast is True\n    \"\"\"\n    try:\n        return func(*args, **kwargs)\n    except Exception as e:\n        api_error = APIErrorHandler.handle_api_error(e, context=context, fail_fast=fail_fast)\n        if fail_fast:\n            raise api_error\n        else:\n            APIErrorHandler.display_api_error(api_error)\n            return None",
    "start_line": 114,
    "end_line": 139,
    "has_docstring": true,
    "docstring": "Wrap an API call with error handling.\n\nArgs:\n    func: Function to call\n    *args: Positional arguments\n    fail_fast: Whether to raise on error (default: True)\n    context: Optional context for error message\n    **kwargs: Keyword arguments\n    \nReturns:\n    Function result\n    \nRaises:\n    APIError: If API call fails and fail_fast is True",
    "parameters": [
      "func"
    ],
    "node_type": "function",
    "base_classes": null,
    "class_name": null,
    "display_name": "function wrap_api_call",
    "component_id": "codewiki.cli.utils.api_errors.wrap_api_call"
  },
  "codewiki.cli.utils.errors.CodeWikiError": {
    "id": "codewiki.cli.utils.errors.CodeWikiError",
    "name": "CodeWikiError",
    "component_type": "class",
    "file_path": "/Users/wufagang/project/aiopen/CodeWiki/codewiki/cli/utils/errors.py",
    "relative_path": "codewiki/cli/utils/errors.py",
    "depends_on": [],
    "source_code": "class CodeWikiError(Exception):\n    \"\"\"Base exception for CodeWiki CLI errors.\"\"\"\n    \n    def __init__(self, message: str, exit_code: int = EXIT_GENERAL_ERROR):\n        self.message = message\n        self.exit_code = exit_code\n        super().__init__(self.message)",
    "start_line": 27,
    "end_line": 33,
    "has_docstring": true,
    "docstring": "Base exception for CodeWiki CLI errors.",
    "parameters": null,
    "node_type": "class",
    "base_classes": [
      "Exception"
    ],
    "class_name": null,
    "display_name": "class CodeWikiError",
    "component_id": "codewiki.cli.utils.errors.CodeWikiError"
  },
  "codewiki.cli.utils.errors.ConfigurationError": {
    "id": "codewiki.cli.utils.errors.ConfigurationError",
    "name": "ConfigurationError",
    "component_type": "class",
    "file_path": "/Users/wufagang/project/aiopen/CodeWiki/codewiki/cli/utils/errors.py",
    "relative_path": "codewiki/cli/utils/errors.py",
    "depends_on": [
      "codewiki.cli.utils.errors.CodeWikiError"
    ],
    "source_code": "class ConfigurationError(CodeWikiError):\n    \"\"\"Configuration-related errors.\"\"\"\n    \n    def __init__(self, message: str):\n        super().__init__(message, EXIT_CONFIG_ERROR)",
    "start_line": 36,
    "end_line": 40,
    "has_docstring": true,
    "docstring": "Configuration-related errors.",
    "parameters": null,
    "node_type": "class",
    "base_classes": [
      "CodeWikiError"
    ],
    "class_name": null,
    "display_name": "class ConfigurationError",
    "component_id": "codewiki.cli.utils.errors.ConfigurationError"
  },
  "codewiki.cli.utils.errors.RepositoryError": {
    "id": "codewiki.cli.utils.errors.RepositoryError",
    "name": "RepositoryError",
    "component_type": "class",
    "file_path": "/Users/wufagang/project/aiopen/CodeWiki/codewiki/cli/utils/errors.py",
    "relative_path": "codewiki/cli/utils/errors.py",
    "depends_on": [
      "codewiki.cli.utils.errors.CodeWikiError"
    ],
    "source_code": "class RepositoryError(CodeWikiError):\n    \"\"\"Repository-related errors.\"\"\"\n    \n    def __init__(self, message: str):\n        super().__init__(message, EXIT_REPOSITORY_ERROR)",
    "start_line": 43,
    "end_line": 47,
    "has_docstring": true,
    "docstring": "Repository-related errors.",
    "parameters": null,
    "node_type": "class",
    "base_classes": [
      "CodeWikiError"
    ],
    "class_name": null,
    "display_name": "class RepositoryError",
    "component_id": "codewiki.cli.utils.errors.RepositoryError"
  },
  "codewiki.cli.utils.errors.APIError": {
    "id": "codewiki.cli.utils.errors.APIError",
    "name": "APIError",
    "component_type": "class",
    "file_path": "/Users/wufagang/project/aiopen/CodeWiki/codewiki/cli/utils/errors.py",
    "relative_path": "codewiki/cli/utils/errors.py",
    "depends_on": [
      "codewiki.cli.utils.errors.CodeWikiError"
    ],
    "source_code": "class APIError(CodeWikiError):\n    \"\"\"LLM API-related errors.\"\"\"\n    \n    def __init__(self, message: str):\n        super().__init__(message, EXIT_API_ERROR)",
    "start_line": 50,
    "end_line": 54,
    "has_docstring": true,
    "docstring": "LLM API-related errors.",
    "parameters": null,
    "node_type": "class",
    "base_classes": [
      "CodeWikiError"
    ],
    "class_name": null,
    "display_name": "class APIError",
    "component_id": "codewiki.cli.utils.errors.APIError"
  },
  "codewiki.cli.utils.errors.FileSystemError": {
    "id": "codewiki.cli.utils.errors.FileSystemError",
    "name": "FileSystemError",
    "component_type": "class",
    "file_path": "/Users/wufagang/project/aiopen/CodeWiki/codewiki/cli/utils/errors.py",
    "relative_path": "codewiki/cli/utils/errors.py",
    "depends_on": [
      "codewiki.cli.utils.errors.CodeWikiError"
    ],
    "source_code": "class FileSystemError(CodeWikiError):\n    \"\"\"File system-related errors.\"\"\"\n    \n    def __init__(self, message: str):\n        super().__init__(message, EXIT_FILESYSTEM_ERROR)",
    "start_line": 57,
    "end_line": 61,
    "has_docstring": true,
    "docstring": "File system-related errors.",
    "parameters": null,
    "node_type": "class",
    "base_classes": [
      "CodeWikiError"
    ],
    "class_name": null,
    "display_name": "class FileSystemError",
    "component_id": "codewiki.cli.utils.errors.FileSystemError"
  },
  "codewiki.cli.utils.errors.handle_error": {
    "id": "codewiki.cli.utils.errors.handle_error",
    "name": "handle_error",
    "component_type": "function",
    "file_path": "/Users/wufagang/project/aiopen/CodeWiki/codewiki/cli/utils/errors.py",
    "relative_path": "codewiki/cli/utils/errors.py",
    "depends_on": [],
    "source_code": "def handle_error(error: Exception, verbose: bool = False) -> int:\n    \"\"\"\n    Handle errors and return appropriate exit code.\n    \n    Args:\n        error: The exception to handle\n        verbose: Whether to show detailed error information\n        \n    Returns:\n        Exit code for the error\n    \"\"\"\n    if isinstance(error, CodeWikiError):\n        click.secho(f\"\\n✗ Error: {error.message}\", fg=\"red\", err=True)\n        return error.exit_code\n    else:\n        click.secho(f\"\\n✗ Unexpected error: {error}\", fg=\"red\", err=True)\n        if verbose:\n            import traceback\n            click.echo(traceback.format_exc(), err=True)\n        return EXIT_GENERAL_ERROR",
    "start_line": 64,
    "end_line": 83,
    "has_docstring": true,
    "docstring": "Handle errors and return appropriate exit code.\n\nArgs:\n    error: The exception to handle\n    verbose: Whether to show detailed error information\n    \nReturns:\n    Exit code for the error",
    "parameters": [
      "error",
      "verbose"
    ],
    "node_type": "function",
    "base_classes": null,
    "class_name": null,
    "display_name": "function handle_error",
    "component_id": "codewiki.cli.utils.errors.handle_error"
  },
  "codewiki.cli.utils.errors.error_with_suggestion": {
    "id": "codewiki.cli.utils.errors.error_with_suggestion",
    "name": "error_with_suggestion",
    "component_type": "function",
    "file_path": "/Users/wufagang/project/aiopen/CodeWiki/codewiki/cli/utils/errors.py",
    "relative_path": "codewiki/cli/utils/errors.py",
    "depends_on": [],
    "source_code": "def error_with_suggestion(message: str, suggestion: str, exit_code: int = EXIT_GENERAL_ERROR):\n    \"\"\"\n    Display error message with actionable suggestion and exit.\n    \n    Args:\n        message: The error message\n        suggestion: Suggested action to resolve the error\n        exit_code: Exit code to use\n    \"\"\"\n    click.secho(f\"\\n✗ Error: {message}\", fg=\"red\", err=True)\n    click.echo(f\"\\n{suggestion}\", err=True)\n    sys.exit(exit_code)",
    "start_line": 86,
    "end_line": 97,
    "has_docstring": true,
    "docstring": "Display error message with actionable suggestion and exit.\n\nArgs:\n    message: The error message\n    suggestion: Suggested action to resolve the error\n    exit_code: Exit code to use",
    "parameters": [
      "message",
      "suggestion",
      "exit_code"
    ],
    "node_type": "function",
    "base_classes": null,
    "class_name": null,
    "display_name": "function error_with_suggestion",
    "component_id": "codewiki.cli.utils.errors.error_with_suggestion"
  },
  "codewiki.cli.utils.errors.warning": {
    "id": "codewiki.cli.utils.errors.warning",
    "name": "warning",
    "component_type": "function",
    "file_path": "/Users/wufagang/project/aiopen/CodeWiki/codewiki/cli/utils/errors.py",
    "relative_path": "codewiki/cli/utils/errors.py",
    "depends_on": [],
    "source_code": "def warning(message: str):\n    \"\"\"Display a warning message.\"\"\"\n    click.secho(f\"⚠️  {message}\", fg=\"yellow\")",
    "start_line": 100,
    "end_line": 102,
    "has_docstring": true,
    "docstring": "Display a warning message.",
    "parameters": [
      "message"
    ],
    "node_type": "function",
    "base_classes": null,
    "class_name": null,
    "display_name": "function warning",
    "component_id": "codewiki.cli.utils.errors.warning"
  },
  "codewiki.cli.utils.errors.success": {
    "id": "codewiki.cli.utils.errors.success",
    "name": "success",
    "component_type": "function",
    "file_path": "/Users/wufagang/project/aiopen/CodeWiki/codewiki/cli/utils/errors.py",
    "relative_path": "codewiki/cli/utils/errors.py",
    "depends_on": [],
    "source_code": "def success(message: str):\n    \"\"\"Display a success message.\"\"\"\n    click.secho(f\"✓ {message}\", fg=\"green\")",
    "start_line": 105,
    "end_line": 107,
    "has_docstring": true,
    "docstring": "Display a success message.",
    "parameters": [
      "message"
    ],
    "node_type": "function",
    "base_classes": null,
    "class_name": null,
    "display_name": "function success",
    "component_id": "codewiki.cli.utils.errors.success"
  },
  "codewiki.cli.utils.errors.info": {
    "id": "codewiki.cli.utils.errors.info",
    "name": "info",
    "component_type": "function",
    "file_path": "/Users/wufagang/project/aiopen/CodeWiki/codewiki/cli/utils/errors.py",
    "relative_path": "codewiki/cli/utils/errors.py",
    "depends_on": [],
    "source_code": "def info(message: str):\n    \"\"\"Display an info message.\"\"\"\n    click.echo(message)",
    "start_line": 110,
    "end_line": 112,
    "has_docstring": true,
    "docstring": "Display an info message.",
    "parameters": [
      "message"
    ],
    "node_type": "function",
    "base_classes": null,
    "class_name": null,
    "display_name": "function info",
    "component_id": "codewiki.cli.utils.errors.info"
  },
  "codewiki.cli.utils.fs.ensure_directory": {
    "id": "codewiki.cli.utils.fs.ensure_directory",
    "name": "ensure_directory",
    "component_type": "function",
    "file_path": "/Users/wufagang/project/aiopen/CodeWiki/codewiki/cli/utils/fs.py",
    "relative_path": "codewiki/cli/utils/fs.py",
    "depends_on": [
      "codewiki.cli.utils.errors.FileSystemError"
    ],
    "source_code": "def ensure_directory(path: Path, mode: int = 0o700) -> Path:\n    \"\"\"\n    Ensure directory exists, create if necessary.\n    \n    Args:\n        path: Directory path\n        mode: Directory permissions (default: 0o700 - user only)\n        \n    Returns:\n        Path to the directory\n        \n    Raises:\n        FileSystemError: If directory cannot be created\n    \"\"\"\n    try:\n        path = Path(path).expanduser().resolve()\n        path.mkdir(parents=True, exist_ok=True, mode=mode)\n        return path\n    except PermissionError:\n        raise FileSystemError(\n            f\"Permission denied: Cannot create directory {path}\\n\"\n            f\"Try: chmod u+w {path.parent}\"\n        )\n    except OSError as e:\n        raise FileSystemError(f\"Cannot create directory {path}: {e}\")",
    "start_line": 13,
    "end_line": 37,
    "has_docstring": true,
    "docstring": "Ensure directory exists, create if necessary.\n\nArgs:\n    path: Directory path\n    mode: Directory permissions (default: 0o700 - user only)\n    \nReturns:\n    Path to the directory\n    \nRaises:\n    FileSystemError: If directory cannot be created",
    "parameters": [
      "path",
      "mode"
    ],
    "node_type": "function",
    "base_classes": null,
    "class_name": null,
    "display_name": "function ensure_directory",
    "component_id": "codewiki.cli.utils.fs.ensure_directory"
  },
  "codewiki.cli.utils.fs.check_writable": {
    "id": "codewiki.cli.utils.fs.check_writable",
    "name": "check_writable",
    "component_type": "function",
    "file_path": "/Users/wufagang/project/aiopen/CodeWiki/codewiki/cli/utils/fs.py",
    "relative_path": "codewiki/cli/utils/fs.py",
    "depends_on": [],
    "source_code": "def check_writable(path: Path) -> bool:\n    \"\"\"\n    Check if a path is writable.\n    \n    Args:\n        path: Path to check\n        \n    Returns:\n        True if writable, False otherwise\n    \"\"\"\n    path = Path(path).expanduser().resolve()\n    \n    if path.exists():\n        return os.access(path, os.W_OK)\n    else:\n        # Check parent directory if path doesn't exist\n        parent = path.parent\n        return parent.exists() and os.access(parent, os.W_OK)",
    "start_line": 40,
    "end_line": 57,
    "has_docstring": true,
    "docstring": "Check if a path is writable.\n\nArgs:\n    path: Path to check\n    \nReturns:\n    True if writable, False otherwise",
    "parameters": [
      "path"
    ],
    "node_type": "function",
    "base_classes": null,
    "class_name": null,
    "display_name": "function check_writable",
    "component_id": "codewiki.cli.utils.fs.check_writable"
  },
  "codewiki.cli.utils.fs.safe_write": {
    "id": "codewiki.cli.utils.fs.safe_write",
    "name": "safe_write",
    "component_type": "function",
    "file_path": "/Users/wufagang/project/aiopen/CodeWiki/codewiki/cli/utils/fs.py",
    "relative_path": "codewiki/cli/utils/fs.py",
    "depends_on": [
      "codewiki.cli.utils.errors.FileSystemError"
    ],
    "source_code": "def safe_write(path: Path, content: str, encoding: str = \"utf-8\"):\n    \"\"\"\n    Safely write content to a file using atomic write (temp file + rename).\n    \n    Args:\n        path: File path\n        content: Content to write\n        encoding: File encoding\n        \n    Raises:\n        FileSystemError: If write fails\n    \"\"\"\n    path = Path(path).expanduser().resolve()\n    temp_path = path.with_suffix(path.suffix + \".tmp\")\n    \n    try:\n        # Write to temp file\n        with open(temp_path, \"w\", encoding=encoding) as f:\n            f.write(content)\n        \n        # Atomic rename\n        temp_path.replace(path)\n    except Exception as e:\n        # Clean up temp file if it exists\n        if temp_path.exists():\n            temp_path.unlink()\n        raise FileSystemError(f\"Cannot write to {path}: {e}\")",
    "start_line": 60,
    "end_line": 86,
    "has_docstring": true,
    "docstring": "Safely write content to a file using atomic write (temp file + rename).\n\nArgs:\n    path: File path\n    content: Content to write\n    encoding: File encoding\n    \nRaises:\n    FileSystemError: If write fails",
    "parameters": [
      "path",
      "content",
      "encoding"
    ],
    "node_type": "function",
    "base_classes": null,
    "class_name": null,
    "display_name": "function safe_write",
    "component_id": "codewiki.cli.utils.fs.safe_write"
  },
  "codewiki.cli.utils.fs.safe_read": {
    "id": "codewiki.cli.utils.fs.safe_read",
    "name": "safe_read",
    "component_type": "function",
    "file_path": "/Users/wufagang/project/aiopen/CodeWiki/codewiki/cli/utils/fs.py",
    "relative_path": "codewiki/cli/utils/fs.py",
    "depends_on": [
      "codewiki.cli.utils.errors.FileSystemError"
    ],
    "source_code": "def safe_read(path: Path, encoding: str = \"utf-8\") -> str:\n    \"\"\"\n    Safely read content from a file.\n    \n    Args:\n        path: File path\n        encoding: File encoding\n        \n    Returns:\n        File content\n        \n    Raises:\n        FileSystemError: If read fails\n    \"\"\"\n    path = Path(path).expanduser().resolve()\n    \n    try:\n        with open(path, \"r\", encoding=encoding) as f:\n            return f.read()\n    except FileNotFoundError:\n        raise FileSystemError(f\"File not found: {path}\")\n    except PermissionError:\n        raise FileSystemError(f\"Permission denied: Cannot read {path}\")\n    except Exception as e:\n        raise FileSystemError(f\"Cannot read {path}: {e}\")",
    "start_line": 89,
    "end_line": 113,
    "has_docstring": true,
    "docstring": "Safely read content from a file.\n\nArgs:\n    path: File path\n    encoding: File encoding\n    \nReturns:\n    File content\n    \nRaises:\n    FileSystemError: If read fails",
    "parameters": [
      "path",
      "encoding"
    ],
    "node_type": "function",
    "base_classes": null,
    "class_name": null,
    "display_name": "function safe_read",
    "component_id": "codewiki.cli.utils.fs.safe_read"
  },
  "codewiki.cli.utils.fs.get_file_size": {
    "id": "codewiki.cli.utils.fs.get_file_size",
    "name": "get_file_size",
    "component_type": "function",
    "file_path": "/Users/wufagang/project/aiopen/CodeWiki/codewiki/cli/utils/fs.py",
    "relative_path": "codewiki/cli/utils/fs.py",
    "depends_on": [],
    "source_code": "def get_file_size(path: Path) -> int:\n    \"\"\"\n    Get file size in bytes.\n    \n    Args:\n        path: File path\n        \n    Returns:\n        File size in bytes\n    \"\"\"\n    return Path(path).stat().st_size",
    "start_line": 116,
    "end_line": 126,
    "has_docstring": true,
    "docstring": "Get file size in bytes.\n\nArgs:\n    path: File path\n    \nReturns:\n    File size in bytes",
    "parameters": [
      "path"
    ],
    "node_type": "function",
    "base_classes": null,
    "class_name": null,
    "display_name": "function get_file_size",
    "component_id": "codewiki.cli.utils.fs.get_file_size"
  },
  "codewiki.cli.utils.fs.find_files": {
    "id": "codewiki.cli.utils.fs.find_files",
    "name": "find_files",
    "component_type": "function",
    "file_path": "/Users/wufagang/project/aiopen/CodeWiki/codewiki/cli/utils/fs.py",
    "relative_path": "codewiki/cli/utils/fs.py",
    "depends_on": [],
    "source_code": "def find_files(\n    directory: Path,\n    extensions: Optional[List[str]] = None,\n    recursive: bool = True\n) -> List[Path]:\n    \"\"\"\n    Find files in directory matching extensions.\n    \n    Args:\n        directory: Directory to search\n        extensions: List of file extensions (e.g., ['.py', '.java'])\n        recursive: Search recursively\n        \n    Returns:\n        List of matching file paths\n    \"\"\"\n    directory = Path(directory).expanduser().resolve()\n    \n    if not directory.exists():\n        return []\n    \n    pattern = \"**/*\" if recursive else \"*\"\n    files = []\n    \n    for path in directory.glob(pattern):\n        if not path.is_file():\n            continue\n        \n        if extensions is None or path.suffix in extensions:\n            files.append(path)\n    \n    return files",
    "start_line": 129,
    "end_line": 160,
    "has_docstring": true,
    "docstring": "Find files in directory matching extensions.\n\nArgs:\n    directory: Directory to search\n    extensions: List of file extensions (e.g., ['.py', '.java'])\n    recursive: Search recursively\n    \nReturns:\n    List of matching file paths",
    "parameters": [
      "directory",
      "extensions",
      "recursive"
    ],
    "node_type": "function",
    "base_classes": null,
    "class_name": null,
    "display_name": "function find_files",
    "component_id": "codewiki.cli.utils.fs.find_files"
  },
  "codewiki.cli.utils.fs.cleanup_directory": {
    "id": "codewiki.cli.utils.fs.cleanup_directory",
    "name": "cleanup_directory",
    "component_type": "function",
    "file_path": "/Users/wufagang/project/aiopen/CodeWiki/codewiki/cli/utils/fs.py",
    "relative_path": "codewiki/cli/utils/fs.py",
    "depends_on": [
      "codewiki.cli.utils.errors.FileSystemError"
    ],
    "source_code": "def cleanup_directory(path: Path, keep_hidden: bool = True):\n    \"\"\"\n    Clean up a directory by removing its contents.\n    \n    Args:\n        path: Directory to clean\n        keep_hidden: Keep hidden files/directories (starting with .)\n        \n    Raises:\n        FileSystemError: If cleanup fails\n    \"\"\"\n    path = Path(path).expanduser().resolve()\n    \n    if not path.exists():\n        return\n    \n    try:\n        for item in path.iterdir():\n            if keep_hidden and item.name.startswith('.'):\n                continue\n            \n            if item.is_file():\n                item.unlink()\n            elif item.is_dir():\n                shutil.rmtree(item)\n    except Exception as e:\n        raise FileSystemError(f\"Cannot clean directory {path}: {e}\")",
    "start_line": 163,
    "end_line": 189,
    "has_docstring": true,
    "docstring": "Clean up a directory by removing its contents.\n\nArgs:\n    path: Directory to clean\n    keep_hidden: Keep hidden files/directories (starting with .)\n    \nRaises:\n    FileSystemError: If cleanup fails",
    "parameters": [
      "path",
      "keep_hidden"
    ],
    "node_type": "function",
    "base_classes": null,
    "class_name": null,
    "display_name": "function cleanup_directory",
    "component_id": "codewiki.cli.utils.fs.cleanup_directory"
  },
  "codewiki.cli.utils.instructions.compute_github_pages_url": {
    "id": "codewiki.cli.utils.instructions.compute_github_pages_url",
    "name": "compute_github_pages_url",
    "component_type": "function",
    "file_path": "/Users/wufagang/project/aiopen/CodeWiki/codewiki/cli/utils/instructions.py",
    "relative_path": "codewiki/cli/utils/instructions.py",
    "depends_on": [],
    "source_code": "def compute_github_pages_url(repo_url: str, repo_name: str) -> str:\n    \"\"\"\n    Compute expected GitHub Pages URL from repository URL.\n    \n    Args:\n        repo_url: GitHub repository URL\n        repo_name: Repository name\n        \n    Returns:\n        Expected GitHub Pages URL\n    \"\"\"\n    # Extract owner from GitHub URL\n    # e.g., \"https://github.com/owner/repo\" -> \"owner\"\n    if \"github.com\" in repo_url:\n        parts = repo_url.rstrip('/').split('/')\n        if len(parts) >= 2:\n            owner = parts[-2]\n            repo = parts[-1].replace('.git', '')\n            return f\"https://{owner}.github.io/{repo}/\"\n    \n    return f\"https://YOUR_USERNAME.github.io/{repo_name}/\"",
    "start_line": 10,
    "end_line": 30,
    "has_docstring": true,
    "docstring": "Compute expected GitHub Pages URL from repository URL.\n\nArgs:\n    repo_url: GitHub repository URL\n    repo_name: Repository name\n    \nReturns:\n    Expected GitHub Pages URL",
    "parameters": [
      "repo_url",
      "repo_name"
    ],
    "node_type": "function",
    "base_classes": null,
    "class_name": null,
    "display_name": "function compute_github_pages_url",
    "component_id": "codewiki.cli.utils.instructions.compute_github_pages_url"
  },
  "codewiki.cli.utils.instructions.get_pr_creation_url": {
    "id": "codewiki.cli.utils.instructions.get_pr_creation_url",
    "name": "get_pr_creation_url",
    "component_type": "function",
    "file_path": "/Users/wufagang/project/aiopen/CodeWiki/codewiki/cli/utils/instructions.py",
    "relative_path": "codewiki/cli/utils/instructions.py",
    "depends_on": [],
    "source_code": "def get_pr_creation_url(repo_url: str, branch_name: str) -> str:\n    \"\"\"\n    Get PR creation URL for GitHub.\n    \n    Args:\n        repo_url: GitHub repository URL\n        branch_name: Branch name\n        \n    Returns:\n        PR creation URL\n    \"\"\"\n    base_url = repo_url.rstrip('/').replace('.git', '')\n    return f\"{base_url}/compare/{branch_name}\"",
    "start_line": 33,
    "end_line": 45,
    "has_docstring": true,
    "docstring": "Get PR creation URL for GitHub.\n\nArgs:\n    repo_url: GitHub repository URL\n    branch_name: Branch name\n    \nReturns:\n    PR creation URL",
    "parameters": [
      "repo_url",
      "branch_name"
    ],
    "node_type": "function",
    "base_classes": null,
    "class_name": null,
    "display_name": "function get_pr_creation_url",
    "component_id": "codewiki.cli.utils.instructions.get_pr_creation_url"
  },
  "codewiki.cli.utils.instructions.display_post_generation_instructions": {
    "id": "codewiki.cli.utils.instructions.display_post_generation_instructions",
    "name": "display_post_generation_instructions",
    "component_type": "function",
    "file_path": "/Users/wufagang/project/aiopen/CodeWiki/codewiki/cli/utils/instructions.py",
    "relative_path": "codewiki/cli/utils/instructions.py",
    "depends_on": [
      "codewiki.cli.utils.instructions.compute_github_pages_url",
      "codewiki.cli.utils.instructions.get_pr_creation_url"
    ],
    "source_code": "def display_post_generation_instructions(\n    output_dir: Path,\n    repo_name: str,\n    repo_url: Optional[str] = None,\n    branch_name: Optional[str] = None,\n    github_pages: bool = False,\n    files_generated: list = None,\n    statistics: dict = None\n):\n    \"\"\"\n    Display post-generation instructions.\n    \n    Args:\n        output_dir: Output directory path\n        repo_name: Repository name\n        repo_url: GitHub repository URL (optional)\n        branch_name: Git branch name (optional)\n        github_pages: Whether GitHub Pages HTML was generated\n        files_generated: List of generated files\n        statistics: Generation statistics\n    \"\"\"\n    click.echo()\n    click.secho(\"✓ Documentation generated successfully!\", fg=\"green\", bold=True)\n    click.echo()\n    \n    # Output directory\n    click.secho(\"Output directory:\", fg=\"cyan\", bold=True)\n    click.echo(f\"  {output_dir}\")\n    click.echo()\n    \n    # Generated files\n    if files_generated:\n        click.secho(\"Generated files:\", fg=\"cyan\", bold=True)\n        for file in files_generated[:10]:  # Show first 10\n            click.echo(f\"  - {file}\")\n        if len(files_generated) > 10:\n            click.echo(f\"  ... and {len(files_generated) - 10} more\")\n        click.echo()\n    \n    # Statistics\n    if statistics:\n        click.secho(\"Statistics:\", fg=\"cyan\", bold=True)\n        if 'module_count' in statistics:\n            click.echo(f\"  Total modules:     {statistics['module_count']}\")\n        if 'total_files_analyzed' in statistics:\n            click.echo(f\"  Files analyzed:    {statistics['total_files_analyzed']}\")\n        if 'generation_time' in statistics:\n            minutes = int(statistics['generation_time'] // 60)\n            seconds = int(statistics['generation_time'] % 60)\n            click.echo(f\"  Generation time:   {minutes} minutes {seconds} seconds\")\n        # if 'total_tokens_used' in statistics:\n        #     tokens = statistics['total_tokens_used']\n        #     click.echo(f\"  Tokens used:       ~{tokens:,}\")\n        click.echo()\n    \n    # Next steps\n    click.secho(\"Next steps:\", fg=\"cyan\", bold=True)\n    click.echo()\n    \n    click.echo(\"1. Review the generated documentation:\")\n    click.echo(f\"   cat {output_dir}/overview.md\")\n    if github_pages:\n        click.echo(f\"   open {output_dir}/index.html  # View in browser\")\n    click.echo()\n    \n    if branch_name:\n        # Git workflow with branch\n        click.echo(\"2. Push the documentation branch:\")\n        click.secho(f\"   git push origin {branch_name}\", fg=\"yellow\")\n        click.echo()\n        \n        if repo_url:\n            pr_url = get_pr_creation_url(repo_url, branch_name)\n            click.echo(\"3. Create a Pull Request to merge documentation:\")\n            click.secho(f\"   {pr_url}\", fg=\"blue\")\n            click.echo()\n            \n            click.echo(\"4. After merge, enable GitHub Pages:\")\n        else:\n            click.echo(\"3. Enable GitHub Pages:\")\n    else:\n        # Direct commit workflow\n        click.echo(\"2. Commit the documentation:\")\n        click.secho(\"   git add docs/\", fg=\"yellow\")\n        click.secho('   git commit -m \"Add generated documentation\"', fg=\"yellow\")\n        click.echo()\n        \n        click.echo(\"3. Push to GitHub:\")\n        click.secho(\"   git push origin main\", fg=\"yellow\")\n        click.echo()\n        \n        click.echo(\"4. Enable GitHub Pages:\")\n    \n    click.echo(\"   - Go to repository Settings → Pages\")\n    click.echo(\"   - Source: Deploy from a branch\")\n    click.echo(\"   - Branch: main, folder: /docs\")\n    click.echo()\n    \n    if repo_url:\n        github_pages_url = compute_github_pages_url(repo_url, repo_name)\n        click.echo(\"5. Your documentation will be available at:\")\n        click.secho(f\"   {github_pages_url}\", fg=\"blue\", bold=True)\n        click.echo()",
    "start_line": 48,
    "end_line": 150,
    "has_docstring": true,
    "docstring": "Display post-generation instructions.\n\nArgs:\n    output_dir: Output directory path\n    repo_name: Repository name\n    repo_url: GitHub repository URL (optional)\n    branch_name: Git branch name (optional)\n    github_pages: Whether GitHub Pages HTML was generated\n    files_generated: List of generated files\n    statistics: Generation statistics",
    "parameters": [
      "output_dir",
      "repo_name",
      "repo_url",
      "branch_name",
      "github_pages",
      "files_generated",
      "statistics"
    ],
    "node_type": "function",
    "base_classes": null,
    "class_name": null,
    "display_name": "function display_post_generation_instructions",
    "component_id": "codewiki.cli.utils.instructions.display_post_generation_instructions"
  },
  "codewiki.cli.utils.instructions.display_generation_summary": {
    "id": "codewiki.cli.utils.instructions.display_generation_summary",
    "name": "display_generation_summary",
    "component_type": "function",
    "file_path": "/Users/wufagang/project/aiopen/CodeWiki/codewiki/cli/utils/instructions.py",
    "relative_path": "codewiki/cli/utils/instructions.py",
    "depends_on": [],
    "source_code": "def display_generation_summary(\n    success: bool,\n    error_message: Optional[str] = None,\n    output_dir: Optional[Path] = None\n):\n    \"\"\"\n    Display generation summary (success or failure).\n    \n    Args:\n        success: Whether generation was successful\n        error_message: Error message if failed\n        output_dir: Output directory if successful\n    \"\"\"\n    if success:\n        click.echo()\n        click.secho(\"✓ Generation completed successfully!\", fg=\"green\", bold=True)\n        if output_dir:\n            click.echo(f\"\\nDocumentation saved to: {output_dir}\")\n        click.echo()\n    else:\n        click.echo()\n        click.secho(\"✗ Generation failed\", fg=\"red\", bold=True)\n        if error_message:\n            click.echo()\n            click.echo(error_message)\n        click.echo()",
    "start_line": 153,
    "end_line": 178,
    "has_docstring": true,
    "docstring": "Display generation summary (success or failure).\n\nArgs:\n    success: Whether generation was successful\n    error_message: Error message if failed\n    output_dir: Output directory if successful",
    "parameters": [
      "success",
      "error_message",
      "output_dir"
    ],
    "node_type": "function",
    "base_classes": null,
    "class_name": null,
    "display_name": "function display_generation_summary",
    "component_id": "codewiki.cli.utils.instructions.display_generation_summary"
  },
  "codewiki.cli.utils.logging.CLILogger": {
    "id": "codewiki.cli.utils.logging.CLILogger",
    "name": "CLILogger",
    "component_type": "class",
    "file_path": "/Users/wufagang/project/aiopen/CodeWiki/codewiki/cli/utils/logging.py",
    "relative_path": "codewiki/cli/utils/logging.py",
    "depends_on": [],
    "source_code": "class CLILogger:\n    \"\"\"Logger for CLI with support for verbose and normal modes.\"\"\"\n    \n    def __init__(self, verbose: bool = False):\n        \"\"\"\n        Initialize the logger.\n        \n        Args:\n            verbose: Enable verbose output\n        \"\"\"\n        self.verbose = verbose\n        self.start_time = datetime.now()\n    \n    def debug(self, message: str):\n        \"\"\"Log debug message (only in verbose mode).\"\"\"\n        if self.verbose:\n            timestamp = datetime.now().strftime(\"%H:%M:%S\")\n            click.secho(f\"[{timestamp}] {message}\", fg=\"cyan\", dim=True)\n    \n    def info(self, message: str):\n        \"\"\"Log info message.\"\"\"\n        click.echo(message)\n    \n    def success(self, message: str):\n        \"\"\"Log success message in green.\"\"\"\n        click.secho(f\"✓ {message}\", fg=\"green\")\n    \n    def warning(self, message: str):\n        \"\"\"Log warning message in yellow.\"\"\"\n        click.secho(f\"⚠️  {message}\", fg=\"yellow\")\n    \n    def error(self, message: str):\n        \"\"\"Log error message in red.\"\"\"\n        click.secho(f\"✗ {message}\", fg=\"red\", err=True)\n    \n    def step(self, message: str, step: Optional[int] = None, total: Optional[int] = None):\n        \"\"\"\n        Log a processing step.\n        \n        Args:\n            message: Step description\n            step: Current step number\n            total: Total number of steps\n        \"\"\"\n        if step is not None and total is not None:\n            prefix = f\"[{step}/{total}]\"\n        else:\n            prefix = \"→\"\n        \n        click.secho(f\"{prefix} {message}\", fg=\"blue\", bold=True)\n    \n    def elapsed_time(self) -> str:\n        \"\"\"Get elapsed time since logger was created.\"\"\"\n        elapsed = datetime.now() - self.start_time\n        minutes = int(elapsed.total_seconds() // 60)\n        seconds = int(elapsed.total_seconds() % 60)\n        \n        if minutes > 0:\n            return f\"{minutes}m {seconds}s\"\n        else:\n            return f\"{seconds}s\"",
    "start_line": 11,
    "end_line": 71,
    "has_docstring": true,
    "docstring": "Logger for CLI with support for verbose and normal modes.",
    "parameters": null,
    "node_type": "class",
    "base_classes": null,
    "class_name": null,
    "display_name": "class CLILogger",
    "component_id": "codewiki.cli.utils.logging.CLILogger"
  },
  "codewiki.cli.utils.logging.create_logger": {
    "id": "codewiki.cli.utils.logging.create_logger",
    "name": "create_logger",
    "component_type": "function",
    "file_path": "/Users/wufagang/project/aiopen/CodeWiki/codewiki/cli/utils/logging.py",
    "relative_path": "codewiki/cli/utils/logging.py",
    "depends_on": [
      "codewiki.cli.utils.logging.CLILogger"
    ],
    "source_code": "def create_logger(verbose: bool = False) -> CLILogger:\n    \"\"\"\n    Create and return a CLI logger.\n    \n    Args:\n        verbose: Enable verbose output\n        \n    Returns:\n        Configured CLILogger instance\n    \"\"\"\n    return CLILogger(verbose=verbose)",
    "start_line": 74,
    "end_line": 84,
    "has_docstring": true,
    "docstring": "Create and return a CLI logger.\n\nArgs:\n    verbose: Enable verbose output\n    \nReturns:\n    Configured CLILogger instance",
    "parameters": [
      "verbose"
    ],
    "node_type": "function",
    "base_classes": null,
    "class_name": null,
    "display_name": "function create_logger",
    "component_id": "codewiki.cli.utils.logging.create_logger"
  },
  "codewiki.cli.utils.progress.ProgressTracker": {
    "id": "codewiki.cli.utils.progress.ProgressTracker",
    "name": "ProgressTracker",
    "component_type": "class",
    "file_path": "/Users/wufagang/project/aiopen/CodeWiki/codewiki/cli/utils/progress.py",
    "relative_path": "codewiki/cli/utils/progress.py",
    "depends_on": [],
    "source_code": "class ProgressTracker:\n    \"\"\"\n    Progress tracker with stages and ETA estimation.\n    \n    Stages:\n    1. Dependency Analysis (40% of time)\n    2. Module Clustering (20% of time)\n    3. Documentation Generation (30% of time)\n    4. HTML Generation (5% of time, optional)\n    5. Finalization (5% of time)\n    \"\"\"\n    \n    # Stage weights (percentage of total time)\n    STAGE_WEIGHTS = {\n        1: 0.40,  # Dependency Analysis\n        2: 0.20,  # Module Clustering\n        3: 0.30,  # Documentation Generation\n        4: 0.05,  # HTML Generation (optional)\n        5: 0.05,  # Finalization\n    }\n    \n    STAGE_NAMES = {\n        1: \"Dependency Analysis\",\n        2: \"Module Clustering\",\n        3: \"Documentation Generation\",\n        4: \"HTML Generation\",\n        5: \"Finalization\",\n    }\n    \n    def __init__(self, total_stages: int = 5, verbose: bool = False):\n        \"\"\"\n        Initialize progress tracker.\n        \n        Args:\n            total_stages: Number of stages\n            verbose: Enable verbose output\n        \"\"\"\n        self.total_stages = total_stages\n        self.current_stage = 0\n        self.stage_progress = 0.0\n        self.start_time = time.time()\n        self.verbose = verbose\n        self.current_stage_start = self.start_time\n    \n    def start_stage(self, stage: int, description: Optional[str] = None):\n        \"\"\"\n        Start a new stage.\n        \n        Args:\n            stage: Stage number (1-5)\n            description: Optional custom description\n        \"\"\"\n        self.current_stage = stage\n        self.stage_progress = 0.0\n        self.current_stage_start = time.time()\n        \n        stage_name = description or self.STAGE_NAMES.get(stage, f\"Stage {stage}\")\n        \n        if self.verbose:\n            elapsed = self._format_elapsed()\n            click.secho(\n                f\"\\n[{elapsed}] Phase {stage}/{self.total_stages}: {stage_name}\",\n                fg=\"blue\",\n                bold=True\n            )\n        else:\n            click.secho(\n                f\"[{stage}/{self.total_stages}] {stage_name}\",\n                fg=\"blue\",\n                bold=True\n            )\n    \n    def update_stage(self, progress: float, message: Optional[str] = None):\n        \"\"\"\n        Update progress within current stage.\n        \n        Args:\n            progress: Progress percentage (0.0 to 1.0)\n            message: Optional progress message\n        \"\"\"\n        self.stage_progress = min(1.0, max(0.0, progress))\n        \n        if self.verbose and message:\n            elapsed = self._format_elapsed()\n            click.echo(f\"[{elapsed}]   {message}\")\n    \n    def complete_stage(self, message: Optional[str] = None):\n        \"\"\"\n        Complete current stage.\n        \n        Args:\n            message: Optional completion message\n        \"\"\"\n        self.stage_progress = 1.0\n        \n        if self.verbose:\n            elapsed = self._format_elapsed()\n            stage_time = time.time() - self.current_stage_start\n            stage_name = self.STAGE_NAMES.get(self.current_stage, f\"Stage {self.current_stage}\")\n            click.secho(\n                f\"[{elapsed}]   {stage_name} complete ({stage_time:.1f}s)\",\n                fg=\"green\"\n            )\n            if message:\n                click.echo(f\"[{elapsed}]   {message}\")\n    \n    def get_overall_progress(self) -> float:\n        \"\"\"\n        Get overall progress percentage.\n        \n        Returns:\n            Progress (0.0 to 1.0)\n        \"\"\"\n        completed_weight = sum(\n            self.STAGE_WEIGHTS.get(s, 0)\n            for s in range(1, self.current_stage)\n        )\n        \n        current_weight = self.STAGE_WEIGHTS.get(self.current_stage, 0) * self.stage_progress\n        \n        return completed_weight + current_weight\n    \n    def _format_elapsed(self) -> str:\n        \"\"\"Format elapsed time.\"\"\"\n        elapsed = time.time() - self.start_time\n        minutes = int(elapsed // 60)\n        seconds = int(elapsed % 60)\n        \n        if minutes > 0:\n            return f\"{minutes:02d}:{seconds:02d}\"\n        else:\n            return f\"00:{seconds:02d}\"\n    \n    def get_eta(self) -> Optional[str]:\n        \"\"\"\n        Estimate time remaining.\n        \n        Returns:\n            ETA string or None if cannot estimate\n        \"\"\"\n        elapsed = time.time() - self.start_time\n        progress = self.get_overall_progress()\n        \n        if progress <= 0.0:\n            return None\n        \n        total_estimated = elapsed / progress\n        remaining = total_estimated - elapsed\n        \n        if remaining < 0:\n            return \"< 1 min\"\n        \n        minutes = int(remaining // 60)\n        seconds = int(remaining % 60)\n        \n        if minutes > 60:\n            hours = minutes // 60\n            minutes = minutes % 60\n            return f\"{hours}h {minutes}m\"\n        elif minutes > 0:\n            return f\"{minutes}m {seconds}s\"\n        else:\n            return f\"{seconds}s\"",
    "start_line": 11,
    "end_line": 173,
    "has_docstring": true,
    "docstring": "Progress tracker with stages and ETA estimation.\n\nStages:\n1. Dependency Analysis (40% of time)\n2. Module Clustering (20% of time)\n3. Documentation Generation (30% of time)\n4. HTML Generation (5% of time, optional)\n5. Finalization (5% of time)",
    "parameters": null,
    "node_type": "class",
    "base_classes": null,
    "class_name": null,
    "display_name": "class ProgressTracker",
    "component_id": "codewiki.cli.utils.progress.ProgressTracker"
  },
  "codewiki.cli.utils.progress.ModuleProgressBar": {
    "id": "codewiki.cli.utils.progress.ModuleProgressBar",
    "name": "ModuleProgressBar",
    "component_type": "class",
    "file_path": "/Users/wufagang/project/aiopen/CodeWiki/codewiki/cli/utils/progress.py",
    "relative_path": "codewiki/cli/utils/progress.py",
    "depends_on": [],
    "source_code": "class ModuleProgressBar:\n    \"\"\"Progress bar for module-by-module generation.\"\"\"\n    \n    def __init__(self, total_modules: int, verbose: bool = False):\n        \"\"\"\n        Initialize module progress bar.\n        \n        Args:\n            total_modules: Total number of modules to process\n            verbose: Enable verbose output\n        \"\"\"\n        self.total_modules = total_modules\n        self.current_module = 0\n        self.verbose = verbose\n        self.bar = None\n        \n        if not verbose:\n            self.bar = click.progressbar(\n                length=total_modules,\n                label=\"Generating modules\",\n                show_eta=True,\n                show_percent=True,\n            )\n            self.bar.__enter__()\n    \n    def update(self, module_name: str, cached: bool = False):\n        \"\"\"\n        Update progress for a module.\n        \n        Args:\n            module_name: Name of the module\n            cached: Whether the module was loaded from cache\n        \"\"\"\n        self.current_module += 1\n        \n        if self.verbose:\n            status = \"✓ (cached)\" if cached else \"⟳ (generating)\"\n            click.echo(f\"  [{self.current_module}/{self.total_modules}] {module_name}... {status}\")\n        elif self.bar:\n            self.bar.update(1)\n    \n    def finish(self):\n        \"\"\"Finish progress bar.\"\"\"\n        if self.bar:\n            self.bar.__exit__(None, None, None)\n            self.bar = None",
    "start_line": 176,
    "end_line": 221,
    "has_docstring": true,
    "docstring": "Progress bar for module-by-module generation.",
    "parameters": null,
    "node_type": "class",
    "base_classes": null,
    "class_name": null,
    "display_name": "class ModuleProgressBar",
    "component_id": "codewiki.cli.utils.progress.ModuleProgressBar"
  },
  "codewiki.cli.utils.repo_validator.validate_repository": {
    "id": "codewiki.cli.utils.repo_validator.validate_repository",
    "name": "validate_repository",
    "component_type": "function",
    "file_path": "/Users/wufagang/project/aiopen/CodeWiki/codewiki/cli/utils/repo_validator.py",
    "relative_path": "codewiki/cli/utils/repo_validator.py",
    "depends_on": [
      "codewiki.cli.utils.validation.validate_repository_path",
      "codewiki.cli.utils.errors.RepositoryError",
      "codewiki.cli.utils.validation.detect_supported_languages"
    ],
    "source_code": "def validate_repository(repo_path: Path) -> Tuple[Path, List[Tuple[str, int]]]:\n    \"\"\"\n    Validate repository for documentation generation.\n    \n    Checks:\n    - Path exists and is a directory\n    - Contains supported code files\n    - Has sufficient files for meaningful documentation\n    \n    Args:\n        repo_path: Path to repository\n        \n    Returns:\n        Tuple of (validated_path, language_counts)\n        \n    Raises:\n        RepositoryError: If validation fails\n    \"\"\"\n    # Validate path exists\n    repo_path = validate_repository_path(repo_path)\n    \n    # Detect languages\n    languages = detect_supported_languages(repo_path)\n    \n    if not languages:\n        raise RepositoryError(\n            f\"No supported code files found in {repo_path}\\n\\n\"\n            \"CodeWiki supports: Python, Java, JavaScript, TypeScript, C, C++, C#, PHP\\n\\n\"\n            \"Please navigate to a code repository or specify a custom directory:\\n\"\n            \"  cd /path/to/your/project\\n\"\n            \"  codewiki generate\"\n        )\n    \n    return repo_path, languages",
    "start_line": 36,
    "end_line": 69,
    "has_docstring": true,
    "docstring": "Validate repository for documentation generation.\n\nChecks:\n- Path exists and is a directory\n- Contains supported code files\n- Has sufficient files for meaningful documentation\n\nArgs:\n    repo_path: Path to repository\n    \nReturns:\n    Tuple of (validated_path, language_counts)\n    \nRaises:\n    RepositoryError: If validation fails",
    "parameters": [
      "repo_path"
    ],
    "node_type": "function",
    "base_classes": null,
    "class_name": null,
    "display_name": "function validate_repository",
    "component_id": "codewiki.cli.utils.repo_validator.validate_repository"
  },
  "codewiki.cli.utils.repo_validator.check_writable_output": {
    "id": "codewiki.cli.utils.repo_validator.check_writable_output",
    "name": "check_writable_output",
    "component_type": "function",
    "file_path": "/Users/wufagang/project/aiopen/CodeWiki/codewiki/cli/utils/repo_validator.py",
    "relative_path": "codewiki/cli/utils/repo_validator.py",
    "depends_on": [
      "codewiki.cli.utils.errors.RepositoryError"
    ],
    "source_code": "def check_writable_output(output_dir: Path) -> Path:\n    \"\"\"\n    Check if output directory is writable.\n    \n    Args:\n        output_dir: Output directory path\n        \n    Returns:\n        Validated output directory path\n        \n    Raises:\n        RepositoryError: If output directory is not writable\n    \"\"\"\n    output_dir = Path(output_dir).expanduser().resolve()\n    \n    # Check if output directory exists\n    if output_dir.exists():\n        if not output_dir.is_dir():\n            raise RepositoryError(\n                f\"Output path exists but is not a directory: {output_dir}\"\n            )\n        \n        # Check if writable\n        if not os.access(output_dir, os.W_OK):\n            raise RepositoryError(\n                f\"Output directory is not writable: {output_dir}\\n\\n\"\n                f\"Try: chmod u+w {output_dir}\"\n            )\n    else:\n        # Check if parent is writable\n        parent = output_dir.parent\n        if not parent.exists():\n            raise RepositoryError(\n                f\"Parent directory does not exist: {parent}\"\n            )\n        \n        if not os.access(parent, os.W_OK):\n            raise RepositoryError(\n                f\"Cannot create output directory (parent not writable): {parent}\\n\\n\"\n                f\"Try: chmod u+w {parent}\"\n            )\n    \n    return output_dir",
    "start_line": 72,
    "end_line": 114,
    "has_docstring": true,
    "docstring": "Check if output directory is writable.\n\nArgs:\n    output_dir: Output directory path\n    \nReturns:\n    Validated output directory path\n    \nRaises:\n    RepositoryError: If output directory is not writable",
    "parameters": [
      "output_dir"
    ],
    "node_type": "function",
    "base_classes": null,
    "class_name": null,
    "display_name": "function check_writable_output",
    "component_id": "codewiki.cli.utils.repo_validator.check_writable_output"
  },
  "codewiki.cli.utils.repo_validator.is_git_repository": {
    "id": "codewiki.cli.utils.repo_validator.is_git_repository",
    "name": "is_git_repository",
    "component_type": "function",
    "file_path": "/Users/wufagang/project/aiopen/CodeWiki/codewiki/cli/utils/repo_validator.py",
    "relative_path": "codewiki/cli/utils/repo_validator.py",
    "depends_on": [],
    "source_code": "def is_git_repository(repo_path: Path) -> bool:\n    \"\"\"\n    Check if path is a git repository.\n    \n    Args:\n        repo_path: Path to check\n        \n    Returns:\n        True if git repository, False otherwise\n    \"\"\"\n    git_dir = repo_path / \".git\"\n    return git_dir.exists() and git_dir.is_dir()",
    "start_line": 117,
    "end_line": 128,
    "has_docstring": true,
    "docstring": "Check if path is a git repository.\n\nArgs:\n    repo_path: Path to check\n    \nReturns:\n    True if git repository, False otherwise",
    "parameters": [
      "repo_path"
    ],
    "node_type": "function",
    "base_classes": null,
    "class_name": null,
    "display_name": "function is_git_repository",
    "component_id": "codewiki.cli.utils.repo_validator.is_git_repository"
  },
  "codewiki.cli.utils.repo_validator.get_git_commit_hash": {
    "id": "codewiki.cli.utils.repo_validator.get_git_commit_hash",
    "name": "get_git_commit_hash",
    "component_type": "function",
    "file_path": "/Users/wufagang/project/aiopen/CodeWiki/codewiki/cli/utils/repo_validator.py",
    "relative_path": "codewiki/cli/utils/repo_validator.py",
    "depends_on": [
      "codewiki.cli.utils.repo_validator.is_git_repository"
    ],
    "source_code": "def get_git_commit_hash(repo_path: Path) -> str:\n    \"\"\"\n    Get current git commit hash.\n    \n    Args:\n        repo_path: Repository path\n        \n    Returns:\n        Commit hash or empty string if not a git repo\n    \"\"\"\n    if not is_git_repository(repo_path):\n        return \"\"\n    \n    try:\n        import git\n        repo = git.Repo(repo_path)\n        return repo.head.commit.hexsha\n    except Exception:\n        return \"\"",
    "start_line": 131,
    "end_line": 149,
    "has_docstring": true,
    "docstring": "Get current git commit hash.\n\nArgs:\n    repo_path: Repository path\n    \nReturns:\n    Commit hash or empty string if not a git repo",
    "parameters": [
      "repo_path"
    ],
    "node_type": "function",
    "base_classes": null,
    "class_name": null,
    "display_name": "function get_git_commit_hash",
    "component_id": "codewiki.cli.utils.repo_validator.get_git_commit_hash"
  },
  "codewiki.cli.utils.repo_validator.get_git_branch": {
    "id": "codewiki.cli.utils.repo_validator.get_git_branch",
    "name": "get_git_branch",
    "component_type": "function",
    "file_path": "/Users/wufagang/project/aiopen/CodeWiki/codewiki/cli/utils/repo_validator.py",
    "relative_path": "codewiki/cli/utils/repo_validator.py",
    "depends_on": [
      "codewiki.cli.utils.repo_validator.is_git_repository"
    ],
    "source_code": "def get_git_branch(repo_path: Path) -> str:\n    \"\"\"\n    Get current git branch name.\n    \n    Args:\n        repo_path: Repository path\n        \n    Returns:\n        Branch name or empty string if not a git repo\n    \"\"\"\n    if not is_git_repository(repo_path):\n        return \"\"\n    \n    try:\n        import git\n        repo = git.Repo(repo_path)\n        return repo.active_branch.name\n    except Exception:\n        return \"\"",
    "start_line": 152,
    "end_line": 170,
    "has_docstring": true,
    "docstring": "Get current git branch name.\n\nArgs:\n    repo_path: Repository path\n    \nReturns:\n    Branch name or empty string if not a git repo",
    "parameters": [
      "repo_path"
    ],
    "node_type": "function",
    "base_classes": null,
    "class_name": null,
    "display_name": "function get_git_branch",
    "component_id": "codewiki.cli.utils.repo_validator.get_git_branch"
  },
  "codewiki.cli.utils.repo_validator.count_code_files": {
    "id": "codewiki.cli.utils.repo_validator.count_code_files",
    "name": "count_code_files",
    "component_type": "function",
    "file_path": "/Users/wufagang/project/aiopen/CodeWiki/codewiki/cli/utils/repo_validator.py",
    "relative_path": "codewiki/cli/utils/repo_validator.py",
    "depends_on": [],
    "source_code": "def count_code_files(repo_path: Path) -> int:\n    \"\"\"\n    Count supported code files in repository.\n    \n    Args:\n        repo_path: Repository path\n        \n    Returns:\n        Number of code files\n    \"\"\"\n    count = 0\n    for ext in SUPPORTED_EXTENSIONS:\n        count += len(list(repo_path.rglob(f\"*{ext}\")))\n    return count",
    "start_line": 173,
    "end_line": 186,
    "has_docstring": true,
    "docstring": "Count supported code files in repository.\n\nArgs:\n    repo_path: Repository path\n    \nReturns:\n    Number of code files",
    "parameters": [
      "repo_path"
    ],
    "node_type": "function",
    "base_classes": null,
    "class_name": null,
    "display_name": "function count_code_files",
    "component_id": "codewiki.cli.utils.repo_validator.count_code_files"
  },
  "codewiki.cli.utils.validation.validate_url": {
    "id": "codewiki.cli.utils.validation.validate_url",
    "name": "validate_url",
    "component_type": "function",
    "file_path": "/Users/wufagang/project/aiopen/CodeWiki/codewiki/cli/utils/validation.py",
    "relative_path": "codewiki/cli/utils/validation.py",
    "depends_on": [
      "codewiki.cli.utils.errors.ConfigurationError"
    ],
    "source_code": "def validate_url(url: str, require_https: bool = False, allow_localhost: bool = True) -> str:\n    \"\"\"\n    Validate URL format.\n    \n    Args:\n        url: URL to validate\n        require_https: Require HTTPS scheme (except localhost)\n        allow_localhost: Allow localhost URLs\n        \n    Returns:\n        Validated URL\n        \n    Raises:\n        ConfigurationError: If URL is invalid\n    \"\"\"\n    try:\n        parsed = urlparse(url)\n        \n        # Check scheme\n        if not parsed.scheme:\n            raise ConfigurationError(f\"Invalid URL (missing scheme): {url}\")\n        \n        # Check HTTPS requirement\n        if require_https and parsed.scheme != 'https':\n            # Allow HTTP for localhost\n            if allow_localhost and parsed.hostname in ['localhost', '127.0.0.1', '::1']:\n                pass\n            else:\n                raise ConfigurationError(\n                    f\"URL must use HTTPS: {url}\\n\"\n                    f\"HTTP is only allowed for localhost\"\n                )\n        \n        # Check hostname\n        if not parsed.hostname:\n            raise ConfigurationError(f\"Invalid URL (missing hostname): {url}\")\n        \n        return url\n    except ValueError as e:\n        raise ConfigurationError(f\"Invalid URL format: {url}\\nError: {e}\")",
    "start_line": 13,
    "end_line": 52,
    "has_docstring": true,
    "docstring": "Validate URL format.\n\nArgs:\n    url: URL to validate\n    require_https: Require HTTPS scheme (except localhost)\n    allow_localhost: Allow localhost URLs\n    \nReturns:\n    Validated URL\n    \nRaises:\n    ConfigurationError: If URL is invalid",
    "parameters": [
      "url",
      "require_https",
      "allow_localhost"
    ],
    "node_type": "function",
    "base_classes": null,
    "class_name": null,
    "display_name": "function validate_url",
    "component_id": "codewiki.cli.utils.validation.validate_url"
  },
  "codewiki.cli.utils.validation.validate_api_key": {
    "id": "codewiki.cli.utils.validation.validate_api_key",
    "name": "validate_api_key",
    "component_type": "function",
    "file_path": "/Users/wufagang/project/aiopen/CodeWiki/codewiki/cli/utils/validation.py",
    "relative_path": "codewiki/cli/utils/validation.py",
    "depends_on": [
      "codewiki.cli.utils.errors.ConfigurationError"
    ],
    "source_code": "def validate_api_key(api_key: str, min_length: int = 10) -> str:\n    \"\"\"\n    Validate API key format.\n    \n    Args:\n        api_key: API key to validate\n        min_length: Minimum key length\n        \n    Returns:\n        Validated API key\n        \n    Raises:\n        ConfigurationError: If API key is invalid\n    \"\"\"\n    if not api_key or not api_key.strip():\n        raise ConfigurationError(\"API key cannot be empty\")\n    \n    api_key = api_key.strip()\n    \n    if len(api_key) < min_length:\n        raise ConfigurationError(\n            f\"API key too short (minimum {min_length} characters)\"\n        )\n    \n    return api_key",
    "start_line": 55,
    "end_line": 79,
    "has_docstring": true,
    "docstring": "Validate API key format.\n\nArgs:\n    api_key: API key to validate\n    min_length: Minimum key length\n    \nReturns:\n    Validated API key\n    \nRaises:\n    ConfigurationError: If API key is invalid",
    "parameters": [
      "api_key",
      "min_length"
    ],
    "node_type": "function",
    "base_classes": null,
    "class_name": null,
    "display_name": "function validate_api_key",
    "component_id": "codewiki.cli.utils.validation.validate_api_key"
  },
  "codewiki.cli.utils.validation.validate_model_name": {
    "id": "codewiki.cli.utils.validation.validate_model_name",
    "name": "validate_model_name",
    "component_type": "function",
    "file_path": "/Users/wufagang/project/aiopen/CodeWiki/codewiki/cli/utils/validation.py",
    "relative_path": "codewiki/cli/utils/validation.py",
    "depends_on": [
      "codewiki.cli.utils.errors.ConfigurationError"
    ],
    "source_code": "def validate_model_name(model: str) -> str:\n    \"\"\"\n    Validate model name format.\n    \n    Args:\n        model: Model name to validate\n        \n    Returns:\n        Validated model name\n        \n    Raises:\n        ConfigurationError: If model name is invalid\n    \"\"\"\n    if not model or not model.strip():\n        raise ConfigurationError(\"Model name cannot be empty\")\n    \n    return model.strip()",
    "start_line": 82,
    "end_line": 98,
    "has_docstring": true,
    "docstring": "Validate model name format.\n\nArgs:\n    model: Model name to validate\n    \nReturns:\n    Validated model name\n    \nRaises:\n    ConfigurationError: If model name is invalid",
    "parameters": [
      "model"
    ],
    "node_type": "function",
    "base_classes": null,
    "class_name": null,
    "display_name": "function validate_model_name",
    "component_id": "codewiki.cli.utils.validation.validate_model_name"
  },
  "codewiki.cli.utils.validation.validate_output_directory": {
    "id": "codewiki.cli.utils.validation.validate_output_directory",
    "name": "validate_output_directory",
    "component_type": "function",
    "file_path": "/Users/wufagang/project/aiopen/CodeWiki/codewiki/cli/utils/validation.py",
    "relative_path": "codewiki/cli/utils/validation.py",
    "depends_on": [
      "codewiki.cli.utils.errors.ConfigurationError"
    ],
    "source_code": "def validate_output_directory(path: str) -> Path:\n    \"\"\"\n    Validate output directory path.\n    \n    Args:\n        path: Directory path to validate\n        \n    Returns:\n        Validated Path object\n        \n    Raises:\n        ConfigurationError: If path is invalid\n    \"\"\"\n    if not path or not path.strip():\n        raise ConfigurationError(\"Output directory cannot be empty\")\n    \n    try:\n        resolved_path = Path(path).expanduser().resolve()\n        \n        # Check if path is writable (or parent is writable if path doesn't exist)\n        if resolved_path.exists():\n            if not resolved_path.is_dir():\n                raise ConfigurationError(\n                    f\"Output path exists but is not a directory: {path}\"\n                )\n        \n        return resolved_path\n    except Exception as e:\n        raise ConfigurationError(f\"Invalid output directory path: {path}\\nError: {e}\")",
    "start_line": 101,
    "end_line": 129,
    "has_docstring": true,
    "docstring": "Validate output directory path.\n\nArgs:\n    path: Directory path to validate\n    \nReturns:\n    Validated Path object\n    \nRaises:\n    ConfigurationError: If path is invalid",
    "parameters": [
      "path"
    ],
    "node_type": "function",
    "base_classes": null,
    "class_name": null,
    "display_name": "function validate_output_directory",
    "component_id": "codewiki.cli.utils.validation.validate_output_directory"
  },
  "codewiki.cli.utils.validation.validate_repository_path": {
    "id": "codewiki.cli.utils.validation.validate_repository_path",
    "name": "validate_repository_path",
    "component_type": "function",
    "file_path": "/Users/wufagang/project/aiopen/CodeWiki/codewiki/cli/utils/validation.py",
    "relative_path": "codewiki/cli/utils/validation.py",
    "depends_on": [
      "codewiki.cli.utils.errors.RepositoryError"
    ],
    "source_code": "def validate_repository_path(path: Path) -> Path:\n    \"\"\"\n    Validate repository path exists and contains code files.\n    \n    Args:\n        path: Repository path to validate\n        \n    Returns:\n        Validated Path object\n        \n    Raises:\n        RepositoryError: If repository is invalid\n    \"\"\"\n    path = Path(path).expanduser().resolve()\n    \n    if not path.exists():\n        raise RepositoryError(f\"Repository path does not exist: {path}\")\n    \n    if not path.is_dir():\n        raise RepositoryError(f\"Repository path is not a directory: {path}\")\n    \n    return path",
    "start_line": 132,
    "end_line": 153,
    "has_docstring": true,
    "docstring": "Validate repository path exists and contains code files.\n\nArgs:\n    path: Repository path to validate\n    \nReturns:\n    Validated Path object\n    \nRaises:\n    RepositoryError: If repository is invalid",
    "parameters": [
      "path"
    ],
    "node_type": "function",
    "base_classes": null,
    "class_name": null,
    "display_name": "function validate_repository_path",
    "component_id": "codewiki.cli.utils.validation.validate_repository_path"
  },
  "codewiki.cli.utils.validation.detect_supported_languages": {
    "id": "codewiki.cli.utils.validation.detect_supported_languages",
    "name": "detect_supported_languages",
    "component_type": "function",
    "file_path": "/Users/wufagang/project/aiopen/CodeWiki/codewiki/cli/utils/validation.py",
    "relative_path": "codewiki/cli/utils/validation.py",
    "depends_on": [],
    "source_code": "def detect_supported_languages(directory: Path) -> List[Tuple[str, int]]:\n    \"\"\"\n    Detect supported programming languages in a directory.\n    \n    Args:\n        directory: Directory to scan\n        \n    Returns:\n        List of (language, file_count) tuples\n    \"\"\"\n    language_extensions = {\n        'Python': ['.py'],\n        'Java': ['.java'],\n        'JavaScript': ['.js', '.jsx'],\n        'TypeScript': ['.ts', '.tsx'],\n        'C': ['.c', '.h'],\n        'C++': ['.cpp', '.hpp', '.cc', '.hh', '.cxx', '.hxx'],\n        'C#': ['.cs'],\n        'PHP': ['.php', '.phtml', '.inc'],\n    }\n    \n    # Directories to exclude from counting\n    excluded_dirs = {\n        'node_modules', '__pycache__', '.git', 'build', 'dist', \n        '.venv', 'venv', 'env', '.env', 'target', 'bin', 'obj',\n        '.pytest_cache', '.mypy_cache', '.tox', 'coverage',\n        'htmlcov', '.eggs', '*.egg-info', 'vendor', 'bower_components',\n        '.idea', '.vscode', '.gradle', '.mvn'\n    }\n    \n    def should_exclude_file(file_path: Path) -> bool:\n        \"\"\"Check if file is in an excluded directory.\"\"\"\n        parts = file_path.parts\n        return any(excluded_dir in parts for excluded_dir in excluded_dirs)\n    \n    language_counts = {}\n    \n    for language, extensions in language_extensions.items():\n        count = 0\n        for ext in extensions:\n            # Filter out files in excluded directories\n            count += sum(\n                1 for f in directory.rglob(f\"*{ext}\")\n                if f.is_file() and not should_exclude_file(f)\n            )\n        \n        if count > 0:\n            language_counts[language] = count\n    \n    # Sort by count descending\n    return sorted(language_counts.items(), key=lambda x: x[1], reverse=True)",
    "start_line": 156,
    "end_line": 206,
    "has_docstring": true,
    "docstring": "Detect supported programming languages in a directory.\n\nArgs:\n    directory: Directory to scan\n    \nReturns:\n    List of (language, file_count) tuples",
    "parameters": [
      "directory"
    ],
    "node_type": "function",
    "base_classes": null,
    "class_name": null,
    "display_name": "function detect_supported_languages",
    "component_id": "codewiki.cli.utils.validation.detect_supported_languages"
  },
  "codewiki.cli.utils.validation.should_exclude_file": {
    "id": "codewiki.cli.utils.validation.should_exclude_file",
    "name": "should_exclude_file",
    "component_type": "function",
    "file_path": "/Users/wufagang/project/aiopen/CodeWiki/codewiki/cli/utils/validation.py",
    "relative_path": "codewiki/cli/utils/validation.py",
    "depends_on": [],
    "source_code": "    def should_exclude_file(file_path: Path) -> bool:\n        \"\"\"Check if file is in an excluded directory.\"\"\"\n        parts = file_path.parts\n        return any(excluded_dir in parts for excluded_dir in excluded_dirs)",
    "start_line": 186,
    "end_line": 189,
    "has_docstring": true,
    "docstring": "Check if file is in an excluded directory.",
    "parameters": [
      "file_path"
    ],
    "node_type": "function",
    "base_classes": null,
    "class_name": null,
    "display_name": "function should_exclude_file",
    "component_id": "codewiki.cli.utils.validation.should_exclude_file"
  },
  "codewiki.cli.utils.validation.is_top_tier_model": {
    "id": "codewiki.cli.utils.validation.is_top_tier_model",
    "name": "is_top_tier_model",
    "component_type": "function",
    "file_path": "/Users/wufagang/project/aiopen/CodeWiki/codewiki/cli/utils/validation.py",
    "relative_path": "codewiki/cli/utils/validation.py",
    "depends_on": [],
    "source_code": "def is_top_tier_model(model: str) -> bool:\n    \"\"\"\n    Check if a model is considered top-tier for clustering.\n    \n    Args:\n        model: Model name\n        \n    Returns:\n        True if top-tier, False otherwise\n    \"\"\"\n    top_tier_models = [\n        'claude-opus',\n        'claude-sonnet',\n        'gpt-4',\n        'gpt-5',\n        'gemini-2.5',\n    ]\n    \n    model_lower = model.lower()\n    return any(tier in model_lower for tier in top_tier_models)",
    "start_line": 209,
    "end_line": 228,
    "has_docstring": true,
    "docstring": "Check if a model is considered top-tier for clustering.\n\nArgs:\n    model: Model name\n    \nReturns:\n    True if top-tier, False otherwise",
    "parameters": [
      "model"
    ],
    "node_type": "function",
    "base_classes": null,
    "class_name": null,
    "display_name": "function is_top_tier_model",
    "component_id": "codewiki.cli.utils.validation.is_top_tier_model"
  },
  "codewiki.cli.utils.validation.mask_api_key": {
    "id": "codewiki.cli.utils.validation.mask_api_key",
    "name": "mask_api_key",
    "component_type": "function",
    "file_path": "/Users/wufagang/project/aiopen/CodeWiki/codewiki/cli/utils/validation.py",
    "relative_path": "codewiki/cli/utils/validation.py",
    "depends_on": [],
    "source_code": "def mask_api_key(api_key: str, visible_chars: int = 4) -> str:\n    \"\"\"\n    Mask API key for display, showing only first and last few characters.\n    \n    Args:\n        api_key: API key to mask\n        visible_chars: Number of visible characters at start and end\n        \n    Returns:\n        Masked API key (e.g., \"sk-1234...5678\")\n    \"\"\"\n    if not api_key:\n        return \"Not set\"\n    \n    if len(api_key) <= visible_chars * 2:\n        # Key too short, mask everything except edges\n        return f\"{api_key[:2]}...{api_key[-2:]}\"\n    \n    return f\"{api_key[:visible_chars]}...{api_key[-visible_chars:]}\"",
    "start_line": 231,
    "end_line": 249,
    "has_docstring": true,
    "docstring": "Mask API key for display, showing only first and last few characters.\n\nArgs:\n    api_key: API key to mask\n    visible_chars: Number of visible characters at start and end\n    \nReturns:\n    Masked API key (e.g., \"sk-1234...5678\")",
    "parameters": [
      "api_key",
      "visible_chars"
    ],
    "node_type": "function",
    "base_classes": null,
    "class_name": null,
    "display_name": "function mask_api_key",
    "component_id": "codewiki.cli.utils.validation.mask_api_key"
  },
  "codewiki.src.be.agent_orchestrator.AgentOrchestrator": {
    "id": "codewiki.src.be.agent_orchestrator.AgentOrchestrator",
    "name": "AgentOrchestrator",
    "component_type": "class",
    "file_path": "/Users/wufagang/project/aiopen/CodeWiki/codewiki/src/be/agent_orchestrator.py",
    "relative_path": "codewiki/src/be/agent_orchestrator.py",
    "depends_on": [
      "codewiki.src.be.llm_services.create_fallback_models",
      "codewiki.src.be.agent_tools.deps.CodeWikiDeps",
      "codewiki.src.be.utils.is_complex_module",
      "codewiki.cli.utils.errors.info",
      "codewiki.src.be.prompt_template.format_user_prompt"
    ],
    "source_code": "class AgentOrchestrator:\n    \"\"\"Orchestrates the AI agents for documentation generation.\"\"\"\n    \n    def __init__(self, config: Config):\n        self.config = config\n        self.fallback_models = create_fallback_models(config)\n    \n    def create_agent(self, module_name: str, components: Dict[str, Any], \n                    core_component_ids: List[str]) -> Agent:\n        \"\"\"Create an appropriate agent based on module complexity.\"\"\"\n        if is_complex_module(components, core_component_ids):\n            return Agent(\n                self.fallback_models,\n                name=module_name,\n                deps_type=CodeWikiDeps,\n                tools=[\n                    read_code_components_tool, \n                    str_replace_editor_tool, \n                    generate_sub_module_documentation_tool\n                ],\n                system_prompt=SYSTEM_PROMPT.format(module_name=module_name),\n            )\n        else:\n            return Agent(\n                self.fallback_models,\n                name=module_name,\n                deps_type=CodeWikiDeps,\n                tools=[read_code_components_tool, str_replace_editor_tool],\n                system_prompt=LEAF_SYSTEM_PROMPT.format(module_name=module_name),\n            )\n    \n    async def process_module(self, module_name: str, components: Dict[str, Node], \n                           core_component_ids: List[str], module_path: List[str], working_dir: str) -> Dict[str, Any]:\n        \"\"\"Process a single module and generate its documentation.\"\"\"\n        logger.info(f\"Processing module: {module_name}\")\n        \n        # Load or create module tree\n        module_tree_path = os.path.join(working_dir, MODULE_TREE_FILENAME)\n        module_tree = file_manager.load_json(module_tree_path)\n        \n        # Create agent\n        agent = self.create_agent(module_name, components, core_component_ids)\n        \n        # Create dependencies\n        deps = CodeWikiDeps(\n            absolute_docs_path=working_dir,\n            absolute_repo_path=str(os.path.abspath(self.config.repo_path)),\n            registry={},\n            components=components,\n            path_to_current_module=module_path,\n            current_module_name=module_name,\n            module_tree=module_tree,\n            max_depth=self.config.max_depth,\n            current_depth=1,\n            config=self.config\n        )\n\n        # check if overview docs already exists\n        overview_docs_path = os.path.join(working_dir, OVERVIEW_FILENAME)\n        if os.path.exists(overview_docs_path):\n            logger.info(f\"✓ Overview docs already exists at {overview_docs_path}\")\n            return module_tree\n\n        # check if module docs already exists\n        docs_path = os.path.join(working_dir, f\"{module_name}.md\")\n        if os.path.exists(docs_path):\n            logger.info(f\"✓ Module docs already exists at {docs_path}\")\n            return module_tree\n        \n        # Run agent\n        try:\n            result = await agent.run(\n                format_user_prompt(\n                    module_name=module_name,\n                    core_component_ids=core_component_ids,\n                    components=components,\n                    module_tree=deps.module_tree\n                ),\n                deps=deps\n            )\n            \n            # Save updated module tree\n            file_manager.save_json(deps.module_tree, module_tree_path)\n            logger.debug(f\"Successfully processed module: {module_name}\")\n            \n            return deps.module_tree\n            \n        except Exception as e:\n            logger.error(f\"Error processing module {module_name}: {str(e)}\")\n            logger.error(f\"Traceback: {traceback.format_exc()}\")\n            raise",
    "start_line": 59,
    "end_line": 149,
    "has_docstring": true,
    "docstring": "Orchestrates the AI agents for documentation generation.",
    "parameters": null,
    "node_type": "class",
    "base_classes": null,
    "class_name": null,
    "display_name": "class AgentOrchestrator",
    "component_id": "codewiki.src.be.agent_orchestrator.AgentOrchestrator"
  },
  "codewiki.src.be.agent_tools.deps.CodeWikiDeps": {
    "id": "codewiki.src.be.agent_tools.deps.CodeWikiDeps",
    "name": "CodeWikiDeps",
    "component_type": "class",
    "file_path": "/Users/wufagang/project/aiopen/CodeWiki/codewiki/src/be/agent_tools/deps.py",
    "relative_path": "codewiki/src/be/agent_tools/deps.py",
    "depends_on": [],
    "source_code": "class CodeWikiDeps:\n    absolute_docs_path: str\n    absolute_repo_path: str\n    registry: dict\n    components: dict[str, Node]\n    path_to_current_module: list[str]\n    current_module_name: str\n    module_tree: dict[str, any]\n    max_depth: int\n    current_depth: int\n    config: Config  # LLM configuration",
    "start_line": 6,
    "end_line": 16,
    "has_docstring": false,
    "docstring": "",
    "parameters": null,
    "node_type": "class",
    "base_classes": null,
    "class_name": null,
    "display_name": "class CodeWikiDeps",
    "component_id": "codewiki.src.be.agent_tools.deps.CodeWikiDeps"
  },
  "codewiki.src.be.agent_tools.generate_sub_module_documentations.generate_sub_module_documentation": {
    "id": "codewiki.src.be.agent_tools.generate_sub_module_documentations.generate_sub_module_documentation",
    "name": "generate_sub_module_documentation",
    "component_type": "function",
    "file_path": "/Users/wufagang/project/aiopen/CodeWiki/codewiki/src/be/agent_tools/generate_sub_module_documentations.py",
    "relative_path": "codewiki/src/be/agent_tools/generate_sub_module_documentations.py",
    "depends_on": [
      "codewiki.src.be.cluster_modules.format_potential_core_components",
      "codewiki.src.be.llm_services.create_fallback_models",
      "codewiki.src.be.utils.count_tokens",
      "codewiki.src.be.utils.is_complex_module",
      "codewiki.cli.utils.errors.info",
      "codewiki.src.be.prompt_template.format_user_prompt"
    ],
    "source_code": "async def generate_sub_module_documentation(\n    ctx: RunContext[CodeWikiDeps],\n    sub_module_specs: dict[str, list[str]]\n) -> str:\n    \"\"\"Generate detailed description of a given sub-module specs to the sub-agents\n\n    Args:\n        sub_module_specs: The specs of the sub-modules to generate documentation for. E.g. {\"sub_module_1\": [\"core_component_1.1\", \"core_component_1.2\"], \"sub_module_2\": [\"core_component_2.1\", \"core_component_2.2\"], ...}\n    \"\"\"\n\n    deps = ctx.deps\n    previous_module_name = deps.current_module_name\n    \n    # Create fallback models from config\n    fallback_models = create_fallback_models(deps.config)\n\n    # add the sub-module to the module tree\n    value = deps.module_tree\n    for key in deps.path_to_current_module:\n        value = value[key][\"children\"]\n    for sub_module_name, core_component_ids in sub_module_specs.items():\n        value[sub_module_name] = {\"components\": core_component_ids, \"children\": {}}\n    \n    for sub_module_name, core_component_ids in sub_module_specs.items():\n\n        # Create visual indentation for nested modules\n        indent = \"  \" * deps.current_depth\n        arrow = \"└─\" if deps.current_depth > 0 else \"→\"\n\n        logger.info(f\"{indent}{arrow} Generating documentation for sub-module: {sub_module_name}\")\n\n        num_tokens = count_tokens(format_potential_core_components(core_component_ids, ctx.deps.components)[-1])\n        \n        if is_complex_module(ctx.deps.components, core_component_ids) and ctx.deps.current_depth < ctx.deps.max_depth and num_tokens >= MAX_TOKEN_PER_LEAF_MODULE:\n            sub_agent = Agent(\n                model=fallback_models,\n                name=sub_module_name,\n                deps_type=CodeWikiDeps,\n                system_prompt=SYSTEM_PROMPT.format(module_name=sub_module_name),\n                tools=[read_code_components_tool, str_replace_editor_tool, generate_sub_module_documentation_tool],\n            )\n        else:\n            sub_agent = Agent(\n                model=fallback_models,\n                name=sub_module_name,\n                deps_type=CodeWikiDeps,\n                system_prompt=LEAF_SYSTEM_PROMPT.format(module_name=sub_module_name),\n                tools=[read_code_components_tool, str_replace_editor_tool],\n            )\n\n        deps.current_module_name = sub_module_name\n        deps.path_to_current_module.append(sub_module_name)\n        deps.current_depth += 1\n        # log the current module tree\n        # print(f\"Current module tree: {json.dumps(deps.module_tree, indent=4)}\")\n\n        result = await sub_agent.run(\n            format_user_prompt(\n                module_name=deps.current_module_name,\n                core_component_ids=core_component_ids,\n                components=ctx.deps.components,\n                module_tree=ctx.deps.module_tree,\n            ),\n            deps=ctx.deps\n        )\n\n        # remove the sub-module name from the path to current module and the module tree\n        deps.path_to_current_module.pop()\n        deps.current_depth -= 1\n\n    # restore the previous module name\n    deps.current_module_name = previous_module_name\n\n    return f\"Generate successfully. Documentations: {', '.join([key + '.md' for key in sub_module_specs.keys()])} are saved in the working directory.\"",
    "start_line": 17,
    "end_line": 90,
    "has_docstring": true,
    "docstring": "Generate detailed description of a given sub-module specs to the sub-agents\n\nArgs:\n    sub_module_specs: The specs of the sub-modules to generate documentation for. E.g. {\"sub_module_1\": [\"core_component_1.1\", \"core_component_1.2\"], \"sub_module_2\": [\"core_component_2.1\", \"core_component_2.2\"], ...}",
    "parameters": [
      "ctx",
      "sub_module_specs"
    ],
    "node_type": "function",
    "base_classes": null,
    "class_name": null,
    "display_name": "function generate_sub_module_documentation",
    "component_id": "codewiki.src.be.agent_tools.generate_sub_module_documentations.generate_sub_module_documentation"
  },
  "codewiki.src.be.agent_tools.read_code_components.read_code_components": {
    "id": "codewiki.src.be.agent_tools.read_code_components.read_code_components",
    "name": "read_code_components",
    "component_type": "function",
    "file_path": "/Users/wufagang/project/aiopen/CodeWiki/codewiki/src/be/agent_tools/read_code_components.py",
    "relative_path": "codewiki/src/be/agent_tools/read_code_components.py",
    "depends_on": [],
    "source_code": "async def read_code_components(ctx: RunContext[CodeWikiDeps], component_ids: list[str]) -> str:\n    \"\"\"Read the code of a given component id\n\n    Args:\n        component_ids: The ids of the components to read, e.g. [\"sweagent.types.AgentRunResult\", \"sweagent.types.AgentRunResult\"] where sweagent.types part is the path to the component and AgentRunResult is the name of the component\n    \"\"\"\n\n    results = []\n\n    for component_id in component_ids:\n        if component_id not in ctx.deps.components:\n            results.append(f\"# Component {component_id} not found\")\n        else:\n            results.append(f\"# Component {component_id}:\\n{ctx.deps.components[component_id].source_code.strip()}\\n\\n\")\n\n    return \"\\n\".join(results)",
    "start_line": 5,
    "end_line": 20,
    "has_docstring": true,
    "docstring": "Read the code of a given component id\n\nArgs:\n    component_ids: The ids of the components to read, e.g. [\"sweagent.types.AgentRunResult\", \"sweagent.types.AgentRunResult\"] where sweagent.types part is the path to the component and AgentRunResult is the name of the component",
    "parameters": [
      "ctx",
      "component_ids"
    ],
    "node_type": "function",
    "base_classes": null,
    "class_name": null,
    "display_name": "function read_code_components",
    "component_id": "codewiki.src.be.agent_tools.read_code_components.read_code_components"
  },
  "codewiki.src.be.agent_tools.str_replace_editor.maybe_truncate": {
    "id": "codewiki.src.be.agent_tools.str_replace_editor.maybe_truncate",
    "name": "maybe_truncate",
    "component_type": "function",
    "file_path": "/Users/wufagang/project/aiopen/CodeWiki/codewiki/src/be/agent_tools/str_replace_editor.py",
    "relative_path": "codewiki/src/be/agent_tools/str_replace_editor.py",
    "depends_on": [],
    "source_code": "def maybe_truncate(content: str, truncate_after: Optional[int] = MAX_RESPONSE_LEN):\n    \"\"\"Truncate content and append a notice if content exceeds the specified length.\"\"\"\n    return (\n        content\n        if not truncate_after or len(content) <= truncate_after\n        else content[:truncate_after] + TRUNCATED_MESSAGE\n    )",
    "start_line": 63,
    "end_line": 69,
    "has_docstring": true,
    "docstring": "Truncate content and append a notice if content exceeds the specified length.",
    "parameters": [
      "content",
      "truncate_after"
    ],
    "node_type": "function",
    "base_classes": null,
    "class_name": null,
    "display_name": "function maybe_truncate",
    "component_id": "codewiki.src.be.agent_tools.str_replace_editor.maybe_truncate"
  },
  "codewiki.src.be.agent_tools.str_replace_editor.Flake8Error": {
    "id": "codewiki.src.be.agent_tools.str_replace_editor.Flake8Error",
    "name": "Flake8Error",
    "component_type": "class",
    "file_path": "/Users/wufagang/project/aiopen/CodeWiki/codewiki/src/be/agent_tools/str_replace_editor.py",
    "relative_path": "codewiki/src/be/agent_tools/str_replace_editor.py",
    "depends_on": [],
    "source_code": "class Flake8Error:\n    \"\"\"A class to represent a single flake8 error\"\"\"\n\n    def __init__(self, filename: str, line_number: int, col_number: int, problem: str):\n        self.filename = filename\n        self.line_number = line_number\n        self.col_number = col_number\n        self.problem = problem\n\n    @classmethod\n    def from_line(cls, line: str):\n        try:\n            prefix, _sep, problem = line.partition(\": \")\n            filename, line_number, col_number = prefix.split(\":\")\n        except (ValueError, IndexError) as e:\n            msg = f\"Invalid flake8 error line: {line}\"\n            raise ValueError(msg) from e\n        return cls(filename, int(line_number), int(col_number), problem)\n\n    def __eq__(self, other):\n        if not isinstance(other, Flake8Error):\n            return NotImplemented\n        return (\n            self.filename == other.filename\n            and self.line_number == other.line_number\n            and self.col_number == other.col_number\n            and self.problem == other.problem\n        )\n\n    def __repr__(self):\n        return f\"Flake8Error(filename={self.filename}, line_number={self.line_number}, col_number={self.col_number}, problem={self.problem})\"",
    "start_line": 72,
    "end_line": 102,
    "has_docstring": true,
    "docstring": "A class to represent a single flake8 error",
    "parameters": null,
    "node_type": "class",
    "base_classes": null,
    "class_name": null,
    "display_name": "class Flake8Error",
    "component_id": "codewiki.src.be.agent_tools.str_replace_editor.Flake8Error"
  },
  "codewiki.src.be.agent_tools.str_replace_editor._update_previous_errors": {
    "id": "codewiki.src.be.agent_tools.str_replace_editor._update_previous_errors",
    "name": "_update_previous_errors",
    "component_type": "function",
    "file_path": "/Users/wufagang/project/aiopen/CodeWiki/codewiki/src/be/agent_tools/str_replace_editor.py",
    "relative_path": "codewiki/src/be/agent_tools/str_replace_editor.py",
    "depends_on": [
      "codewiki.src.be.agent_tools.str_replace_editor.Flake8Error"
    ],
    "source_code": "def _update_previous_errors(\n    previous_errors: List[Flake8Error], replacement_window: Tuple[int, int], replacement_n_lines: int\n) -> List[Flake8Error]:\n    \"\"\"Update the line numbers of the previous errors to what they would be after the edit window.\n    This is a helper function for `_filter_previous_errors`.\n\n    All previous errors that are inside of the edit window should not be ignored,\n    so they are removed from the previous errors list.\n\n    Args:\n        previous_errors: list of errors with old line numbers\n        replacement_window: the window of the edit/lines that will be replaced\n        replacement_n_lines: the number of lines that will be used to replace the text\n\n    Returns:\n        list of errors with updated line numbers\n    \"\"\"\n    updated = []\n    lines_added = replacement_n_lines - (replacement_window[1] - replacement_window[0] + 1)\n    for error in previous_errors:\n        if error.line_number < replacement_window[0]:\n            # no need to adjust the line number\n            updated.append(error)\n            continue\n        if replacement_window[0] <= error.line_number <= replacement_window[1]:\n            # The error is within the edit window, so let's not ignore it\n            # either way (we wouldn't know how to adjust the line number anyway)\n            continue\n        # We're out of the edit window, so we need to adjust the line number\n        updated.append(Flake8Error(error.filename, error.line_number + lines_added, error.col_number, error.problem))\n    return updated",
    "start_line": 105,
    "end_line": 135,
    "has_docstring": true,
    "docstring": "Update the line numbers of the previous errors to what they would be after the edit window.\nThis is a helper function for `_filter_previous_errors`.\n\nAll previous errors that are inside of the edit window should not be ignored,\nso they are removed from the previous errors list.\n\nArgs:\n    previous_errors: list of errors with old line numbers\n    replacement_window: the window of the edit/lines that will be replaced\n    replacement_n_lines: the number of lines that will be used to replace the text\n\nReturns:\n    list of errors with updated line numbers",
    "parameters": [
      "previous_errors",
      "replacement_window",
      "replacement_n_lines"
    ],
    "node_type": "function",
    "base_classes": null,
    "class_name": null,
    "display_name": "function _update_previous_errors",
    "component_id": "codewiki.src.be.agent_tools.str_replace_editor._update_previous_errors"
  },
  "codewiki.src.be.agent_tools.str_replace_editor.format_flake8_output": {
    "id": "codewiki.src.be.agent_tools.str_replace_editor.format_flake8_output",
    "name": "format_flake8_output",
    "component_type": "function",
    "file_path": "/Users/wufagang/project/aiopen/CodeWiki/codewiki/src/be/agent_tools/str_replace_editor.py",
    "relative_path": "codewiki/src/be/agent_tools/str_replace_editor.py",
    "depends_on": [
      "codewiki.src.be.agent_tools.str_replace_editor._update_previous_errors"
    ],
    "source_code": "def format_flake8_output(\n    input_string: str,\n    show_line_numbers: bool = False,\n    *,\n    previous_errors_string: str = \"\",\n    replacement_window: Optional[Tuple[int, int]] = None,\n    replacement_n_lines: Optional[int] = None,\n) -> str:\n    \"\"\"Filter flake8 output for previous errors and print it for a given file.\n\n    Args:\n        input_string: The flake8 output as a string\n        show_line_numbers: Whether to show line numbers in the output\n        previous_errors_string: The previous errors as a string\n        replacement_window: The window of the edit (lines that will be replaced)\n        replacement_n_lines: The number of lines used to replace the text\n\n    Returns:\n        The filtered flake8 output as a string\n    \"\"\"\n    # print(f\"Replacement window: {replacement_window}\")\n    # print(\"Replacement n lines:\", replacement_n_lines)\n    # print(\"Previous errors string:\", previous_errors_string)\n    # print(\"Input string:\", input_string)\n    errors = [Flake8Error.from_line(line.strip()) for line in input_string.split(\"\\n\") if line.strip()]\n    # print(f\"New errors before filtering: {errors=}\")\n    lines = []\n    if previous_errors_string:\n        assert replacement_window is not None\n        assert replacement_n_lines is not None\n        previous_errors = [\n            Flake8Error.from_line(line.strip()) for line in previous_errors_string.split(\"\\n\") if line.strip()\n        ]\n        # print(f\"Previous errors before updating: {previous_errors=}\")\n        previous_errors = _update_previous_errors(previous_errors, replacement_window, replacement_n_lines)\n        # print(f\"Previous errors after updating: {previous_errors=}\")\n        errors = [error for error in errors if error not in previous_errors]\n        # Sometimes new errors appear above the replacement window that were 'shadowed' by the previous errors\n        # they still clearly aren't caused by the edit.\n        errors = [error for error in errors if error.line_number >= replacement_window[0]]\n        # print(f\"New errors after filtering: {errors=}\")\n    for error in errors:\n        if not show_line_numbers:\n            lines.append(f\"- {error.problem}\")\n        else:\n            lines.append(f\"- line {error.line_number} col {error.col_number}: {error.problem}\")\n    return \"\\n\".join(lines)",
    "start_line": 138,
    "end_line": 184,
    "has_docstring": true,
    "docstring": "Filter flake8 output for previous errors and print it for a given file.\n\nArgs:\n    input_string: The flake8 output as a string\n    show_line_numbers: Whether to show line numbers in the output\n    previous_errors_string: The previous errors as a string\n    replacement_window: The window of the edit (lines that will be replaced)\n    replacement_n_lines: The number of lines used to replace the text\n\nReturns:\n    The filtered flake8 output as a string",
    "parameters": [
      "input_string",
      "show_line_numbers"
    ],
    "node_type": "function",
    "base_classes": null,
    "class_name": null,
    "display_name": "function format_flake8_output",
    "component_id": "codewiki.src.be.agent_tools.str_replace_editor.format_flake8_output"
  },
  "codewiki.src.be.agent_tools.str_replace_editor.flake8": {
    "id": "codewiki.src.be.agent_tools.str_replace_editor.flake8",
    "name": "flake8",
    "component_type": "function",
    "file_path": "/Users/wufagang/project/aiopen/CodeWiki/codewiki/src/be/agent_tools/str_replace_editor.py",
    "relative_path": "codewiki/src/be/agent_tools/str_replace_editor.py",
    "depends_on": [],
    "source_code": "def flake8(file_path: str) -> str:\n    \"\"\"Run flake8 on a given file and return the output as a string\"\"\"\n    if Path(file_path).suffix != \".py\":\n        return \"\"\n    cmd = \"flake8 --isolated --select=F821,F822,F831,E111,E112,E113,E999,E902 {file_path}\"\n    # don't use capture_output because it's not compatible with python3.6\n    out = subprocess.run(cmd.format(file_path=file_path), shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n    return out.stdout.decode()",
    "start_line": 187,
    "end_line": 194,
    "has_docstring": true,
    "docstring": "Run flake8 on a given file and return the output as a string",
    "parameters": [
      "file_path"
    ],
    "node_type": "function",
    "base_classes": null,
    "class_name": null,
    "display_name": "function flake8",
    "component_id": "codewiki.src.be.agent_tools.str_replace_editor.flake8"
  },
  "codewiki.src.be.agent_tools.str_replace_editor.Filemap": {
    "id": "codewiki.src.be.agent_tools.str_replace_editor.Filemap",
    "name": "Filemap",
    "component_type": "class",
    "file_path": "/Users/wufagang/project/aiopen/CodeWiki/codewiki/src/be/agent_tools/str_replace_editor.py",
    "relative_path": "codewiki/src/be/agent_tools/str_replace_editor.py",
    "depends_on": [],
    "source_code": "class Filemap:\n    def show_filemap(self, file_contents: str, encoding: str = \"utf8\"):\n        import warnings\n        from tree_sitter_languages import get_language, get_parser\n\n        warnings.simplefilter(\"ignore\", category=FutureWarning)\n\n        parser = get_parser(\"python\")\n        language = get_language(\"python\")\n\n        tree = parser.parse(bytes(file_contents.encode(encoding, errors=\"replace\")))\n\n        # See https://tree-sitter.github.io/tree-sitter/using-parsers#pattern-matching-with-queries.\n        query = language.query(\"\"\"\n        (function_definition\n        body: (_) @body)\n        \"\"\")\n\n        # TODO: consider special casing docstrings such that they are not elided. This\n        # could be accomplished by checking whether `body.text.decode('utf8')` starts\n        # with `\"\"\"` or `'''`.\n        elide_line_ranges = [\n            (node.start_point[0], node.end_point[0])\n            for node, _ in query.captures(tree.root_node)\n            # Only elide if it's sufficiently long\n            if node.end_point[0] - node.start_point[0] >= 5\n        ]\n        # Note that tree-sitter line numbers are 0-indexed, but we display 1-indexed.\n        elide_lines = {line for start, end in elide_line_ranges for line in range(start, end + 1)}\n        elide_messages = [(start, f\"... eliding lines {start+1}-{end+1} ...\") for start, end in elide_line_ranges]\n        out = []\n        for i, line in sorted(\n            elide_messages + [(i, line) for i, line in enumerate(file_contents.splitlines()) if i not in elide_lines]\n        ):\n            out.append(f\"{i+1:6d} {line}\")\n        return \"\\n\".join(out)",
    "start_line": 197,
    "end_line": 232,
    "has_docstring": false,
    "docstring": "",
    "parameters": null,
    "node_type": "class",
    "base_classes": null,
    "class_name": null,
    "display_name": "class Filemap",
    "component_id": "codewiki.src.be.agent_tools.str_replace_editor.Filemap"
  },
  "codewiki.src.be.agent_tools.str_replace_editor.WindowExpander": {
    "id": "codewiki.src.be.agent_tools.str_replace_editor.WindowExpander",
    "name": "WindowExpander",
    "component_type": "class",
    "file_path": "/Users/wufagang/project/aiopen/CodeWiki/codewiki/src/be/agent_tools/str_replace_editor.py",
    "relative_path": "codewiki/src/be/agent_tools/str_replace_editor.py",
    "depends_on": [],
    "source_code": "class WindowExpander:\n    def __init__(self, suffix: str = \"\"):\n        \"\"\"Try to expand viewports to include whole functions, classes, etc. rather than\n        using fixed line windows.\n\n        Args:\n            suffix: Filename suffix\n        \"\"\"\n        self.suffix = suffix\n        if self.suffix:\n            assert self.suffix.startswith(\".\")\n\n    def _find_breakpoints(self, lines: List[str], current_line: int, direction=1, max_added_lines: int = 30) -> int:\n        \"\"\"Returns 1-based line number of breakpoint. This line is meant to still be included in the viewport.\n\n        Args:\n            lines: List of lines of the file\n            current_line: 1-based line number of the current viewport\n            direction: 1 for down, -1 for up\n            max_added_lines: Maximum number of lines to extend\n\n        Returns:\n            1-based line number of breakpoint. This line is meant to still be included in the viewport.\n        \"\"\"\n        assert 1 <= current_line <= len(lines)\n        assert 0 <= max_added_lines\n\n        # 1. Find line range that we want to search for breakpoints in\n\n        if direction == 1:\n            # down\n            if current_line == len(lines):\n                # already last line, can't extend down\n                return current_line\n            iter_lines = range(current_line, 1 + min(current_line + max_added_lines, len(lines)))\n        elif direction == -1:\n            # up\n            if current_line == 1:\n                # already first line, can't extend up\n                return current_line\n            iter_lines = range(current_line, -1 + max(current_line - max_added_lines, 1), -1)\n        else:\n            msg = f\"Invalid direction {direction}\"\n            raise ValueError(msg)\n\n        # 2. Find the best breakpoint in the line range\n\n        # Every condition gives a score, the best score is the best breakpoint\n        best_score = 0\n        best_breakpoint = current_line\n        for i_line in iter_lines:\n            next_line = None\n            line = lines[i_line - 1]\n            if i_line + direction in iter_lines:\n                next_line = lines[i_line + direction - 1]\n            score = 0\n            if line == \"\":\n                score = 1\n                if next_line == \"\":\n                    # Double new blank line:\n                    score = 2\n            if self.suffix == \".py\" and any(\n                re.match(regex, line) for regex in [r\"^\\s*def\\s+\", r\"^\\s*class\\s+\", r\"^\\s*@\"]\n            ):\n                # We include decorators here, because they are always on top of the function/class definition\n                score = 3\n            if score > best_score:\n                best_score = score\n                best_breakpoint = i_line\n                if direction == 1 and i_line != current_line:\n                    best_breakpoint -= 1\n            if i_line == 1 or i_line == len(lines):\n                score = 3\n                if score > best_score:\n                    best_score = score\n                    best_breakpoint = i_line\n            # print(f\"Score {score} for line {i_line} ({line})\")\n\n        # print(f\"Best score {best_score} for line {best_breakpoint} ({lines[best_breakpoint-1]})\")\n        if direction == 1 and best_breakpoint < current_line or direction == -1 and best_breakpoint > current_line:\n            # We don't want to shrink the view port, so we return the current line\n            return current_line\n\n        return best_breakpoint\n\n    def expand_window(self, lines: List[str], start: int, stop: int, max_added_lines: int) -> Tuple[int, int]:\n        \"\"\"\n\n        Args:\n            lines: All lines of the file\n            start: 1-based line number of the start of the viewport\n            stop: 1-based line number of the end of the viewport\n            max_added_lines: Maximum number of lines to extend (separately for each side)\n\n        Returns:\n            Tuple of 1-based line numbers of the start and end of the viewport.\n            Both inclusive.\n        \"\"\"\n        # print(\"Input:\", start, stop)\n        assert 1 <= start <= stop <= len(lines), (start, stop, len(lines))\n        if max_added_lines <= 0:\n            # Already at max range, no expansion\n            return start, stop\n        new_start = self._find_breakpoints(lines, start, direction=-1, max_added_lines=max_added_lines)\n        new_stop = self._find_breakpoints(lines, stop, direction=1, max_added_lines=max_added_lines)\n        # print(f\"Expanded window is {new_start} to {new_stop}\")\n        assert new_start <= new_stop, (new_start, new_stop)\n        assert new_start <= start, (new_start, start)\n        assert start - new_start <= max_added_lines, (start, new_start)\n        assert new_stop >= stop, (new_stop, stop)\n        assert new_stop - stop <= max_added_lines, (new_stop, stop)\n        return new_start, new_stop",
    "start_line": 235,
    "end_line": 346,
    "has_docstring": false,
    "docstring": "",
    "parameters": null,
    "node_type": "class",
    "base_classes": null,
    "class_name": null,
    "display_name": "class WindowExpander",
    "component_id": "codewiki.src.be.agent_tools.str_replace_editor.WindowExpander"
  },
  "codewiki.src.be.agent_tools.str_replace_editor.EditTool": {
    "id": "codewiki.src.be.agent_tools.str_replace_editor.EditTool",
    "name": "EditTool",
    "component_type": "class",
    "file_path": "/Users/wufagang/project/aiopen/CodeWiki/codewiki/src/be/agent_tools/str_replace_editor.py",
    "relative_path": "codewiki/src/be/agent_tools/str_replace_editor.py",
    "depends_on": [
      "codewiki.src.be.agent_tools.str_replace_editor.Filemap",
      "codewiki.src.be.agent_tools.str_replace_editor.format_flake8_output",
      "codewiki.src.be.agent_tools.str_replace_editor.WindowExpander",
      "codewiki.src.be.agent_tools.str_replace_editor.flake8",
      "codewiki.src.be.agent_tools.str_replace_editor.maybe_truncate"
    ],
    "source_code": "class EditTool:\n    \"\"\"\n    An filesystem editor tool that allows the agent to view, create, and edit files.\n    The tool parameters are defined by Anthropic and are not editable.\n    \"\"\"\n\n    name = \"str_replace_editor\"\n\n    def __init__(self, REGISTRY, absolute_docs_path=None):\n        super().__init__()\n        self._encoding = None\n        self.REGISTRY = REGISTRY\n        self.logs = []\n        self.absolute_docs_path = Path(absolute_docs_path) if absolute_docs_path else None\n\n    def _get_display_path(self, path: Path) -> str:\n        \"\"\"Get path for display purposes - relative to absolute_docs_path if available\"\"\"\n        if self.absolute_docs_path and path.is_absolute():\n            try:\n                return str(path.relative_to(self.absolute_docs_path))\n            except ValueError:\n                # Path is not under absolute_docs_path, return as-is\n                return str(path)\n        return str(path)\n\n    @property\n    def _file_history(self):\n        return defaultdict(list, json.loads(self.REGISTRY.get(\"file_history\", \"{}\")))\n\n    @_file_history.setter\n    def _file_history(self, value: dict):\n        self.REGISTRY[\"file_history\"] = json.dumps(value)\n\n    def __call__(\n        self,\n        *,\n        command: Command,\n        path: str,\n        file_text: Optional[str] = None,\n        view_range: Optional[List[int]] = None,\n        old_str: Optional[str] = None,\n        new_str: Optional[str] = None,\n        insert_line: Optional[int] = None,\n        **kwargs,\n    ):\n        _path = Path(path)\n        if not self.validate_path(command, _path):\n            return\n        if command == \"view\":\n            return self.view(_path, view_range)\n        elif command == \"create\":\n            if file_text is None:\n                self.logs.append(\"Parameter `file_text` is required for command: create\")\n                return\n            self.create_file(_path, file_text)\n            return None\n        elif command == \"str_replace\":\n            if old_str is None:\n                self.logs.append(\"Parameter `old_str` is required for command: str_replace\")\n                return\n            return self.str_replace(_path, old_str, new_str)\n        elif command == \"insert\":\n            if insert_line is None:\n                self.logs.append(\"Parameter `insert_line` is required for command: insert\")\n                return\n            if new_str is None:\n                self.logs.append(\"Parameter `new_str` is required for command: insert\")\n                return\n            return self.insert(_path, insert_line, new_str)\n        elif command == \"undo_edit\":\n            return self.undo_edit(_path)\n        self.logs.append(\n            f'Unrecognized command {command}. The allowed commands for the {self.name} tool are: \"view\", \"create\", \"str_replace\", \"insert\", \"undo_edit\"'\n        )\n        return\n\n    def validate_path(self, command: str, path: Path):\n        \"\"\"\n        Check that the path/command combination is valid.\n        \"\"\"\n        # Check if its an absolute path\n        if not path.is_absolute():\n            suggested_path = Path.cwd() / path\n            self.logs.append(\n                f\"The path {self._get_display_path(path)} is not an absolute path, it should start with `/`. Maybe you meant {self._get_display_path(suggested_path)}?\"\n            )\n            return False\n        # Check if path exists\n        if not path.exists() and command != \"create\":\n            self.logs.append(f\"The path {self._get_display_path(path)} does not exist. Please provide a valid path.\")\n            return False\n        if path.exists() and command == \"create\":\n            self.logs.append(f\"File already exists at: {self._get_display_path(path)}. Cannot overwrite files using command `create`.\")\n            return False\n        # Check if the path points to a directory\n        if path.is_dir():\n            if command != \"view\":\n                self.logs.append(f\"The path {self._get_display_path(path)} is a directory and only the `view` command can be used on directories\")\n                return False\n        return True\n\n    def create_file(self, path: Path, file_text: str):\n        if not path.parent.exists():\n            self.logs.append(f\"The parent directory {self._get_display_path(path.parent)} does not exist. Please create it first.\")\n            return\n        self.write_file(path, file_text)\n        self._file_history[path].append(file_text)\n        self.logs.append(f\"File created successfully at: {self._get_display_path(path)}\")\n\n    def view(self, path: Path, view_range: Optional[List[int]] = None):\n        \"\"\"Implement the view command\"\"\"\n        if path.is_dir():\n            if view_range:\n                self.logs.append(\"The `view_range` parameter is not allowed when `path` points to a directory.\")\n                return\n\n            out = subprocess.run(\n                rf\"find {path} -maxdepth 2 -not -path '*/\\.*'\",\n                shell=True,\n                stdout=subprocess.PIPE,\n                stderr=subprocess.PIPE,\n            )\n            stdout = out.stdout.decode()\n            stderr = out.stderr.decode()\n\n            if not stderr:\n                stdout = stdout.replace(str(path), self._get_display_path(path))\n                stdout = f\"Here's the files and directories up to 2 levels deep in {self._get_display_path(path)}, excluding hidden items:\\n{stdout}\\n\"\n                self.logs.append(stdout)\n            return\n\n        file_content = self.read_file(path)\n        if view_range:\n            if len(view_range) != 2 or not all(isinstance(i, int) for i in view_range):\n                self.logs.append(\"Invalid `view_range`. It should be a list of two integers.\")\n                return\n            file_lines = file_content.split(\"\\n\")\n            n_lines_file = len(file_lines)\n            init_line, final_line = view_range\n            if init_line < 1 or init_line > n_lines_file:\n                self.logs.append(\n                    f\"Invalid `view_range`: {view_range}. Its first element `{init_line}` should be within the range of lines of the file: {[1, n_lines_file]}\"\n                )\n                return\n            if final_line > n_lines_file:\n                self.logs.append(\n                    f\"Invalid `view_range`: {view_range}. Its second element `{final_line}` should be smaller than the number of lines in the file: `{n_lines_file}`\"\n                )\n                return\n            if final_line != -1 and final_line < init_line:\n                self.logs.append(\n                    f\"Invalid `view_range`: {view_range}. Its second element `{final_line}` should be larger or equal than its first `{init_line}`\"\n                )\n                return\n\n            if final_line == -1:\n                final_line = n_lines_file\n\n            # Expand the viewport to include the whole function or class\n            init_line, final_line = WindowExpander(suffix=path.suffix).expand_window(\n                file_lines, init_line, final_line, max_added_lines=MAX_WINDOW_EXPANSION_VIEW\n            )\n\n            file_content = \"\\n\".join(file_lines[init_line - 1 : final_line])\n        else:\n            if path.suffix == \".py\" and len(file_content) > MAX_RESPONSE_LEN and USE_FILEMAP:\n                try:\n                    filemap = Filemap().show_filemap(file_content, encoding=self._encoding or \"utf-8\")\n                except Exception:\n                    # If we fail to show the filemap, just show the truncated file content\n                    pass\n                else:\n                    self.logs.append(\n                        \"<NOTE>This file is too large to display entirely. Showing abbreviated version. \"\n                        \"Please use `str_replace_editor view` with the `view_range` parameter to show selected lines next.</NOTE>\"\n                    )\n                    filemap = maybe_truncate(filemap.expandtabs())\n                    self.logs.append(filemap)\n                    self.logs.append(\n                        \"<IMPORTANT><NOTE>The above file has been abbreviated. Please use `str_replace editor view` with `view_range` to look at relevant files in detail.</NOTE></IMPORTANT>\"\n                    )\n                    return\n            # Else just show\n            init_line = 1\n\n        # init_line is 1-based\n        self.logs.append(self._make_output(file_content, self._get_display_path(path), init_line=init_line))\n\n    def str_replace(self, path: Path, old_str: str, new_str: Optional[str]):\n        \"\"\"Implement the str_replace command, which replaces old_str with new_str in the file content\"\"\"\n        # Read the file content\n        file_content = self.read_file(path).expandtabs()\n        old_str = old_str.expandtabs()\n        new_str = new_str.expandtabs() if new_str is not None else \"\"\n\n        # Check if old_str is unique in the file\n        occurrences = file_content.count(old_str)\n        if occurrences == 0:\n            self.logs.append(f\"No replacement was performed, old_str `{old_str}` did not appear verbatim in {self._get_display_path(path)}.\")\n            return\n        elif occurrences > 1:\n            file_content_lines = file_content.split(\"\\n\")\n            lines = [idx + 1 for idx, line in enumerate(file_content_lines) if old_str in line]\n            self.logs.append(\n                f\"No replacement was performed. Multiple occurrences of old_str `{old_str}` in lines {lines}. Please ensure it is unique\"\n            )\n            return\n\n        if new_str == old_str:\n            self.logs.append(f\"No replacement was performed, old_str `{old_str}` is the same as new_str `{new_str}`.\")\n            return\n\n        pre_edit_lint = \"\"\n        if USE_LINTER:\n            try:\n                pre_edit_lint = flake8(str(path))\n            except Exception as e:\n                self.logs.append(f\"Warning: Failed to run pre-edit linter on {path}: {e}\")\n\n        # Replace old_str with new_str\n        new_file_content = file_content.replace(old_str, new_str)\n\n        # Write the new content to the file\n        self.write_file(path, new_file_content)\n\n        post_edit_lint = \"\"\n        if USE_LINTER:\n            try:\n                post_edit_lint = flake8(str(path))\n            except Exception as e:\n                self.logs.append(f\"Warning: Failed to run post-edit linter on {path}: {e}\")\n\n        epilogue = \"\"\n        if post_edit_lint:\n            ...\n            replacement_window_start_line = file_content.split(old_str)[0].count(\"\\n\") + 1\n            replacement_lines = len(new_str.split(\"\\n\"))\n            replacement_window_end_line = replacement_window_start_line + replacement_lines - 1\n            replacement_window = (replacement_window_start_line, replacement_window_end_line)\n            errors = format_flake8_output(\n                post_edit_lint,\n                previous_errors_string=pre_edit_lint,\n                replacement_window=replacement_window,\n                replacement_n_lines=replacement_lines,\n            )\n            if errors.strip():\n                epilogue = LINT_WARNING_TEMPLATE.format(errors=errors)\n\n        # Save the content to history\n        self._file_history[path].append(file_content)\n\n        # Create a snippet of the edited section\n        replacement_line = file_content.split(old_str)[0].count(\"\\n\")\n        start_line = max(1, replacement_line - SNIPPET_LINES)\n        end_line = min(replacement_line + SNIPPET_LINES + new_str.count(\"\\n\"), len(new_file_content.splitlines()))\n        start_line, end_line = WindowExpander(suffix=path.suffix).expand_window(\n            new_file_content.split(\"\\n\"), start_line, end_line, max_added_lines=MAX_WINDOW_EXPANSION_EDIT_CONFIRM\n        )\n        snippet = \"\\n\".join(new_file_content.split(\"\\n\")[start_line - 1 : end_line])\n\n        # Prepare the success message\n        success_msg = f\"The file {self._get_display_path(path)} has been edited. \"\n        success_msg += self._make_output(snippet, f\"a snippet of {self._get_display_path(path)}\", start_line)\n        success_msg += \"Review the changes and make sure they are as expected. Edit the file again if necessary.\"\n        success_msg += epilogue\n\n        self.logs.append(success_msg)\n\n    def insert(self, path: Path, insert_line: int, new_str: str):\n        \"\"\"Implement the insert command, which inserts new_str at the specified line in the file content.\"\"\"\n        file_text = self.read_file(path).expandtabs()\n        new_str = new_str.expandtabs()\n        file_text_lines = file_text.split(\"\\n\")\n        n_lines_file = len(file_text_lines)\n\n        if insert_line < 0 or insert_line > n_lines_file:\n            self.logs.append(\n                f\"Invalid `insert_line` parameter: {insert_line}. It should be within the range of lines of the file: {[0, n_lines_file]}\"\n            )\n            return\n\n        new_str_lines = new_str.split(\"\\n\")\n        new_file_text_lines = file_text_lines[:insert_line] + new_str_lines + file_text_lines[insert_line:]\n        snippet_lines = (\n            file_text_lines[max(0, insert_line - SNIPPET_LINES) : insert_line]\n            + new_str_lines\n            + file_text_lines[insert_line : insert_line + SNIPPET_LINES]\n        )\n\n        new_file_text = \"\\n\".join(new_file_text_lines)\n        snippet = \"\\n\".join(snippet_lines)\n\n        self.write_file(path, new_file_text)\n        self._file_history[path].append(file_text)\n\n        # todo: Also expand these windows\n\n        success_msg = f\"The file {self._get_display_path(path)} has been edited. \"\n        success_msg += self._make_output(\n            snippet,\n            \"a snippet of the edited file\",\n            max(1, insert_line - SNIPPET_LINES + 1),\n        )\n        success_msg += \"Review the changes and make sure they are as expected (correct indentation, no duplicate lines, etc). Edit the file again if necessary.\"\n        self.logs.append(success_msg)\n\n    def undo_edit(self, path: Path):\n        \"\"\"Implement the undo_edit command.\"\"\"\n        if not self._file_history[path]:\n            self.logs.append(f\"No edit history found for {self._get_display_path(path)}.\")\n            return\n\n        old_text = self._file_history[path].pop()\n        self.write_file(path, old_text)\n\n        self.logs.append(f\"Last edit to {self._get_display_path(path)} undone successfully. {self._make_output(old_text, self._get_display_path(path))}\")\n\n    def read_file(self, path: Path):\n        \"\"\"Read the content of a file from a given path; raise a ToolError if an error occurs.\"\"\"\n        encodings = [\n            (None, None),\n            (\"utf-8\", None),\n            (\"latin-1\", None),\n            (\"utf-8\", \"replace\"),\n        ]\n        exception = None\n        for self._encoding, errors in encodings:\n            try:\n                text = path.read_text(encoding=self._encoding, errors=errors)\n            except UnicodeDecodeError as e:\n                exception = e\n            else:\n                break\n        else:\n            self.logs.append(f\"Ran into UnicodeDecodeError {exception} while trying to read {self._get_display_path(path)}\")\n            return\n        return text\n\n    def write_file(self, path: Path, file: str):\n        \"\"\"Write the content of a file to a given path; raise a ToolError if an error occurs.\"\"\"\n        try:\n            path.write_text(file, encoding=self._encoding or \"utf-8\")\n        except Exception as e:\n            self.logs.append(f\"Ran into {e} while trying to write to {self._get_display_path(path)}\")\n            return\n\n    def _make_output(\n        self,\n        file_content: str,\n        file_descriptor: str,\n        init_line: int = 1,\n        expand_tabs: bool = True,\n    ):\n        \"\"\"Generate output for the CLI based on the content of a file.\"\"\"\n        file_content = maybe_truncate(file_content)\n        if expand_tabs:\n            file_content = file_content.expandtabs()\n        file_content = \"\\n\".join([f\"{i + init_line:6}\\t{line}\" for i, line in enumerate(file_content.split(\"\\n\"))])\n        return f\"Here's the result of running `cat -n` on {file_descriptor}:\\n\" + file_content + \"\\n\"",
    "start_line": 349,
    "end_line": 707,
    "has_docstring": true,
    "docstring": "An filesystem editor tool that allows the agent to view, create, and edit files.\nThe tool parameters are defined by Anthropic and are not editable.",
    "parameters": null,
    "node_type": "class",
    "base_classes": null,
    "class_name": null,
    "display_name": "class EditTool",
    "component_id": "codewiki.src.be.agent_tools.str_replace_editor.EditTool"
  },
  "codewiki.src.be.agent_tools.str_replace_editor.str_replace_editor": {
    "id": "codewiki.src.be.agent_tools.str_replace_editor.str_replace_editor",
    "name": "str_replace_editor",
    "component_type": "function",
    "file_path": "/Users/wufagang/project/aiopen/CodeWiki/codewiki/src/be/agent_tools/str_replace_editor.py",
    "relative_path": "codewiki/src/be/agent_tools/str_replace_editor.py",
    "depends_on": [
      "codewiki.src.be.utils.validate_mermaid_diagrams",
      "codewiki.src.be.agent_tools.str_replace_editor.EditTool"
    ],
    "source_code": "async def str_replace_editor(\n    ctx: RunContext[CodeWikiDeps],\n    working_dir: Literal[\"repo\", \"docs\"],\n    command: Literal[\"view\", \"create\", \"str_replace\", \"insert\", \"undo_edit\"],\n    path: str,\n    file_text: Optional[str] = None,\n    view_range: Optional[List[int]] = None,\n    old_str: Optional[str] = None,\n    new_str: Optional[str] = None,\n    insert_line: Optional[int] = None,\n) -> str:\n    \"\"\"\n    Custom editing tool for viewing, creating and editing files\n        * State is persistent across command calls and discussions with the user\n        * If `path` is a file, `view` displays the result of applying `cat -n`. If `path` is a directory, `view` lists non-hidden files and directories up to 2 levels deep.\n        * The `create` command cannot be used if the specified `path` already exists as a file\n        * If a `command` generates a long output, it will be truncated and marked with `<response clipped>`\n        * The `undo_edit` command will revert the last edit made to the file at `path`\n        * Only `view` command is allowed when `working_dir` is `repo`.\n\n    Args:\n        working_dir: The working directory to use. Choose `repo` to work with the repository files, or `docs` to work with the generated documentation files.\n        command: The command to run. Allowed options are: `view`, `create`, `str_replace`, `insert`, `undo_edit`.\n        path: Path to file or directory, e.g. `./chat_core.md` or `./agents/`\n        file_text: Required parameter of `create` command, with the content of the file to be created.\n        view_range: Optional parameter of `view` command when `path` points to a file. If none is given, the full file is shown. If provided, the file will be shown in the indicated line number range, e.g. [11, 12] will show lines 11 and 12. Indexing at 1 to start. Setting `[start_line, -1]` shows all lines from `start_line` to the end of the file.\n        old_str: Required parameter of `str_replace` command containing the string in `path` to replace.\n        new_str: Optional parameter of `str_replace` command containing the new string (if not given, no string will be added). Required parameter of `insert` command containing the string to insert.\n    \"\"\"\n\n\n    tool = EditTool(ctx.deps.registry, ctx.deps.absolute_docs_path)\n    if working_dir == \"docs\":\n        absolute_path = str(Path(ctx.deps.absolute_docs_path) / path)\n    else:\n        absolute_path = str(Path(ctx.deps.absolute_repo_path) / path)\n\n    # validate command\n    if command != \"view\" and working_dir == \"repo\":\n        return \"The `view` command is the only allowed command when `working_dir` is `repo`.\"\n    \n    tool(\n        command=command,\n        path=absolute_path,\n        file_text=file_text,\n        view_range=view_range,\n        old_str=old_str,\n        new_str=new_str,\n        insert_line=insert_line,\n    )\n\n    result = \"\\n\".join(tool.logs)\n\n    if command != \"view\" and path.endswith(\".md\"):\n        mermaid_validation = await validate_mermaid_diagrams(absolute_path, path)\n        result = result + \"\\n---------- Mermaid validation ----------\\n\" + mermaid_validation\n\n    return result",
    "start_line": 709,
    "end_line": 766,
    "has_docstring": true,
    "docstring": "Custom editing tool for viewing, creating and editing files\n    * State is persistent across command calls and discussions with the user\n    * If `path` is a file, `view` displays the result of applying `cat -n`. If `path` is a directory, `view` lists non-hidden files and directories up to 2 levels deep.\n    * The `create` command cannot be used if the specified `path` already exists as a file\n    * If a `command` generates a long output, it will be truncated and marked with `<response clipped>`\n    * The `undo_edit` command will revert the last edit made to the file at `path`\n    * Only `view` command is allowed when `working_dir` is `repo`.\n\nArgs:\n    working_dir: The working directory to use. Choose `repo` to work with the repository files, or `docs` to work with the generated documentation files.\n    command: The command to run. Allowed options are: `view`, `create`, `str_replace`, `insert`, `undo_edit`.\n    path: Path to file or directory, e.g. `./chat_core.md` or `./agents/`\n    file_text: Required parameter of `create` command, with the content of the file to be created.\n    view_range: Optional parameter of `view` command when `path` points to a file. If none is given, the full file is shown. If provided, the file will be shown in the indicated line number range, e.g. [11, 12] will show lines 11 and 12. Indexing at 1 to start. Setting `[start_line, -1]` shows all lines from `start_line` to the end of the file.\n    old_str: Required parameter of `str_replace` command containing the string in `path` to replace.\n    new_str: Optional parameter of `str_replace` command containing the new string (if not given, no string will be added). Required parameter of `insert` command containing the string to insert.",
    "parameters": [
      "ctx",
      "working_dir",
      "command",
      "path",
      "file_text",
      "view_range",
      "old_str",
      "new_str",
      "insert_line"
    ],
    "node_type": "function",
    "base_classes": null,
    "class_name": null,
    "display_name": "function str_replace_editor",
    "component_id": "codewiki.src.be.agent_tools.str_replace_editor.str_replace_editor"
  },
  "codewiki.src.be.cluster_modules.format_potential_core_components": {
    "id": "codewiki.src.be.cluster_modules.format_potential_core_components",
    "name": "format_potential_core_components",
    "component_type": "function",
    "file_path": "/Users/wufagang/project/aiopen/CodeWiki/codewiki/src/be/cluster_modules.py",
    "relative_path": "codewiki/src/be/cluster_modules.py",
    "depends_on": [
      "codewiki.cli.utils.errors.warning"
    ],
    "source_code": "def format_potential_core_components(leaf_nodes: List[str], components: Dict[str, Node]) -> tuple[str, str]:\n    \"\"\"\n    Format the potential core components into a string that can be used in the prompt.\n    \"\"\"\n    # Filter out any invalid leaf nodes that don't exist in components\n    valid_leaf_nodes = []\n    for leaf_node in leaf_nodes:\n        if leaf_node in components:\n            valid_leaf_nodes.append(leaf_node)\n        else:\n            logger.warning(f\"Skipping invalid leaf node '{leaf_node}' - not found in components\")\n    \n    #group leaf nodes by file\n    leaf_nodes_by_file = defaultdict(list)\n    for leaf_node in valid_leaf_nodes:\n        leaf_nodes_by_file[components[leaf_node].relative_path].append(leaf_node)\n\n    potential_core_components = \"\"\n    potential_core_components_with_code = \"\"\n    for file, leaf_nodes in dict(sorted(leaf_nodes_by_file.items())).items():\n        potential_core_components += f\"# {file}\\n\"\n        potential_core_components_with_code += f\"# {file}\\n\"\n        for leaf_node in leaf_nodes:\n            potential_core_components += f\"\\t{leaf_node}\\n\"\n            potential_core_components_with_code += f\"\\t{leaf_node}\\n\"\n            potential_core_components_with_code += f\"{components[leaf_node].source_code}\\n\"\n\n    return potential_core_components, potential_core_components_with_code",
    "start_line": 14,
    "end_line": 41,
    "has_docstring": true,
    "docstring": "Format the potential core components into a string that can be used in the prompt.",
    "parameters": [
      "leaf_nodes",
      "components"
    ],
    "node_type": "function",
    "base_classes": null,
    "class_name": null,
    "display_name": "function format_potential_core_components",
    "component_id": "codewiki.src.be.cluster_modules.format_potential_core_components"
  },
  "codewiki.src.be.cluster_modules.cluster_modules": {
    "id": "codewiki.src.be.cluster_modules.cluster_modules",
    "name": "cluster_modules",
    "component_type": "function",
    "file_path": "/Users/wufagang/project/aiopen/CodeWiki/codewiki/src/be/cluster_modules.py",
    "relative_path": "codewiki/src/be/cluster_modules.py",
    "depends_on": [
      "codewiki.cli.utils.errors.warning",
      "codewiki.src.be.cluster_modules.format_potential_core_components",
      "codewiki.src.be.cluster_modules.cluster_modules",
      "codewiki.src.be.utils.count_tokens",
      "codewiki.src.be.prompt_template.format_cluster_prompt",
      "codewiki.src.be.llm_services.call_llm"
    ],
    "source_code": "def cluster_modules(\n    leaf_nodes: List[str],\n    components: Dict[str, Node],\n    config: Config,\n    current_module_tree: dict[str, Any] = {},\n    current_module_name: str = None,\n    current_module_path: List[str] = []\n) -> Dict[str, Any]:\n    \"\"\"\n    Cluster the potential core components into modules.\n    \"\"\"\n    potential_core_components, potential_core_components_with_code = format_potential_core_components(leaf_nodes, components)\n\n    if count_tokens(potential_core_components_with_code) <= MAX_TOKEN_PER_MODULE:\n        logger.debug(f\"Skipping clustering for {current_module_name} because the potential core components are too few: {count_tokens(potential_core_components_with_code)} tokens\")\n        return {}\n\n    prompt = format_cluster_prompt(potential_core_components, current_module_tree, current_module_name)\n    response = call_llm(prompt, config, model=config.cluster_model)\n\n    #parse the response\n    try:\n        if \"<GROUPED_COMPONENTS>\" not in response or \"</GROUPED_COMPONENTS>\" not in response:\n            logger.error(f\"Invalid LLM response format - missing component tags: {response[:200]}...\")\n            return {}\n        \n        response_content = response.split(\"<GROUPED_COMPONENTS>\")[1].split(\"</GROUPED_COMPONENTS>\")[0]\n        module_tree = eval(response_content)\n        \n        if not isinstance(module_tree, dict):\n            logger.error(f\"Invalid module tree format - expected dict, got {type(module_tree)}\")\n            return {}\n            \n    except Exception as e:\n        logger.error(f\"Failed to parse LLM response: {e}. Response: {response[:200]}...\")\n        logger.error(f\"Traceback: {traceback.format_exc()}\")\n        return {}\n\n    # check if the module tree is valid\n    if len(module_tree) <= 1:\n        logger.debug(f\"Skipping clustering for {current_module_name} because the module tree is too small: {len(module_tree)} modules\")\n        return {}\n\n    if current_module_tree == {}:\n        current_module_tree = module_tree\n    else:\n        value = current_module_tree\n        for key in current_module_path:\n            value = value[key][\"children\"]\n        for module_name, module_info in module_tree.items():\n            del module_info[\"path\"]\n            value[module_name] = module_info\n\n    for module_name, module_info in module_tree.items():\n        sub_leaf_nodes = module_info.get(\"components\", [])\n        \n        # Filter sub_leaf_nodes to ensure they exist in components\n        valid_sub_leaf_nodes = []\n        for node in sub_leaf_nodes:\n            if node in components:\n                valid_sub_leaf_nodes.append(node)\n            else:\n                logger.warning(f\"Skipping invalid sub leaf node '{node}' in module '{module_name}' - not found in components\")\n        \n        current_module_path.append(module_name)\n        module_info[\"children\"] = {}\n        module_info[\"children\"] = cluster_modules(valid_sub_leaf_nodes, components, config, current_module_tree, module_name, current_module_path)\n        current_module_path.pop()\n\n    return module_tree",
    "start_line": 44,
    "end_line": 113,
    "has_docstring": true,
    "docstring": "Cluster the potential core components into modules.",
    "parameters": [
      "leaf_nodes",
      "components",
      "config",
      "current_module_tree",
      "current_module_name",
      "current_module_path"
    ],
    "node_type": "function",
    "base_classes": null,
    "class_name": null,
    "display_name": "function cluster_modules",
    "component_id": "codewiki.src.be.cluster_modules.cluster_modules"
  },
  "codewiki.src.be.dependency_analyzer.analysis.analysis_service.AnalysisService": {
    "id": "codewiki.src.be.dependency_analyzer.analysis.analysis_service.AnalysisService",
    "name": "AnalysisService",
    "component_type": "class",
    "file_path": "/Users/wufagang/project/aiopen/CodeWiki/codewiki/src/be/dependency_analyzer/analysis/analysis_service.py",
    "relative_path": "codewiki/src/be/dependency_analyzer/analysis/analysis_service.py",
    "depends_on": [
      "codewiki.cli.utils.errors.warning",
      "codewiki.src.be.dependency_analyzer.models.analysis.AnalysisResult",
      "codewiki.src.be.dependency_analyzer.models.core.Repository",
      "codewiki.src.be.dependency_analyzer.analysis.call_graph_analyzer.CallGraphAnalyzer",
      "codewiki.src.be.dependency_analyzer.utils.security.safe_open_text",
      "codewiki.src.be.dependency_analyzer.analysis.cloning.clone_repository",
      "codewiki.src.be.dependency_analyzer.analysis.repo_analyzer.RepoAnalyzer",
      "codewiki.src.be.dependency_analyzer.analysis.cloning.cleanup_repository",
      "codewiki.src.be.dependency_analyzer.utils.security.assert_safe_path",
      "codewiki.src.be.dependency_analyzer.analysis.cloning.parse_github_url"
    ],
    "source_code": "class AnalysisService:\n    \"\"\"\n    Centralized analysis service supporting multiple programming languages.\n\n    This service orchestrates the complete analysis workflow:\n    1. Repository cloning and validation\n    2. File structure analysis with filtering\n    3. Multi-language AST parsing and call graph generation\n    4. Result consolidation and cleanup\n\n    \"\"\"\n\n    def __init__(self):\n        \"\"\"Initialize the analysis service with language-specific analyzers.\"\"\"\n        self.call_graph_analyzer = CallGraphAnalyzer()\n        self._temp_directories = []\n\n    def analyze_local_repository(\n        self,\n        repo_path: str,\n        max_files: int = 100,\n        languages: Optional[List[str]] = None\n    ) -> Dict[str, Any]:\n        \"\"\"\n        Analyze a local repository folder.\n        \n        Args:\n            repo_path: Path to local repository folder\n            max_files: Maximum number of files to analyze\n            languages: List of languages to include (e.g., ['python', 'javascript'])\n            \n        Returns:\n            Dict with analysis results including nodes and relationships\n        \"\"\"\n        try:\n            logger.debug(f\"Analyzing local repository at {repo_path}\")\n            \n            # Get repo analyzer to find files\n            repo_analyzer = RepoAnalyzer()\n            structure_result = repo_analyzer.analyze_repository_structure(repo_path)\n            \n            # Extract code files\n            code_files = self.call_graph_analyzer.extract_code_files(structure_result[\"file_tree\"])\n            \n            # Filter by languages if specified\n            if languages:\n                code_files = [f for f in code_files if f.get(\"language\") in languages]\n            \n            # Limit number of files\n            if len(code_files) > max_files:\n                code_files = code_files[:max_files]\n                logger.debug(f\"Limited analysis to {max_files} files\")\n            \n            logger.debug(f\"Analyzing {len(code_files)} files\")\n            \n            # Analyze files\n            result = self.call_graph_analyzer.analyze_code_files(code_files, repo_path)\n            \n            return {\n                \"nodes\": result.get(\"functions\", {}),\n                \"relationships\": result.get(\"relationships\", []),\n                \"summary\": {\n                    \"total_files\": len(code_files),\n                    \"total_nodes\": len(result.get(\"functions\", {})),\n                    \"total_relationships\": len(result.get(\"relationships\", []))\n                }\n            }\n            \n        except Exception as e:\n            logger.error(f\"Local repository analysis failed: {str(e)}\", exc_info=True)\n            raise RuntimeError(f\"Analysis failed: {str(e)}\")\n\n    def analyze_repository_full(\n        self,\n        github_url: str,\n        include_patterns: Optional[List[str]] = None,\n        exclude_patterns: Optional[List[str]] = None,\n    ) -> AnalysisResult:\n        \"\"\"\n        Perform complete repository analysis including call graph generation.\n\n        Args:\n            github_url: GitHub repository URL to analyze\n            include_patterns: File patterns to include (e.g., ['*.py', '*.js'])\n            exclude_patterns: Additional patterns to exclude\n\n        Returns:\n            AnalysisResult: Complete analysis with functions, relationships, and visualization\n\n        Raises:\n            ValueError: If GitHub URL is invalid\n            RuntimeError: If analysis fails\n        \"\"\"\n        temp_dir = None\n        try:\n            logger.debug(f\"Starting full analysis of {github_url}\")\n\n            temp_dir = self._clone_repository(github_url)\n            repo_info = self._parse_repository_info(github_url)\n\n            logger.debug(\"Analyzing repository file structure...\")\n            structure_result = self._analyze_structure(temp_dir, include_patterns, exclude_patterns)\n            logger.debug(f\"Found {structure_result['summary']['total_files']} files to analyze.\")\n\n            logger.debug(\"Starting call graph analysis...\")\n            call_graph_result = self._analyze_call_graph(structure_result[\"file_tree\"], temp_dir)\n            logger.debug(\n                f\"Call graph analysis complete. Found {call_graph_result['call_graph']['total_functions']} functions.\"\n            )\n\n            readme_content = self._read_readme_file(temp_dir)\n\n            analysis_result = AnalysisResult(\n                repository=Repository(\n                    url=repo_info[\"url\"],\n                    name=repo_info[\"name\"],\n                    clone_path=temp_dir,\n                    analysis_id=f\"{repo_info['owner']}-{repo_info['name']}\",\n                ),\n                functions=call_graph_result[\"functions\"],\n                relationships=call_graph_result[\"relationships\"],\n                file_tree=structure_result[\"file_tree\"],\n                summary={\n                    **structure_result[\"summary\"],\n                    **call_graph_result[\"call_graph\"],\n                    \"analysis_type\": \"full\",\n                    \"languages_analyzed\": call_graph_result[\"call_graph\"][\"languages_found\"],\n                },\n                visualization=call_graph_result[\"visualization\"],\n                readme_content=readme_content,\n            )\n\n            logger.debug(f\"Cleaning up temporary repository directory: {temp_dir}\")\n            self._cleanup_repository(temp_dir)\n\n            logger.debug(\n                f\"Analysis completed: {analysis_result.summary['total_functions']} functions found\"\n            )\n            return analysis_result\n\n        except Exception as e:\n            logger.error(f\"Analysis failed: {str(e)}\", exc_info=True)\n            if \"temp_dir\" in locals() and Path(temp_dir).exists():\n                self._cleanup_repository(temp_dir)\n            raise RuntimeError(f\"Repository analysis failed: {str(e)}\")\n\n    def analyze_repository_structure_only(\n        self,\n        github_url: str,\n        include_patterns: Optional[List[str]] = None,\n        exclude_patterns: Optional[List[str]] = None,\n    ) -> Dict[str, Any]:\n        \"\"\"\n        Perform lightweight structure-only analysis without call graph generation.\n\n        Args:\n            github_url: GitHub repository URL to analyze\n            include_patterns: File patterns to include\n            exclude_patterns: Additional patterns to exclude\n\n        Returns:\n            Dict: Repository structure with file tree and summary statistics\n        \"\"\"\n        temp_dir = None\n        try:\n            logger.debug(f\"Starting structure analysis of {github_url}\")\n\n            temp_dir = self._clone_repository(github_url)\n            repo_info = self._parse_repository_info(github_url)\n\n            structure_result = self._analyze_structure(temp_dir, include_patterns, exclude_patterns)\n\n            result = {\n                \"repository\": repo_info,\n                \"file_tree\": structure_result[\"file_tree\"],\n                \"file_summary\": {\n                    **structure_result[\"summary\"],\n                    \"analysis_type\": \"structure_only\",\n                },\n            }\n\n            self._cleanup_repository(temp_dir)\n\n            logger.debug(\n                f\"Structure analysis completed: {result['file_summary']['total_files']} files found\"\n            )\n            return result\n\n        except Exception as e:\n            if temp_dir:\n                self._cleanup_repository(temp_dir)\n            logger.error(f\"Structure analysis failed for {github_url}: {str(e)}\")\n            logger.error(f\"Traceback: {traceback.format_exc()}\")\n            raise RuntimeError(f\"Structure analysis failed: {str(e)}\") from e\n\n    def _clone_repository(self, github_url: str) -> str:\n        \"\"\"Clone repository and return temp dir path.\"\"\"\n        logger.debug(f\"Cloning {github_url}...\")\n        temp_dir = clone_repository(github_url)\n        logger.debug(f\"Repository cloned to {temp_dir}\")\n        self._temp_directories.append(temp_dir)\n        return temp_dir\n\n    def _parse_repository_info(self, github_url: str) -> Dict[str, str]:\n        \"\"\"Parse GitHub URL and extract repository metadata.\"\"\"\n        return parse_github_url(github_url)\n\n    def _analyze_structure(\n        self,\n        repo_dir: str,\n        include_patterns: Optional[List[str]],\n        exclude_patterns: Optional[List[str]],\n    ) -> Dict[str, Any]:\n        \"\"\"Analyze repository file structure with filtering.\"\"\"\n        logger.debug(\n            f\"Initializing RepoAnalyzer with include: {include_patterns}, exclude: {exclude_patterns}\"\n        )\n        repo_analyzer = RepoAnalyzer(include_patterns, exclude_patterns)\n        return repo_analyzer.analyze_repository_structure(repo_dir)\n\n    def _read_readme_file(self, repo_dir: str) -> Optional[str]:\n        \"\"\"Find and read the README file from the repository root.\"\"\"\n        # possible_readme_names = [\"README.md\", \"README\", \"readme.md\", \"README.txt\"]\n        # for name in possible_readme_names:\n        #     readme_path = Path(repo_dir) / name\n        #     if readme_path.exists():\n        #         try:\n        #             logger.debug(f\"Found README file at {readme_path}\")\n        #             return readme_path.read_text(encoding=\"utf-8\")\n        #         except Exception as e:\n        #             logger.warning(f\"Could not read README file at {readme_path}: {e}\")\n        #             return None\n        # logger.debug(\"No README file found in repository root.\")\n        # return None\n        base = Path(repo_dir)\n        possible_readme_names = [\"README.md\", \"README\", \"readme.md\", \"README.txt\"]\n        for name in possible_readme_names:\n            p = base / name\n            if p.exists():\n                try:\n                    assert_safe_path(base, p)\n                    logger.debug(f\"Found README file at {p}\")\n                    return safe_open_text(base, p, encoding=\"utf-8\")\n                except Exception as e:\n                    logger.warning(f\"Skipping unsafe/ unreadable README at {p}: {e}\")\n                    return None\n        logger.debug(\"No README file found in repository root.\")\n        return None\n\n    def _analyze_call_graph(self, file_tree: Dict[str, Any], repo_dir: str) -> Dict[str, Any]:\n        \"\"\"\n        Perform multi-language call graph analysis.\n\n        This method will be expanded to handle:\n        - Python AST analysis (current)\n        - JavaScript/TypeScript AST analysis (planned)\n        - Additional language support (future)\n        \"\"\"\n        logger.debug(\"Extracting code files from file tree...\")\n        code_files = self.call_graph_analyzer.extract_code_files(file_tree)\n\n        logger.debug(f\"Found {len(code_files)} total code files. Filtering for supported languages.\")\n        supported_files = self._filter_supported_languages(code_files)\n        logger.debug(f\"Analyzing {len(supported_files)} supported files.\")\n\n        result = self.call_graph_analyzer.analyze_code_files(supported_files, repo_dir)\n\n        result[\"call_graph\"][\"supported_languages\"] = self._get_supported_languages()\n        result[\"call_graph\"][\"unsupported_files\"] = len(code_files) - len(supported_files)\n\n        return result\n\n    def _filter_supported_languages(self, code_files: List[Dict]) -> List[Dict]:\n        \"\"\"\n        Filter code files to only include supported languages.\n\n        Supports Python, JavaScript, TypeScript, Java, C#, C, C++, PHP, Go, and Rust.\n        \"\"\"\n        supported_languages = {\n            \"python\",\n            \"javascript\",\n            \"typescript\",\n            \"java\",\n            \"csharp\",\n            \"c\",\n            \"cpp\",\n            \"php\",\n            \"go\",\n            \"rust\",\n        }\n\n        return [\n            file_info\n            for file_info in code_files\n            if file_info.get(\"language\") in supported_languages\n        ]\n\n    def _get_supported_languages(self) -> List[str]:\n        \"\"\"Get list of currently supported languages for analysis.\"\"\"\n        return [\"python\", \"javascript\", \"typescript\", \"java\", \"csharp\", \"c\", \"cpp\", \"php\"]\n\n    def _cleanup_repository(self, temp_dir: str):\n        \"\"\"Clean up cloned repository.\"\"\"\n        logger.debug(f\"Attempting to clean up {temp_dir}\")\n        cleanup_repository(temp_dir)\n        if temp_dir in self._temp_directories:\n            self._temp_directories.remove(temp_dir)\n\n    def cleanup_all(self):\n        \"\"\"Clean up all tracked temporary directories.\"\"\"\n        for temp_dir in self._temp_directories[:]:\n            self._cleanup_repository(temp_dir)\n\n    def __del__(self):\n        \"\"\"Ensure cleanup on service destruction.\"\"\"\n        self.cleanup_all()",
    "start_line": 24,
    "end_line": 339,
    "has_docstring": true,
    "docstring": "Centralized analysis service supporting multiple programming languages.\n\nThis service orchestrates the complete analysis workflow:\n1. Repository cloning and validation\n2. File structure analysis with filtering\n3. Multi-language AST parsing and call graph generation\n4. Result consolidation and cleanup",
    "parameters": null,
    "node_type": "class",
    "base_classes": null,
    "class_name": null,
    "display_name": "class AnalysisService",
    "component_id": "codewiki.src.be.dependency_analyzer.analysis.analysis_service.AnalysisService"
  },
  "codewiki.src.be.dependency_analyzer.analysis.analysis_service.analyze_repository": {
    "id": "codewiki.src.be.dependency_analyzer.analysis.analysis_service.analyze_repository",
    "name": "analyze_repository",
    "component_type": "function",
    "file_path": "/Users/wufagang/project/aiopen/CodeWiki/codewiki/src/be/dependency_analyzer/analysis/analysis_service.py",
    "relative_path": "codewiki/src/be/dependency_analyzer/analysis/analysis_service.py",
    "depends_on": [
      "codewiki.src.be.dependency_analyzer.analysis.analysis_service.AnalysisService"
    ],
    "source_code": "def analyze_repository(\n    github_url: str, include_patterns=None, exclude_patterns=None\n) -> tuple[AnalysisResult, None]:\n    \"\"\"\n    Backward compatibility function.\n\n    Returns:\n        tuple: (AnalysisResult, None) - None instead of temp_dir since cleanup is handled internally\n    \"\"\"\n    service = AnalysisService()\n    result = service.analyze_repository_full(github_url, include_patterns, exclude_patterns)\n    return result, None",
    "start_line": 342,
    "end_line": 353,
    "has_docstring": true,
    "docstring": "Backward compatibility function.\n\nReturns:\n    tuple: (AnalysisResult, None) - None instead of temp_dir since cleanup is handled internally",
    "parameters": [
      "github_url",
      "include_patterns",
      "exclude_patterns"
    ],
    "node_type": "function",
    "base_classes": null,
    "class_name": null,
    "display_name": "function analyze_repository",
    "component_id": "codewiki.src.be.dependency_analyzer.analysis.analysis_service.analyze_repository"
  },
  "codewiki.src.be.dependency_analyzer.analysis.analysis_service.analyze_repository_structure_only": {
    "id": "codewiki.src.be.dependency_analyzer.analysis.analysis_service.analyze_repository_structure_only",
    "name": "analyze_repository_structure_only",
    "component_type": "function",
    "file_path": "/Users/wufagang/project/aiopen/CodeWiki/codewiki/src/be/dependency_analyzer/analysis/analysis_service.py",
    "relative_path": "codewiki/src/be/dependency_analyzer/analysis/analysis_service.py",
    "depends_on": [
      "codewiki.src.be.dependency_analyzer.analysis.analysis_service.AnalysisService",
      "codewiki.src.be.dependency_analyzer.analysis.analysis_service.analyze_repository_structure_only"
    ],
    "source_code": "def analyze_repository_structure_only(\n    github_url: str, include_patterns=None, exclude_patterns=None\n) -> tuple[Dict, None]:\n    \"\"\"\n    Backward compatibility function.\n\n    Returns:\n        tuple: (structure_result, None) - None instead of temp_dir since cleanup is handled internally\n    \"\"\"\n    service = AnalysisService()\n    result = service.analyze_repository_structure_only(\n        github_url, include_patterns, exclude_patterns\n    )\n    return result, None",
    "start_line": 356,
    "end_line": 369,
    "has_docstring": true,
    "docstring": "Backward compatibility function.\n\nReturns:\n    tuple: (structure_result, None) - None instead of temp_dir since cleanup is handled internally",
    "parameters": [
      "github_url",
      "include_patterns",
      "exclude_patterns"
    ],
    "node_type": "function",
    "base_classes": null,
    "class_name": null,
    "display_name": "function analyze_repository_structure_only",
    "component_id": "codewiki.src.be.dependency_analyzer.analysis.analysis_service.analyze_repository_structure_only"
  },
  "codewiki.src.be.dependency_analyzer.analysis.call_graph_analyzer.CallGraphAnalyzer": {
    "id": "codewiki.src.be.dependency_analyzer.analysis.call_graph_analyzer.CallGraphAnalyzer",
    "name": "CallGraphAnalyzer",
    "component_type": "class",
    "file_path": "/Users/wufagang/project/aiopen/CodeWiki/codewiki/src/be/dependency_analyzer/analysis/call_graph_analyzer.py",
    "relative_path": "codewiki/src/be/dependency_analyzer/analysis/call_graph_analyzer.py",
    "depends_on": [
      "codewiki.cli.utils.errors.warning",
      "codewiki.src.be.dependency_analyzer.analyzers.typescript.analyze_typescript_file_treesitter",
      "codewiki.src.be.dependency_analyzer.analyzers.c.analyze_c_file",
      "codewiki.src.be.dependency_analyzer.utils.security.safe_open_text",
      "codewiki.src.be.dependency_analyzer.analyzers.php.analyze_php_file",
      "codewiki.src.be.dependency_analyzer.analyzers.python.analyze_python_file",
      "codewiki.src.be.dependency_analyzer.analyzers.csharp.analyze_csharp_file",
      "codewiki.src.be.dependency_analyzer.analyzers.cpp.analyze_cpp_file",
      "codewiki.src.be.dependency_analyzer.analyzers.javascript.analyze_javascript_file_treesitter",
      "codewiki.src.be.dependency_analyzer.analyzers.java.analyze_java_file"
    ],
    "source_code": "class CallGraphAnalyzer:\n    def __init__(self):\n        \"\"\"Initialize the call graph analyzer.\"\"\"\n        self.functions: Dict[str, Node] = {}\n        self.call_relationships: List[CallRelationship] = []\n        logger.debug(\"CallGraphAnalyzer initialized.\")\n\n    def analyze_code_files(self, code_files: List[Dict], base_dir: str) -> Dict:\n        \"\"\"\n        Complete analysis: Analyze all files to build complete call graph with all nodes.\n\n        This approach:\n        1. Analyzes all code files \n        2. Extracts all functions and relationships\n        3. Builds complete call graph\n        4. Returns all nodes and relationships \n        \"\"\"\n        logger.debug(f\"Starting analysis of {len(code_files)} files\")\n\n        self.functions = {}\n        self.call_relationships = []\n\n        files_analyzed = 0\n        for file_info in code_files:\n            logger.debug(f\"Analyzing: {file_info['path']}\")\n            self._analyze_code_file(base_dir, file_info)\n            files_analyzed += 1\n        logger.debug(\n            f\"Analysis complete: {files_analyzed} files analyzed, {len(self.functions)} functions, {len(self.call_relationships)} relationships\"\n        )\n\n        logger.debug(\"Resolving call relationships\")\n        self._resolve_call_relationships()\n        self._deduplicate_relationships()\n        viz_data = self._generate_visualization_data()\n\n        return {\n            \"call_graph\": {\n                \"total_functions\": len(self.functions),\n                \"total_calls\": len(self.call_relationships),\n                \"languages_found\": list(set(f.get(\"language\") for f in code_files)),\n                \"files_analyzed\": files_analyzed,\n                \"analysis_approach\": \"complete_unlimited\",\n            },\n            \"functions\": [func.model_dump() for func in self.functions.values()],\n            \"relationships\": [rel.model_dump() for rel in self.call_relationships],\n            \"visualization\": viz_data,\n        }\n\n    def extract_code_files(self, file_tree: Dict) -> List[Dict]:\n        \"\"\"\n        Extract code files from file tree structure.\n\n        Filters files based on supported extensions and excludes test/config files.\n\n        Args:\n            file_tree: Nested dictionary representing file structure\n\n        Returns:\n            List of code file information dictionaries\n        \"\"\"\n        code_files = []\n\n        def traverse(tree):\n            if tree[\"type\"] == \"file\":\n                ext = tree.get(\"extension\", \"\").lower()\n                if ext in CODE_EXTENSIONS:\n                    name = tree[\"name\"].lower()\n                    if not any(skip in name for skip in []):\n                        code_files.append(\n                            {\n                                \"path\": tree[\"path\"],\n                                \"name\": tree[\"name\"],\n                                \"extension\": ext,\n                                \"language\": CODE_EXTENSIONS[ext],\n                            }\n                        )\n            elif tree[\"type\"] == \"directory\" and tree.get(\"children\"):\n                for child in tree[\"children\"]:\n                    traverse(child)\n\n        traverse(file_tree)\n        return code_files\n\n    def _analyze_code_file(self, repo_dir: str, file_info: Dict):\n        \"\"\"\n        Analyze a single code file based on its language.\n\n        Routes to appropriate language-specific analyzer.\n\n        Args:\n            repo_dir: Repository directory path\n            file_info: File information dictionary\n        \"\"\"\n\n        base = Path(repo_dir)\n        file_path = base / file_info[\"path\"]\n\n        try:\n            content = safe_open_text(base, file_path)\n            language = file_info[\"language\"]\n            if language == \"python\":\n                self._analyze_python_file(file_path, content, repo_dir)\n            elif language == \"javascript\":\n                self._analyze_javascript_file(file_path, content, repo_dir)\n            elif language == \"typescript\":\n                self._analyze_typescript_file(file_path, content, repo_dir)\n            elif language == \"java\":\n                self._analyze_java_file(file_path, content, repo_dir)\n            elif language == \"csharp\":\n                self._analyze_csharp_file(file_path, content, repo_dir)\n            elif language == \"c\":\n                self._analyze_c_file(file_path, content, repo_dir)\n            elif language == \"cpp\":\n                self._analyze_cpp_file(file_path, content, repo_dir)\n            elif language == \"php\":\n                self._analyze_php_file(file_path, content, repo_dir)\n            # else:\n            #     logger.warning(\n            #         f\"Unsupported language for call graph analysis: {language} for file {file_path}\"\n            #     )\n\n        except Exception as e:\n            logger.error(f\"⚠️ Error analyzing {file_path}: {str(e)}\")\n            logger.error(f\"Traceback: {traceback.format_exc()}\")\n\n    def _analyze_python_file(self, file_path: str, content: str, base_dir: str):\n        \"\"\"\n        Analyze Python file using Python AST analyzer.\n\n        Args:\n            file_path: Relative path to the Python file\n            content: File content string\n            base_dir: Repository base directory path\n        \"\"\"\n        from codewiki.src.be.dependency_analyzer.analyzers.python import analyze_python_file\n\n        try:\n            functions, relationships = analyze_python_file(\n                file_path, content, repo_path=base_dir\n            )\n\n            for func in functions:\n                func_id = func.id if func.id else f\"{file_path}:{func.name}\"\n                self.functions[func_id] = func\n\n            self.call_relationships.extend(relationships)\n        except Exception as e:\n            logger.error(f\"Failed to analyze Python file {file_path}: {e}\", exc_info=True)\n\n    def _analyze_javascript_file(self, file_path: str, content: str, repo_dir: str):\n        \"\"\"\n        Analyze JavaScript file using tree-sitter based AST analyzer\n\n        Args:\n            file_path: Relative path to the JavaScript file\n            content: File content string\n            repo_dir: Repository base directory\n        \"\"\"\n        try:\n\n            from codewiki.src.be.dependency_analyzer.analyzers.javascript import analyze_javascript_file_treesitter\n\n            functions, relationships = analyze_javascript_file_treesitter(\n                file_path, content, repo_path=repo_dir\n            )\n\n            for func in functions:\n                func_id = func.id if func.id else f\"{file_path}:{func.name}\"\n                self.functions[func_id] = func\n\n            self.call_relationships.extend(relationships)\n\n        except Exception as e:\n            logger.error(f\"Failed to analyze JavaScript file {file_path}: {e}\", exc_info=True)\n\n    def _analyze_typescript_file(self, file_path: str, content: str, repo_dir: str):\n        \"\"\"\n        Analyze TypeScript file using tree-sitter based AST analyzer \n\n        Args:\n            file_path: Relative path to the TypeScript file\n            content: File content string\n        \"\"\"\n        try:\n\n            from codewiki.src.be.dependency_analyzer.analyzers.typescript import analyze_typescript_file_treesitter\n\n            functions, relationships = analyze_typescript_file_treesitter(\n                file_path, content, repo_path=repo_dir\n            )\n\n            for func in functions:\n                func_id = func.id if func.id else f\"{file_path}:{func.name}\"\n                self.functions[func_id] = func\n\n            self.call_relationships.extend(relationships)\n\n        except Exception as e:\n            logger.error(f\"Failed to analyze TypeScript file {file_path}: {e}\", exc_info=True)\n\n\n\n    def _analyze_c_file(self, file_path: str, content: str, repo_dir: str):\n        \"\"\"\n        Analyze C file using tree-sitter based analyzer.\n\n        Args:\n            file_path: Relative path to the C file\n            content: File content string\n            repo_dir: Repository base directory\n        \"\"\"\n        from codewiki.src.be.dependency_analyzer.analyzers.c import analyze_c_file\n\n        functions, relationships = analyze_c_file(file_path, content, repo_path=repo_dir)\n\n        for func in functions:\n            func_id = func.id if func.id else f\"{file_path}:{func.name}\"\n            self.functions[func_id] = func\n\n        self.call_relationships.extend(relationships)\n\n    def _analyze_cpp_file(self, file_path: str, content: str, repo_dir: str):\n        \"\"\"\n        Analyze C++ file using tree-sitter based analyzer.\n\n        Args:\n            file_path: Relative path to the C++ file\n            content: File content string\n        \"\"\"\n        from codewiki.src.be.dependency_analyzer.analyzers.cpp import analyze_cpp_file\n\n        functions, relationships = analyze_cpp_file(\n            file_path, content, repo_path=repo_dir\n        )\n\n        for func in functions:\n            func_id = func.id if func.id else f\"{file_path}:{func.name}\"\n            self.functions[func_id] = func\n\n        self.call_relationships.extend(relationships)\n\n    def _analyze_java_file(self, file_path: str, content: str, repo_dir: str):\n        \"\"\"\n        Analyze Java file using tree-sitter based analyzer.\n\n        Args:\n            file_path: Relative path to the Java file\n            content: File content string\n            repo_dir: Repository base directory\n        \"\"\"\n        from codewiki.src.be.dependency_analyzer.analyzers.java import analyze_java_file\n\n        try:\n            functions, relationships = analyze_java_file(file_path, content, repo_path=repo_dir)\n            for func in functions:\n                func_id = func.id if func.id else f\"{file_path}:{func.name}\"\n                self.functions[func_id] = func\n\n            self.call_relationships.extend(relationships)\n        except Exception as e:\n            logger.error(f\"Failed to analyze Java file {file_path}: {e}\", exc_info=True)\n\n    def _analyze_csharp_file(self, file_path: str, content: str, repo_dir: str):\n        \"\"\"\n        Analyze C# file using tree-sitter based analyzer.\n\n        Args:\n            file_path: Relative path to the C# file\n            content: File content string\n            repo_dir: Repository base directory\n        \"\"\"\n        from codewiki.src.be.dependency_analyzer.analyzers.csharp import analyze_csharp_file\n\n        try:\n            functions, relationships = analyze_csharp_file(file_path, content, repo_path=repo_dir)\n\n            for func in functions:\n                func_id = func.id if func.id else f\"{file_path}:{func.name}\"\n                self.functions[func_id] = func\n\n            self.call_relationships.extend(relationships)\n        except Exception as e:\n            logger.error(f\"Failed to analyze C# file {file_path}: {e}\", exc_info=True)\n\n    def _analyze_php_file(self, file_path: str, content: str, repo_dir: str):\n        \"\"\"\n        Analyze PHP file using tree-sitter based analyzer.\n\n        Args:\n            file_path: Relative path to the PHP file\n            content: File content string\n            repo_dir: Repository base directory\n        \"\"\"\n        from codewiki.src.be.dependency_analyzer.analyzers.php import analyze_php_file\n\n        try:\n            functions, relationships = analyze_php_file(file_path, content, repo_path=repo_dir)\n\n            for func in functions:\n                func_id = func.id if func.id else f\"{file_path}:{func.name}\"\n                self.functions[func_id] = func\n\n            self.call_relationships.extend(relationships)\n        except Exception as e:\n            logger.error(f\"Failed to analyze PHP file {file_path}: {e}\", exc_info=True)\n\n    def _resolve_call_relationships(self):\n        \"\"\"\n        Resolve function call relationships across all languages.\n\n        Attempts to match function calls to actual function definitions,\n        handling cross-language calls where possible.\n        \"\"\"\n        func_lookup = {}\n        for func_id, func_info in self.functions.items():\n            func_lookup[func_id] = func_id\n            func_lookup[func_info.name] = func_id\n            if func_info.component_id:\n                func_lookup[func_info.component_id] = func_id\n                method_name = func_info.component_id.split(\".\")[-1]\n                if method_name not in func_lookup:\n                    func_lookup[method_name] = func_id\n\n        resolved_count = 0\n        for relationship in self.call_relationships:\n            callee_name = relationship.callee\n\n            if callee_name in func_lookup:\n                relationship.callee = func_lookup[callee_name]\n                relationship.is_resolved = True\n                resolved_count += 1\n            elif \".\" in callee_name:\n                if callee_name in func_lookup:\n                    relationship.callee = func_lookup[callee_name]\n                    relationship.is_resolved = True\n                    resolved_count += 1\n                else:\n                    method_name = callee_name.split(\".\")[-1]\n                    if method_name in func_lookup:\n                        relationship.callee = func_lookup[method_name]\n                        relationship.is_resolved = True\n                        resolved_count += 1\n\n    def _deduplicate_relationships(self):\n        \"\"\"\n        Deduplicate call relationships based on caller-callee pairs.\n\n        Removes duplicate relationships while preserving the first occurrence.\n        This helps eliminate noise from multiple calls to the same function.\n        \"\"\"\n        seen = set()\n        unique_relationships = []\n\n        for rel in self.call_relationships:\n            key = (rel.caller, rel.callee)\n            if key not in seen:\n                seen.add(key)\n                unique_relationships.append(rel)\n\n        self.call_relationships = unique_relationships\n\n    def _generate_visualization_data(self) -> Dict:\n        \"\"\"\n        Generate visualization data for graph rendering.\n\n        Creates Cytoscape.js compatible graph data with nodes and edges.\n\n        Returns:\n            Dict: Visualization data with cytoscape elements and summary\n        \"\"\"\n        cytoscape_elements = []\n\n        for func_id, func_info in self.functions.items():\n            node_classes = []\n            if func_info.node_type == \"method\":\n                node_classes.append(\"node-method\")\n            else:\n                node_classes.append(\"node-function\")\n\n            file_ext = Path(func_info.file_path).suffix.lower()\n            if file_ext == \".py\":\n                node_classes.append(\"lang-python\")\n            elif file_ext == \".js\":\n                node_classes.append(\"lang-javascript\")\n            elif file_ext == \".ts\":\n                node_classes.append(\"lang-typescript\")\n            elif file_ext in [\".c\", \".h\"]:\n                node_classes.append(\"lang-c\")\n            elif file_ext in [\".cpp\", \".cc\", \".cxx\", \".hpp\", \".hxx\"]:\n                node_classes.append(\"lang-cpp\")\n            elif file_ext in [\".php\", \".phtml\", \".inc\"]:\n                node_classes.append(\"lang-php\")\n\n            cytoscape_elements.append(\n                {\n                    \"data\": {\n                        \"id\": func_id,\n                        \"label\": func_info.name,\n                        \"file\": func_info.file_path,\n                        \"type\": func_info.node_type or \"function\",\n                        \"language\": CODE_EXTENSIONS.get(file_ext, \"unknown\"),\n                    },\n                    \"classes\": \" \".join(node_classes),\n                }\n            )\n\n        resolved_rels = [r for r in self.call_relationships if r.is_resolved]\n        for rel in resolved_rels:\n            cytoscape_elements.append(\n                {\n                    \"data\": {\n                        \"id\": f\"{rel.caller}->{rel.callee}\",\n                        \"source\": rel.caller,\n                        \"target\": rel.callee,\n                        \"line\": rel.call_line,\n                    },\n                    \"classes\": \"edge-call\",\n                }\n            )\n\n        summary = {\n            \"total_nodes\": len(self.functions),\n            \"total_edges\": len(resolved_rels),\n            \"unresolved_calls\": len(self.call_relationships) - len(resolved_rels),\n        }\n\n        return {\n            \"cytoscape\": {\"elements\": cytoscape_elements},\n            \"summary\": summary,\n        }\n\n    def generate_llm_format(self) -> Dict:\n        \"\"\"Generate clean format optimized for LLM consumption.\"\"\"\n        return {\n            \"functions\": [\n                {\n                    \"name\": func.name,\n                    \"file\": Path(func.file_path).name,\n                    \"purpose\": (func.docstring.split(\"\\n\")[0] if func.docstring else None),\n                    \"parameters\": func.parameters,\n                    \"is_recursive\": func.name\n                    in [\n                        rel.callee\n                        for rel in self.call_relationships\n                        if rel.caller.endswith(func.name)\n                    ],\n                }\n                for func in self.functions.values()\n            ],\n            \"relationships\": {\n                func.name: {\n                    \"calls\": [\n                        rel.callee.split(\":\")[-1]\n                        for rel in self.call_relationships\n                        if rel.caller.endswith(func.name) and rel.is_resolved\n                    ],\n                    \"called_by\": [\n                        rel.caller.split(\":\")[-1]\n                        for rel in self.call_relationships\n                        if rel.callee.endswith(func.name) and rel.is_resolved\n                    ],\n                }\n                for func in self.functions.values()\n            },\n        }\n\n    def _select_most_connected_nodes(self, target_count: int):\n        \"\"\"\n        Select the most connected nodes from the call graph.\n\n        Args:\n            target_count: The number of nodes to select\n        \"\"\"\n        if len(self.functions) <= target_count:\n            return\n\n        if not self.call_relationships:\n            logger.warning(\"No call relationships found - keeping all functions by name\")\n            func_ids = list(self.functions.keys())[:target_count]\n            self.functions = {fid: func for fid, func in self.functions.items() if fid in func_ids}\n            return\n\n        graph = {}\n        for rel in self.call_relationships:\n            if rel.caller in self.functions:\n                if rel.caller not in graph:\n                    graph[rel.caller] = set()\n            if rel.callee in self.functions:\n                if rel.callee not in graph:\n                    graph[rel.callee] = set()\n\n            if rel.caller in graph and rel.callee in graph:\n                graph[rel.caller].add(rel.callee)\n                graph[rel.callee].add(rel.caller)\n\n        degree_centrality = {}\n        for func_id in self.functions.keys():\n            degree_centrality[func_id] = len(graph.get(func_id, set()))\n\n        sorted_func_ids = sorted(degree_centrality, key=degree_centrality.get, reverse=True)\n\n        selected_func_ids = sorted_func_ids[:target_count]\n\n        original_func_count = len(self.functions)\n        self.functions = {\n            fid: func for fid, func in self.functions.items() if fid in selected_func_ids\n        }\n\n        original_rel_count = len(self.call_relationships)\n        self.call_relationships = [\n            rel\n            for rel in self.call_relationships\n            if rel.caller in selected_func_ids and rel.callee in selected_func_ids\n        ]",
    "start_line": 20,
    "end_line": 534,
    "has_docstring": false,
    "docstring": "",
    "parameters": null,
    "node_type": "class",
    "base_classes": null,
    "class_name": null,
    "display_name": "class CallGraphAnalyzer",
    "component_id": "codewiki.src.be.dependency_analyzer.analysis.call_graph_analyzer.CallGraphAnalyzer"
  },
  "codewiki.src.be.dependency_analyzer.analysis.cloning.sanitize_github_url": {
    "id": "codewiki.src.be.dependency_analyzer.analysis.cloning.sanitize_github_url",
    "name": "sanitize_github_url",
    "component_type": "function",
    "file_path": "/Users/wufagang/project/aiopen/CodeWiki/codewiki/src/be/dependency_analyzer/analysis/cloning.py",
    "relative_path": "codewiki/src/be/dependency_analyzer/analysis/cloning.py",
    "depends_on": [],
    "source_code": "def sanitize_github_url(github_url: str) -> str:\n    \"\"\"\n    Sanitize GitHub URL to ensure proper format and remove extra path components.\n\n    Args:\n        github_url: Raw GitHub URL or repository path\n\n    Returns:\n        str: Sanitized GitHub URL suitable for cloning\n    \"\"\"\n\n    url = github_url.strip()\n\n    protocol = \"https://\"\n    if url.startswith(\"https://\"):\n        url = url[8:]\n    elif url.startswith(\"http://\"):\n        url = url[7:]\n        protocol = \"http://\"\n\n    if url.startswith(\"www.\"):\n        url = url[4:]\n\n    parts = url.split(\"/\")\n\n    if url.startswith(\"github.com/\"):\n        url_parts = url.split(\"/\")\n        if len(url_parts) >= 3:\n            owner = url_parts[1]\n            repo = url_parts[2]\n        else:\n            return github_url\n    elif \"/\" in url and not url.startswith(\"github.com\"):\n        url_parts = url.split(\"/\")\n        if len(url_parts) >= 2:\n            owner = url_parts[0]\n            repo = url_parts[1]\n        else:\n            return github_url\n    else:\n        return github_url\n\n    if repo.endswith(\".git\"):\n        repo = repo[:-4]\n\n    return f\"{protocol}github.com/{owner}/{repo}\"",
    "start_line": 12,
    "end_line": 57,
    "has_docstring": true,
    "docstring": "Sanitize GitHub URL to ensure proper format and remove extra path components.\n\nArgs:\n    github_url: Raw GitHub URL or repository path\n\nReturns:\n    str: Sanitized GitHub URL suitable for cloning",
    "parameters": [
      "github_url"
    ],
    "node_type": "function",
    "base_classes": null,
    "class_name": null,
    "display_name": "function sanitize_github_url",
    "component_id": "codewiki.src.be.dependency_analyzer.analysis.cloning.sanitize_github_url"
  },
  "codewiki.src.be.dependency_analyzer.analysis.cloning.clone_repository": {
    "id": "codewiki.src.be.dependency_analyzer.analysis.cloning.clone_repository",
    "name": "clone_repository",
    "component_type": "function",
    "file_path": "/Users/wufagang/project/aiopen/CodeWiki/codewiki/src/be/dependency_analyzer/analysis/cloning.py",
    "relative_path": "codewiki/src/be/dependency_analyzer/analysis/cloning.py",
    "depends_on": [
      "codewiki.src.be.dependency_analyzer.analysis.cloning.sanitize_github_url",
      "codewiki.src.be.dependency_analyzer.analysis.cloning.cleanup_repository_safe"
    ],
    "source_code": "def clone_repository(github_url: str) -> str:\n    \"\"\"\n    Clone a GitHub repository to a temporary directory.\n\n    Args:\n        github_url: GitHub repository URL (will be sanitized automatically)\n\n    Returns:\n        str: Path to the cloned repository directory\n\n    Raises:\n        RuntimeError: If cloning fails or git executable is not found.\n    \"\"\"\n    if not GIT_EXECUTABLE_PATH:\n        raise RuntimeError(\n            \"Git executable not found. Please install Git and ensure it is in the system's PATH.\"\n        )\n\n    sanitized_url = sanitize_github_url(github_url)\n\n    temp_dir = tempfile.mkdtemp(prefix=\"gitprobe_\")\n\n    try:\n        if os.name == \"nt\":\n            try:\n                subprocess.run(\n                    [\n                        GIT_EXECUTABLE_PATH,\n                        \"config\",\n                        \"--global\",\n                        \"core.longpaths\",\n                        \"true\",\n                    ],\n                    capture_output=True,\n                    text=True,\n                )\n            except:\n                pass\n\n        subprocess.run(\n            [\n                GIT_EXECUTABLE_PATH,\n                \"clone\",\n                \"--depth\",\n                \"1\",\n                \"--filter=blob:none\",\n                sanitized_url,\n                temp_dir,\n            ],\n            check=True,\n            capture_output=True,\n            text=True,\n            timeout=300,\n        )\n\n        if os.name == \"nt\":\n            try:\n                subprocess.run(\n                    [\n                        GIT_EXECUTABLE_PATH,\n                        \"-C\",\n                        temp_dir,\n                        \"config\",\n                        \"core.sparseCheckout\",\n                        \"true\",\n                    ],\n                    capture_output=True,\n                    text=True,\n                )\n\n                sparse_checkout_path = os.path.join(temp_dir, \".git\", \"info\", \"sparse-checkout\")\n                os.makedirs(os.path.dirname(sparse_checkout_path), exist_ok=True)\n                with open(sparse_checkout_path, \"w\") as f:\n                    f.write(\"*\\n\")\n                    f.write(\"!**/tests/**/CvnF9nAXfESwhrtdkjGhX2wAkKHzwr8N2rjExPK8eZYS/**\\n\")\n                    f.write(\n                        \"!**/0x0000000000000000000000000000000000000000000000000000000000000002/**\\n\"\n                    )\n\n                subprocess.run(\n                    [\n                        GIT_EXECUTABLE_PATH,\n                        \"-C\",\n                        temp_dir,\n                        \"read-tree\",\n                        \"-m\",\n                        \"-u\",\n                        \"HEAD\",\n                    ],\n                    capture_output=True,\n                    text=True,\n                )\n            except:\n                pass\n        return temp_dir\n    except subprocess.TimeoutExpired:\n        if os.path.exists(temp_dir):\n            cleanup_repository_safe(temp_dir)\n        raise RuntimeError(\n            f\"Repository cloning timed out after 5 minutes. The repository may be too large or network is slow.\"\n        )\n    except subprocess.CalledProcessError as e:\n        if os.path.exists(temp_dir):\n            cleanup_repository_safe(temp_dir)\n        raise RuntimeError(f\"Failed to clone repository: {e.stderr}\")\n    except FileNotFoundError:\n        if os.path.exists(temp_dir):\n            cleanup_repository_safe(temp_dir)\n        raise RuntimeError(\n            f\"Git executable not found at '{GIT_EXECUTABLE_PATH}'. \"\n            \"Please ensure Git is installed and the path is correct.\"\n        )",
    "start_line": 60,
    "end_line": 171,
    "has_docstring": true,
    "docstring": "Clone a GitHub repository to a temporary directory.\n\nArgs:\n    github_url: GitHub repository URL (will be sanitized automatically)\n\nReturns:\n    str: Path to the cloned repository directory\n\nRaises:\n    RuntimeError: If cloning fails or git executable is not found.",
    "parameters": [
      "github_url"
    ],
    "node_type": "function",
    "base_classes": null,
    "class_name": null,
    "display_name": "function clone_repository",
    "component_id": "codewiki.src.be.dependency_analyzer.analysis.cloning.clone_repository"
  },
  "codewiki.src.be.dependency_analyzer.analysis.cloning.cleanup_repository_safe": {
    "id": "codewiki.src.be.dependency_analyzer.analysis.cloning.cleanup_repository_safe",
    "name": "cleanup_repository_safe",
    "component_type": "function",
    "file_path": "/Users/wufagang/project/aiopen/CodeWiki/codewiki/src/be/dependency_analyzer/analysis/cloning.py",
    "relative_path": "codewiki/src/be/dependency_analyzer/analysis/cloning.py",
    "depends_on": [],
    "source_code": "def cleanup_repository_safe(repo_dir: str) -> bool:\n    \"\"\"\n    Windows-safe removal of the cloned repository directory.\n    Handles read-only files and permission issues common on Windows.\n\n    Args:\n        repo_dir: Path to the repository directory to remove\n\n    Returns:\n        bool: True if cleanup successful, False otherwise\n    \"\"\"\n\n    def handle_remove_readonly(func, path, exc):\n        \"\"\"Error handler for Windows read-only files.\"\"\"\n        if os.path.exists(path):\n            os.chmod(path, stat.S_IWRITE)\n            func(path)\n\n    try:\n        if os.path.exists(repo_dir):\n            if os.name == \"nt\":\n                shutil.rmtree(repo_dir, onerror=handle_remove_readonly)\n            else:\n                shutil.rmtree(repo_dir)\n            return True\n        return False\n    except PermissionError as e:\n        try:\n            time.sleep(1)\n            if os.path.exists(repo_dir):\n                for root, dirs, files in os.walk(repo_dir):\n                    for dir in dirs:\n                        os.chmod(os.path.join(root, dir), stat.S_IWRITE)\n                    for file in files:\n                        file_path = os.path.join(root, file)\n                        if os.path.exists(file_path):\n                            os.chmod(file_path, stat.S_IWRITE)\n                shutil.rmtree(repo_dir)\n            return True\n        except Exception as retry_e:\n            print(f\"⚠️ Warning: Failed to cleanup {repo_dir} after retry: {str(retry_e)}\")\n            return False\n    except Exception as e:\n        print(f\"⚠️ Warning: Failed to cleanup {repo_dir}: {str(e)}\")\n        return False",
    "start_line": 174,
    "end_line": 218,
    "has_docstring": true,
    "docstring": "Windows-safe removal of the cloned repository directory.\nHandles read-only files and permission issues common on Windows.\n\nArgs:\n    repo_dir: Path to the repository directory to remove\n\nReturns:\n    bool: True if cleanup successful, False otherwise",
    "parameters": [
      "repo_dir"
    ],
    "node_type": "function",
    "base_classes": null,
    "class_name": null,
    "display_name": "function cleanup_repository_safe",
    "component_id": "codewiki.src.be.dependency_analyzer.analysis.cloning.cleanup_repository_safe"
  },
  "codewiki.src.be.dependency_analyzer.analysis.cloning.handle_remove_readonly": {
    "id": "codewiki.src.be.dependency_analyzer.analysis.cloning.handle_remove_readonly",
    "name": "handle_remove_readonly",
    "component_type": "function",
    "file_path": "/Users/wufagang/project/aiopen/CodeWiki/codewiki/src/be/dependency_analyzer/analysis/cloning.py",
    "relative_path": "codewiki/src/be/dependency_analyzer/analysis/cloning.py",
    "depends_on": [],
    "source_code": "    def handle_remove_readonly(func, path, exc):\n        \"\"\"Error handler for Windows read-only files.\"\"\"\n        if os.path.exists(path):\n            os.chmod(path, stat.S_IWRITE)\n            func(path)",
    "start_line": 186,
    "end_line": 190,
    "has_docstring": true,
    "docstring": "Error handler for Windows read-only files.",
    "parameters": [
      "func",
      "path",
      "exc"
    ],
    "node_type": "function",
    "base_classes": null,
    "class_name": null,
    "display_name": "function handle_remove_readonly",
    "component_id": "codewiki.src.be.dependency_analyzer.analysis.cloning.handle_remove_readonly"
  },
  "codewiki.src.be.dependency_analyzer.analysis.cloning.cleanup_repository": {
    "id": "codewiki.src.be.dependency_analyzer.analysis.cloning.cleanup_repository",
    "name": "cleanup_repository",
    "component_type": "function",
    "file_path": "/Users/wufagang/project/aiopen/CodeWiki/codewiki/src/be/dependency_analyzer/analysis/cloning.py",
    "relative_path": "codewiki/src/be/dependency_analyzer/analysis/cloning.py",
    "depends_on": [
      "codewiki.src.be.dependency_analyzer.analysis.cloning.cleanup_repository_safe"
    ],
    "source_code": "def cleanup_repository(repo_dir: str) -> bool:\n    \"\"\"\n    Remove the cloned repository directory (wrapper for backward compatibility).\n\n    Args:\n        repo_dir: Path to the repository directory to remove\n\n    Returns:\n        bool: True if cleanup successful, False otherwise\n    \"\"\"\n    return cleanup_repository_safe(repo_dir)",
    "start_line": 221,
    "end_line": 231,
    "has_docstring": true,
    "docstring": "Remove the cloned repository directory (wrapper for backward compatibility).\n\nArgs:\n    repo_dir: Path to the repository directory to remove\n\nReturns:\n    bool: True if cleanup successful, False otherwise",
    "parameters": [
      "repo_dir"
    ],
    "node_type": "function",
    "base_classes": null,
    "class_name": null,
    "display_name": "function cleanup_repository",
    "component_id": "codewiki.src.be.dependency_analyzer.analysis.cloning.cleanup_repository"
  },
  "codewiki.src.be.dependency_analyzer.analysis.cloning.parse_github_url": {
    "id": "codewiki.src.be.dependency_analyzer.analysis.cloning.parse_github_url",
    "name": "parse_github_url",
    "component_type": "function",
    "file_path": "/Users/wufagang/project/aiopen/CodeWiki/codewiki/src/be/dependency_analyzer/analysis/cloning.py",
    "relative_path": "codewiki/src/be/dependency_analyzer/analysis/cloning.py",
    "depends_on": [],
    "source_code": "def parse_github_url(github_url: str) -> dict:\n    \"\"\"\n    Parse GitHub URL to extract owner and repository name.\n\n    Args:\n        github_url: GitHub repository URL\n\n    Returns:\n        dict: Repository information\n    \"\"\"\n    parts = github_url.rstrip(\"/\").split(\"/\")\n    if len(parts) >= 2:\n        owner = parts[-2]\n        name = parts[-1].replace(\".git\", \"\")\n        return {\n            \"owner\": owner,\n            \"name\": name,\n            \"full_name\": f\"{owner}/{name}\",\n            \"url\": github_url,\n        }\n    return {\n        \"owner\": \"unknown\",\n        \"name\": \"unknown\",\n        \"full_name\": \"unknown\",\n        \"url\": github_url,\n    }",
    "start_line": 234,
    "end_line": 259,
    "has_docstring": true,
    "docstring": "Parse GitHub URL to extract owner and repository name.\n\nArgs:\n    github_url: GitHub repository URL\n\nReturns:\n    dict: Repository information",
    "parameters": [
      "github_url"
    ],
    "node_type": "function",
    "base_classes": null,
    "class_name": null,
    "display_name": "function parse_github_url",
    "component_id": "codewiki.src.be.dependency_analyzer.analysis.cloning.parse_github_url"
  },
  "codewiki.src.be.dependency_analyzer.analysis.repo_analyzer.RepoAnalyzer": {
    "id": "codewiki.src.be.dependency_analyzer.analysis.repo_analyzer.RepoAnalyzer",
    "name": "RepoAnalyzer",
    "component_type": "class",
    "file_path": "/Users/wufagang/project/aiopen/CodeWiki/codewiki/src/be/dependency_analyzer/analysis/repo_analyzer.py",
    "relative_path": "codewiki/src/be/dependency_analyzer/analysis/repo_analyzer.py",
    "depends_on": [],
    "source_code": "class RepoAnalyzer:\n    def __init__(\n        self,\n        include_patterns: Optional[List[str]] = None,\n        exclude_patterns: Optional[List[str]] = None,\n    ) -> None:\n        self.include_patterns = (\n            include_patterns if include_patterns is not None else DEFAULT_INCLUDE_PATTERNS\n        )\n        self.exclude_patterns = (\n            list(DEFAULT_IGNORE_PATTERNS) + exclude_patterns\n            if exclude_patterns is not None\n            else list(DEFAULT_IGNORE_PATTERNS)\n        )\n\n    def analyze_repository_structure(self, repo_dir: str) -> Dict:\n        file_tree = self._build_file_tree(repo_dir)\n        return {\n            \"file_tree\": file_tree,\n            \"summary\": {\n                \"total_files\": self._count_files(file_tree),\n                \"total_size_kb\": self._calculate_size(file_tree),\n            },\n        }\n\n    def _build_file_tree(self, repo_dir: str) -> Dict:\n        def build_tree(path: Path, base_path: Path) -> Optional[Dict]:\n            relative_path = path.relative_to(base_path)\n            relative_path_str = str(relative_path)\n\n            # 🚫 Reject symlinks\n            if path.is_symlink():\n                return None\n\n            # 🚫 Reject escaped paths (e.g., symlinks pointing outside)\n            try:\n                if not path.resolve().is_relative_to(base_path.resolve()):\n                    return None\n            except AttributeError:\n                if not str(path.resolve()).startswith(str(base_path.resolve())):\n                    return None\n\n            if self._should_exclude_path(relative_path_str, path.name):\n                return None\n\n            if path.is_file():\n                if not self._should_include_file(relative_path_str, path.name):\n                    return None\n\n                size = path.stat().st_size\n                return {\n                    \"type\": \"file\",\n                    \"name\": path.name,\n                    \"path\": relative_path_str,\n                    \"extension\": path.suffix,\n                    \"_size_bytes\": size,\n                }\n\n            elif path.is_dir():\n                children = []\n                try:\n                    for child in sorted(path.iterdir()):\n                        child_tree = build_tree(child, base_path)\n                        if child_tree is not None:\n                            children.append(child_tree)\n                except PermissionError:\n                    pass\n\n                if children or str(relative_path) == \".\":\n                    return {\n                        \"type\": \"directory\",\n                        \"name\": path.name,\n                        \"path\": relative_path_str,\n                        \"children\": children,\n                    }\n                return None\n\n            # Other types (sockets, devices, etc.)\n            return None\n\n        return build_tree(Path(repo_dir), Path(repo_dir))\n\n    def _should_exclude_path(self, path: str, filename: str) -> bool:\n        for pattern in self.exclude_patterns:\n            if fnmatch.fnmatch(path, pattern) or fnmatch.fnmatch(filename, pattern):\n                return True\n            if pattern.endswith(\"/\") and path.startswith(pattern.rstrip(\"/\")):\n                return True\n            if path.startswith(pattern + \"/\") or path == pattern:\n                return True\n            if pattern in path.split(\"/\"):\n                return True\n        return False\n\n    def _should_include_file(self, path: str, filename: str) -> bool:\n        if not self.include_patterns:\n            return True\n        for pattern in self.include_patterns:\n            if fnmatch.fnmatch(path, pattern) or fnmatch.fnmatch(filename, pattern):\n                return True\n        return False\n\n    def _count_files(self, tree: Dict) -> int:\n        if tree[\"type\"] == \"file\":\n            return 1\n        return sum(self._count_files(child) for child in tree.get(\"children\", []))\n\n    def _calculate_size(self, tree: Dict) -> float:\n        if tree[\"type\"] == \"file\":\n            return tree.get(\"_size_bytes\", 0) / 1024\n        return sum(self._calculate_size(child) for child in tree.get(\"children\", []))",
    "start_line": 16,
    "end_line": 126,
    "has_docstring": false,
    "docstring": "",
    "parameters": null,
    "node_type": "class",
    "base_classes": null,
    "class_name": null,
    "display_name": "class RepoAnalyzer",
    "component_id": "codewiki.src.be.dependency_analyzer.analysis.repo_analyzer.RepoAnalyzer"
  },
  "codewiki.src.be.dependency_analyzer.analyzers.c.TreeSitterCAnalyzer": {
    "id": "codewiki.src.be.dependency_analyzer.analyzers.c.TreeSitterCAnalyzer",
    "name": "TreeSitterCAnalyzer",
    "component_type": "class",
    "file_path": "/Users/wufagang/project/aiopen/CodeWiki/codewiki/src/be/dependency_analyzer/analyzers/c.py",
    "relative_path": "codewiki/src/be/dependency_analyzer/analyzers/c.py",
    "depends_on": [
      "codewiki.src.be.dependency_analyzer.models.core.CallRelationship",
      "codewiki.src.be.dependency_analyzer.models.core.Node"
    ],
    "source_code": "class TreeSitterCAnalyzer:\n\tdef __init__(self, file_path: str, content: str, repo_path: str = None):\n\t\tself.file_path = Path(file_path)\n\t\tself.content = content\n\t\tself.repo_path = repo_path or \"\"\n\t\tself.nodes: List[Node] = []\n\t\tself.call_relationships: List[CallRelationship] = []\n\t\tself._analyze()\n\t\n\tdef _get_module_path(self) -> str:\n\t\tif self.repo_path:\n\t\t\ttry:\n\t\t\t\trel_path = os.path.relpath(str(self.file_path), self.repo_path)\n\t\t\texcept ValueError:\n\t\t\t\trel_path = str(self.file_path)\n\t\telse:\n\t\t\trel_path = str(self.file_path)\n\t\t\n\t\tfor ext in ['.c', '.h']:\n\t\t\tif rel_path.endswith(ext):\n\t\t\t\trel_path = rel_path[:-len(ext)]\n\t\t\t\tbreak\n\t\treturn rel_path.replace('/', '.').replace('\\\\', '.')\n\t\n\tdef _get_relative_path(self) -> str:\n\t\tif self.repo_path:\n\t\t\ttry:\n\t\t\t\treturn os.path.relpath(str(self.file_path), self.repo_path)\n\t\t\texcept ValueError:\n\t\t\t\treturn str(self.file_path)\n\t\telse:\n\t\t\treturn str(self.file_path)\n\t\n\tdef _get_component_id(self, name: str) -> str:\n\t\tmodule_path = self._get_module_path()\n\t\treturn f\"{module_path}.{name}\" if module_path else name\n\n\tdef _analyze(self):\n\t\tlanguage_capsule = tree_sitter_c.language()\n\t\tc_language = Language(language_capsule)\n\t\tparser = Parser(c_language)\n\t\ttree = parser.parse(bytes(self.content, \"utf8\"))\n\t\troot = tree.root_node\n\t\tlines = self.content.splitlines()\n\t\t\n\t\ttop_level_nodes = {}\n\t\t\n\t\t# collect all top-level nodes using recursive traversal\n\t\tself._extract_nodes(root, top_level_nodes, lines)\n\t\t\n\t\t# extract relationships between top-level nodes\n\t\tself._extract_relationships(root, top_level_nodes)\n\t\n\tdef _extract_nodes(self, node, top_level_nodes, lines):\n\t\t\"\"\"Recursively extract top-level nodes (functions, structs, and global variables).\"\"\"\n\t\tnode_type = None\n\t\tnode_name = None\n\t\t\n\t\tif node.type == \"function_definition\":\n\t\t\tnode_type = \"function\"\n\t\t\t# look for function_declarator\n\t\t\tdeclarator = next((c for c in node.children if c.type == \"function_declarator\"), None)\n\t\t\tif declarator:\n\t\t\t\tidentifier = next((c for c in declarator.children if c.type == \"identifier\"), None)\n\t\t\t\tif identifier:\n\t\t\t\t\tnode_name = identifier.text.decode()\n\t\telif node.type == \"struct_specifier\":\n\t\t\t# Extract struct definitions: struct Name { ... }\n\t\t\tnode_type = \"struct\"\n\t\t\t# Find type_identifier that represents the struct name\n\t\t\tfor child in node.children:\n\t\t\t\tif child.type == \"type_identifier\":\n\t\t\t\t\tnode_name = child.text.decode()\n\t\t\t\t\tbreak\n\t\telif node.type == \"type_definition\":\n\t\t\t# Handle typedef struct definitions: typedef struct { ... } Name;\n\t\t\t# Check if this typedef contains a struct\n\t\t\tstruct_spec = next((c for c in node.children if c.type == \"struct_specifier\"), None)\n\t\t\tif struct_spec:\n\t\t\t\tnode_type = \"struct\"\n\t\t\t\t# The typedef name is the type_identifier at the end\n\t\t\t\ttype_declarator = next((c for c in node.children if c.type == \"type_identifier\"), None)\n\t\t\t\tif type_declarator:\n\t\t\t\t\tnode_name = type_declarator.text.decode()\n\t\telif node.type == \"declaration\":\n\t\t\tif self._is_global_variable(node):\n\t\t\t\tnode_type = \"variable\"\n\t\t\t\tfor child in node.children:\n\t\t\t\t\tif child.type == \"init_declarator\":\n\t\t\t\t\t\tidentifier = next((c for c in child.children if c.type == \"identifier\"), None)\n\t\t\t\t\t\tif identifier:\n\t\t\t\t\t\t\tnode_name = identifier.text.decode()\n\t\t\t\t\t\t\tbreak\n\t\t\t\t\t\tpointer_declarator = next((c for c in child.children if c.type == \"pointer_declarator\"), None)\n\t\t\t\t\t\tif pointer_declarator:\n\t\t\t\t\t\t\tidentifier = next((c for c in pointer_declarator.children if c.type == \"identifier\"), None)\n\t\t\t\t\t\t\tif identifier:\n\t\t\t\t\t\t\t\tnode_name = identifier.text.decode()\n\t\t\t\t\t\t\t\tbreak\n\t\t\t\t\telif child.type == \"identifier\":\n\t\t\t\t\t\tnode_name = child.text.decode()\n\t\t\t\t\t\tbreak\n\t\t\n\t\tif node_type and node_name:\n\t\t\tcomponent_id = self._get_component_id(node_name)\n\t\t\trelative_path = self._get_relative_path()\n\t\t\tnode_obj = Node(\n\t\t\t\tid=component_id,\n\t\t\t\tname=node_name,\n\t\t\t\tcomponent_type=node_type,\n\t\t\t\tfile_path=str(self.file_path),\n\t\t\t\trelative_path=relative_path,\n\t\t\t\tsource_code=\"\\n\".join(lines[node.start_point[0]:node.end_point[0]+1]),\n\t\t\t\tstart_line=node.start_point[0]+1,\n\t\t\t\tend_line=node.end_point[0]+1,\n\t\t\t\thas_docstring=False,\n\t\t\t\tdocstring=\"\",\n\t\t\t\tparameters=None,\n\t\t\t\tnode_type=node_type,\n\t\t\t\tbase_classes=None,\n\t\t\t\tclass_name=None,\n\t\t\t\tdisplay_name=f\"{node_type} {node_name}\",\n\t\t\t\tcomponent_id=component_id\n\t\t\t)\n\n\t\t\tif node_type in [\"function\", \"struct\"]:\n\t\t\t\tself.nodes.append(node_obj)\n\t\t\ttop_level_nodes[node_name] = node_obj\n\t\t\n\t\tfor child in node.children:\n\t\t\tself._extract_nodes(child, top_level_nodes, lines)\n\t\n\tdef _is_global_variable(self, node) -> bool:\n\t\tparent = node.parent\n\t\twhile parent:\n\t\t\tif parent.type == \"function_definition\":\n\t\t\t\treturn False\n\t\t\tparent = parent.parent\n\t\treturn True\n\t\n\tdef _extract_relationships(self, node, top_level_nodes):\n\t\t\"\"\"Extract various types of relationships between top-level nodes.\"\"\"\n\t\t\n\t\t# 1. function calls other functions\n\t\tif node.type == \"call_expression\":\n\t\t\tcontaining_function = self._find_containing_function(node, top_level_nodes)\n\t\t\tif containing_function:\n\t\t\t\tcontaining_function_id = self._get_component_id(containing_function)\n\t\t\t\t\n\t\t\t\t# Get called function name\n\t\t\t\tfunction_node = next((c for c in node.children if c.type == \"identifier\"), None)\n\t\t\t\tif function_node:\n\t\t\t\t\tcalled_function = function_node.text.decode()\n\t\t\t\t\tif not self._is_system_function(called_function):\n\t\t\t\t\t\tself.call_relationships.append(CallRelationship(\n\t\t\t\t\t\t\tcaller=containing_function_id,\n\t\t\t\t\t\t\tcallee=called_function,  # Use simple name for cross-file resolution\n\t\t\t\t\t\t\tcall_line=node.start_point[0]+1,\n\t\t\t\t\t\t\tis_resolved=False  # Let CallGraphAnalyzer resolve\n\t\t\t\t\t\t))\n\t\t\n\t\t# 2. function uses global variables\n\t\tif node.type == \"identifier\":\n\t\t\tcontaining_function = self._find_containing_function(node, top_level_nodes)\n\t\t\tif containing_function:\n\t\t\t\tvar_name = node.text.decode()\n\t\t\t\t# Check if this identifier refers to a global variable\n\t\t\t\tif var_name in top_level_nodes and top_level_nodes[var_name].component_type == \"variable\":\n\t\t\t\t\tcontaining_function_id = self._get_component_id(containing_function)\n\t\t\t\t\tvar_component_id = self._get_component_id(var_name)\n\t\t\t\t\tself.call_relationships.append(CallRelationship(\n\t\t\t\t\t\tcaller=containing_function_id,\n\t\t\t\t\t\tcallee=var_component_id,\n\t\t\t\t\t\tcall_line=node.start_point[0]+1,\n\t\t\t\t\t\tis_resolved=True  # Local file relationship\n\t\t\t\t\t))\n\t\t\n\t\t# Recursively process children\n\t\tfor child in node.children:\n\t\t\tself._extract_relationships(child, top_level_nodes)\n\t\n\tdef _find_containing_function(self, node, top_level_nodes):\n\t\t\"\"\"Find the function that contains this node.\"\"\"\n\t\tcurrent = node.parent\n\t\twhile current:\n\t\t\tif current.type == \"function_definition\":\n\t\t\t\t# Get function name\n\t\t\t\tdeclarator = next((c for c in current.children if c.type == \"function_declarator\"), None)\n\t\t\t\tif declarator:\n\t\t\t\t\tidentifier = next((c for c in declarator.children if c.type == \"identifier\"), None)\n\t\t\t\t\tif identifier:\n\t\t\t\t\t\tfunc_name = identifier.text.decode()\n\t\t\t\t\t\tif func_name in top_level_nodes:\n\t\t\t\t\t\t\treturn func_name\n\t\t\tcurrent = current.parent\n\t\treturn None\n\t\n\tdef _is_system_function(self, func_name: str) -> bool:\n\t\t\"\"\"Check if function is a system/library function.\"\"\"\n\t\t# Common C library functions\n\t\tsystem_functions = {\n\t\t\t\"printf\", \"scanf\", \"malloc\", \"free\", \"strlen\", \"strcpy\", \"strcmp\", \n\t\t\t\"memcpy\", \"memset\", \"exit\", \"abort\", \"fopen\", \"fclose\", \"fread\", \"fwrite\",\n\t\t\t\"SDL_Init\", \"SDL_CreateWindow\", \"SDL_Log\", \"SDL_GetError\", \"SDL_Quit\"\n\t\t}\n\t\treturn func_name in system_functions",
    "start_line": 13,
    "end_line": 218,
    "has_docstring": false,
    "docstring": "",
    "parameters": null,
    "node_type": "class",
    "base_classes": null,
    "class_name": null,
    "display_name": "class TreeSitterCAnalyzer",
    "component_id": "codewiki.src.be.dependency_analyzer.analyzers.c.TreeSitterCAnalyzer"
  },
  "codewiki.src.be.dependency_analyzer.analyzers.c.analyze_c_file": {
    "id": "codewiki.src.be.dependency_analyzer.analyzers.c.analyze_c_file",
    "name": "analyze_c_file",
    "component_type": "function",
    "file_path": "/Users/wufagang/project/aiopen/CodeWiki/codewiki/src/be/dependency_analyzer/analyzers/c.py",
    "relative_path": "codewiki/src/be/dependency_analyzer/analyzers/c.py",
    "depends_on": [
      "codewiki.src.be.dependency_analyzer.analyzers.c.TreeSitterCAnalyzer"
    ],
    "source_code": "def analyze_c_file(file_path: str, content: str, repo_path: str = None) -> Tuple[List[Node], List[CallRelationship]]:\n\tanalyzer = TreeSitterCAnalyzer(file_path, content, repo_path)\n\treturn analyzer.nodes, analyzer.call_relationships",
    "start_line": 220,
    "end_line": 222,
    "has_docstring": false,
    "docstring": "",
    "parameters": [
      "file_path",
      "content",
      "repo_path"
    ],
    "node_type": "function",
    "base_classes": null,
    "class_name": null,
    "display_name": "function analyze_c_file",
    "component_id": "codewiki.src.be.dependency_analyzer.analyzers.c.analyze_c_file"
  },
  "codewiki.src.be.dependency_analyzer.analyzers.cpp.TreeSitterCppAnalyzer": {
    "id": "codewiki.src.be.dependency_analyzer.analyzers.cpp.TreeSitterCppAnalyzer",
    "name": "TreeSitterCppAnalyzer",
    "component_type": "class",
    "file_path": "/Users/wufagang/project/aiopen/CodeWiki/codewiki/src/be/dependency_analyzer/analyzers/cpp.py",
    "relative_path": "codewiki/src/be/dependency_analyzer/analyzers/cpp.py",
    "depends_on": [
      "codewiki.src.be.dependency_analyzer.models.core.CallRelationship",
      "codewiki.src.be.dependency_analyzer.models.core.Node"
    ],
    "source_code": "class TreeSitterCppAnalyzer:\n\tdef __init__(self, file_path: str, content: str, repo_path: str = None):\n\t\tself.file_path = Path(file_path)\n\t\tself.content = content\n\t\tself.repo_path = repo_path or \"\"\n\t\tself.nodes: List[Node] = []\n\t\tself.call_relationships: List[CallRelationship] = []\n\t\tself._analyze()\n\t\n\tdef _get_module_path(self) -> str:\n\t\tif self.repo_path:\n\t\t\ttry:\n\t\t\t\trel_path = os.path.relpath(str(self.file_path), self.repo_path)\n\t\t\texcept ValueError:\n\t\t\t\trel_path = str(self.file_path)\n\t\telse:\n\t\t\trel_path = str(self.file_path)\n\n\t\tfor ext in ['.cpp', '.cc', '.cxx', '.hpp', '.h']:\n\t\t\tif rel_path.endswith(ext):\n\t\t\t\trel_path = rel_path[:-len(ext)]\n\t\t\t\tbreak\n\t\treturn rel_path.replace('/', '.').replace('\\\\', '.')\n\t\n\tdef _get_relative_path(self) -> str:\n\t\tif self.repo_path:\n\t\t\ttry:\n\t\t\t\treturn os.path.relpath(str(self.file_path), self.repo_path)\n\t\t\texcept ValueError:\n\t\t\t\treturn str(self.file_path)\n\t\telse:\n\t\t\treturn str(self.file_path)\n\t\n\tdef _get_component_id(self, name: str, parent_class: str = None) -> str:\n\t\tmodule_path = self._get_module_path()\n\t\tif parent_class:\n\t\t\treturn f\"{module_path}.{parent_class}.{name}\" if module_path else f\"{parent_class}.{name}\"\n\t\treturn f\"{module_path}.{name}\" if module_path else name\n\n\tdef _analyze(self):\n\t\tlanguage_capsule = tree_sitter_cpp.language()\n\t\tcpp_language = Language(language_capsule)\n\t\tparser = Parser(cpp_language)\n\t\ttree = parser.parse(bytes(self.content, \"utf8\"))\n\t\troot = tree.root_node\n\t\tlines = self.content.splitlines()\n\t\t\n\t\ttop_level_nodes = {}\n\t\t\n\t\t# collect all top-level nodes using recursive traversal\n\t\tself._extract_nodes(root, top_level_nodes, lines)\n\t\t\n\t\t# extract relationships between top-level nodes\n\t\tself._extract_relationships(root, top_level_nodes)\n\t\n\tdef _extract_nodes(self, node, top_level_nodes, lines):\n\t\t\"\"\"Recursively extract top-level nodes (classes, functions, global variables).\"\"\"\n\t\tnode_type = None\n\t\tnode_name = None\n\t\t\n\t\tif node.type == \"class_specifier\":\n\t\t\t# \"class\" + type_identifier + { ... }\n\t\t\tnode_type = \"class\"\n\t\t\t# Find type_identifier that represents the class name\n\t\t\tfor child in node.children:\n\t\t\t\tif child.type == \"type_identifier\":\n\t\t\t\t\tnode_name = child.text.decode()\n\t\t\t\t\tbreak\n\t\telif node.type == \"struct_specifier\":\n\t\t\t# \"struct\" + type_identifier + { ... }\n\t\t\tnode_type = \"struct\"\n\t\t\t# Find type_identifier that represents the struct name\n\t\t\tfor child in node.children:\n\t\t\t\tif child.type == \"type_identifier\":\n\t\t\t\t\tnode_name = child.text.decode()\n\t\t\t\t\tbreak\n\t\telif node.type == \"function_definition\":\n\t\t\t# Check if this is inside a class or function\n\t\t\tcontaining_class = self._find_containing_class_for_method(node)\n\t\t\tif containing_class:\n\t\t\t\tnode_type = \"method\"\n\t\t\telse:\n\t\t\t\tnode_type = \"function\"\n\t\t\t\n\t\t\tdeclarator = next((c for c in node.children if c.type == \"function_declarator\"), None)\n\t\t\tif declarator:\n\t\t\t\tfor child in declarator.children:\n\t\t\t\t\tif child.type == \"identifier\":\n\t\t\t\t\t\tnode_name = child.text.decode()\n\t\t\t\t\t\tbreak\n\t\t\t\t\telif child.type == \"field_identifier\":\n\t\t\t\t\t\tnode_name = child.text.decode()\n\t\t\t\t\t\tbreak\n\t\t\t\t\telif child.type == \"qualified_identifier\":\n\t\t\t\t\t\tidentifiers = [c for c in child.children if c.type == \"identifier\"]\n\t\t\t\t\t\tif identifiers:\n\t\t\t\t\t\t\tnode_name = identifiers[-1].text.decode()\n\t\t\t\t\t\t\tbreak\n\t\telif node.type == \"declaration\":\n\t\t\tif self._is_global_variable(node):\n\t\t\t\tnode_type = \"variable\"\n\t\t\t\tfor child in node.children:\n\t\t\t\t\tif child.type == \"init_declarator\":\n\t\t\t\t\t\tidentifier = next((c for c in child.children if c.type == \"identifier\"), None)\n\t\t\t\t\t\tif identifier:\n\t\t\t\t\t\t\tnode_name = identifier.text.decode()\n\t\t\t\t\t\t\tbreak\n\t\t\t\t\telif child.type == \"identifier\":\n\t\t\t\t\t\tnode_name = child.text.decode()\n\t\t\t\t\t\tbreak\n\t\telif node.type == \"namespace_definition\":\n\t\t\tnode_type = \"namespace\"\n\t\t\tfound_namespace_keyword = False\n\t\t\tfor child in node.children:\n\t\t\t\tif child.type == \"namespace\":\n\t\t\t\t\tfound_namespace_keyword = True\n\t\t\t\telif found_namespace_keyword and child.type == \"identifier\":\n\t\t\t\t\tnode_name = child.text.decode()\n\t\t\t\t\tbreak\n\t\t\n\t\tif node_type and node_name:\n\t\t\tif node_type == \"method\":\n\t\t\t\tcomponent_id = self._get_component_id(node_name, containing_class)\n\t\t\t\ttop_level_key = component_id\n\t\t\telse:\n\t\t\t\tcomponent_id = self._get_component_id(node_name)\n\t\t\t\ttop_level_key = node_name\n\t\t\t\t\n\t\t\trelative_path = self._get_relative_path()\n\t\t\t\n\t\t\tnode_obj = Node(\n\t\t\t\tid=component_id,\n\t\t\t\tname=node_name,\n\t\t\t\tcomponent_type=node_type,\n\t\t\t\tfile_path=str(self.file_path),\n\t\t\t\trelative_path=relative_path,\n\t\t\t\tsource_code=\"\\n\".join(lines[node.start_point[0]:node.end_point[0]+1]),\n\t\t\t\tstart_line=node.start_point[0]+1,\n\t\t\t\tend_line=node.end_point[0]+1,\n\t\t\t\thas_docstring=False,\n\t\t\t\tdocstring=\"\",\n\t\t\t\tparameters=None,\n\t\t\t\tnode_type=node_type,\n\t\t\t\tbase_classes=None,\n\t\t\t\tclass_name=containing_class if node_type == \"method\" else None,\n\t\t\t\tdisplay_name=f\"{node_type} {node_name}\",\n\t\t\t\tcomponent_id=component_id\n\t\t\t)\n\t\t\t\n\t\t\ttop_level_nodes[top_level_key] = node_obj\n\t\t\t\n\t\t\tif node_type in [\"class\", \"struct\", \"function\"]:\n\t\t\t\tself.nodes.append(node_obj)\n\t\t\n\t\t# Recursively process children\n\t\tfor child in node.children:\n\t\t\tself._extract_nodes(child, top_level_nodes, lines)\n\n\tdef _is_global_variable(self, node) -> bool:\n\t\t\"\"\"Check if a declaration node is a global variable.\"\"\"\n\t\tparent = node.parent\n\t\twhile parent:\n\t\t\tif parent.type in [\"function_definition\", \"class_specifier\", \"struct_specifier\"]:\n\t\t\t\treturn False\n\t\t\tparent = parent.parent\n\t\treturn True\n\n\tdef _find_containing_class_for_method(self, node):\n\t\t\"\"\"Find the class that contains this method definition.\"\"\"\n\t\tcurrent = node.parent\n\t\twhile current:\n\t\t\tif current.type == \"class_specifier\":\n\t\t\t\t# Get class name\n\t\t\t\tfor child in current.children:\n\t\t\t\t\tif child.type == \"type_identifier\":\n\t\t\t\t\t\treturn child.text.decode()\n\t\t\telif current.type == \"struct_specifier\":\n\t\t\t\t# Get struct name \n\t\t\t\tfor child in current.children:\n\t\t\t\t\tif child.type == \"type_identifier\":\n\t\t\t\t\t\treturn child.text.decode()\n\t\t\tcurrent = current.parent\n\t\treturn None\n\n\tdef _extract_relationships(self, node, top_level_nodes):\n\t\tif node.type == \"call_expression\":\n\t\t\tcontaining_function = self._find_containing_function_or_method(node, top_level_nodes)\n\t\t\tif containing_function:\n\t\t\t\tcontaining_function_id = self._get_component_id_for_function(containing_function, top_level_nodes)\n\t\t\t\t\n\t\t\t\t# Get called function name \n\t\t\t\tcalled_function = None\n\t\t\t\tfor child in node.children:\n\t\t\t\t\tif child.type == \"identifier\":\n\t\t\t\t\t\tcalled_function = child.text.decode()\n\t\t\t\t\t\tbreak\n\t\t\t\t\telif child.type == \"field_expression\":\n\t\t\t\t\t\tmethod_name = None\n\t\t\t\t\t\tfor field_child in child.children:\n\t\t\t\t\t\t\tif field_child.type == \"field_identifier\":\n\t\t\t\t\t\t\t\tmethod_name = field_child.text.decode()\n\t\t\t\t\t\t\t\tbreak\n\t\t\t\t\t\tif method_name:\n\t\t\t\t\t\t\tcalled_function = method_name\n\t\t\t\t\t\t\tbreak\n\t\t\t\t\n\t\t\t\tif called_function and not self._is_system_function(called_function):\n\t\t\t\t\ttarget_class = self._find_class_containing_method(called_function, top_level_nodes)\n\t\t\t\t\t\n\t\t\t\t\tif target_class:\n\t\t\t\t\t\ttarget_class_id = self._get_component_id(target_class)\n\t\t\t\t\t\tself.call_relationships.append(CallRelationship(\n\t\t\t\t\t\t\tcaller=containing_function_id,\n\t\t\t\t\t\t\tcallee=target_class_id,\n\t\t\t\t\t\t\tcall_line=node.start_point[0]+1,\n\t\t\t\t\t\t\trelationship_type=\"calls\"\n\t\t\t\t\t\t))\n\t\t\t\t\telif called_function in top_level_nodes:\n\t\t\t\t\t\tcalled_function_id = self._get_component_id(called_function)\n\t\t\t\t\t\tself.call_relationships.append(CallRelationship(\n\t\t\t\t\t\t\tcaller=containing_function_id,\n\t\t\t\t\t\t\tcallee=called_function_id,\n\t\t\t\t\t\t\tcall_line=node.start_point[0]+1,\n\t\t\t\t\t\t\trelationship_type=\"calls\"\n\t\t\t\t\t\t))\n\t\t\n\t\telif node.type == \"base_class_clause\":\n\t\t\t# Find the containing class\n\t\t\tcontaining_class = self._find_containing_class(node)\n\t\t\tif containing_class:\n\t\t\t\t# Extract base class names\n\t\t\t\tfor child in node.children:\n\t\t\t\t\tif child.type == \"type_identifier\":\n\t\t\t\t\t\tbase_class = child.text.decode()\n\t\t\t\t\t\tcontaining_class_id = self._get_component_id(containing_class)\n\t\t\t\t\t\tself.call_relationships.append(CallRelationship(\n\t\t\t\t\t\t\tcaller=containing_class_id,\n\t\t\t\t\t\t\tcallee=base_class,\n\t\t\t\t\t\t\tcall_line=node.start_point[0]+1,\n\t\t\t\t\t\t\trelationship_type=\"inherits\"\n\t\t\t\t\t\t))\n\t\t\n\t\telif node.type == \"new_expression\":\n\t\t\tcontaining_function = self._find_containing_function_or_method(node, top_level_nodes)\n\t\t\tif containing_function:\n\t\t\t\tcontaining_function_id = self._get_component_id_for_function(containing_function, top_level_nodes)\n\t\t\t\t\n\t\t\t\t# Get the class being instantiated\n\t\t\t\tfor child in node.children:\n\t\t\t\t\tif child.type == \"type_identifier\":\n\t\t\t\t\t\tclass_name = child.text.decode()\n\t\t\t\t\t\tif class_name in top_level_nodes:\n\t\t\t\t\t\t\tclass_id = self._get_component_id(class_name)\n\t\t\t\t\t\t\tself.call_relationships.append(CallRelationship(\n\t\t\t\t\t\t\t\tcaller=containing_function_id,\n\t\t\t\t\t\t\t\tcallee=class_id,\n\t\t\t\t\t\t\t\tcall_line=node.start_point[0]+1,\n\t\t\t\t\t\t\t\trelationship_type=\"creates\"\n\t\t\t\t\t\t\t))\n\t\t\t\t\t\tbreak\n\t\t\n\t\telif node.type == \"identifier\":\n\t\t\tparent = node.parent\n\t\t\tif parent and parent.type not in [\"function_definition\", \"class_specifier\", \"declaration\", \"function_declarator\"]:\n\t\t\t\tvar_name = node.text.decode()\n\t\t\t\tif var_name in top_level_nodes and top_level_nodes[var_name].component_type == \"variable\":\n\t\t\t\t\tcontaining_function = self._find_containing_function_or_method(node, top_level_nodes)\n\t\t\t\t\tif containing_function and containing_function != var_name:\n\t\t\t\t\t\tcontaining_function_id = self._get_component_id_for_function(containing_function, top_level_nodes)\n\t\t\t\t\t\tself.call_relationships.append(CallRelationship(\n\t\t\t\t\t\t\tcaller=containing_function_id,\n\t\t\t\t\t\t\tcallee=var_name,\n\t\t\t\t\t\t\tcall_line=node.start_point[0]+1,\n\t\t\t\t\t\t\trelationship_type=\"uses\"\n\t\t\t\t\t\t))\n\t\t\n\t\t# Recursively process children\n\t\tfor child in node.children:\n\t\t\tself._extract_relationships(child, top_level_nodes)\n\n\tdef _find_containing_function(self, node, top_level_nodes):\n\t\t\"\"\"Find the function that contains this node.\"\"\"\n\t\tcurrent = node.parent\n\t\twhile current:\n\t\t\tif current.type == \"function_definition\":\n\t\t\t\t# Get function name\n\t\t\t\tdeclarator = next((c for c in current.children if c.type == \"function_declarator\"), None)\n\t\t\t\tif declarator:\n\t\t\t\t\tidentifier = next((c for c in declarator.children if c.type == \"identifier\"), None)\n\t\t\t\t\tif identifier:\n\t\t\t\t\t\tfunc_name = identifier.text.decode()\n\t\t\t\t\t\tif func_name in top_level_nodes:\n\t\t\t\t\t\t\treturn func_name\n\t\t\tcurrent = current.parent\n\t\treturn None\n\n\tdef _find_containing_function_or_method(self, node, top_level_nodes):\n\t\t\"\"\"Find the function or method that contains this node.\"\"\"\n\t\tcurrent = node.parent\n\t\twhile current:\n\t\t\tif current.type == \"function_definition\":\n\t\t\t\tdeclarator = next((c for c in current.children if c.type == \"function_declarator\"), None)\n\t\t\t\tif declarator:\n\t\t\t\t\tidentifier = next((c for c in declarator.children if c.type == \"identifier\"), None)\n\t\t\t\t\tif identifier:\n\t\t\t\t\t\tfunc_name = identifier.text.decode()\n\t\t\t\t\t\treturn func_name\n\t\t\tcurrent = current.parent\n\t\treturn None\n\n\tdef _get_component_id_for_function(self, func_name, top_level_nodes):\n\t\tif func_name in top_level_nodes:\n\t\t\tnode_obj = top_level_nodes[func_name]\n\t\t\tif hasattr(node_obj, 'class_name') and node_obj.class_name:\n\t\t\t\treturn self._get_component_id(func_name, node_obj.class_name)\n\t\t\telse:\n\t\t\t\treturn self._get_component_id(func_name)\n\t\treturn self._get_component_id(func_name)\n\n\tdef _find_containing_class(self, node):\n\t\t\"\"\"Find the class that contains this node.\"\"\"\n\t\tcurrent = node.parent\n\t\twhile current:\n\t\t\tif current.type == \"class_specifier\":\n\t\t\t\t# Get class name\n\t\t\t\tfor child in current.children:\n\t\t\t\t\tif child.type == \"type_identifier\":\n\t\t\t\t\t\treturn child.text.decode()\n\t\t\tcurrent = current.parent\n\t\treturn None\n\n\tdef _is_system_function(self, func_name: str) -> bool:\n\t\t\"\"\"Check if function is a system/library function.\"\"\"\n\t\tsystem_functions = {\n\t\t\t'printf', 'scanf', 'malloc', 'free', 'strlen', 'strcpy', 'strcmp',\n\t\t\t'cout', 'cin', 'endl', 'std', 'new', 'delete'\n\t\t}\n\t\treturn func_name in system_functions\n\n\tdef _find_class_containing_method(self, method_name, top_level_nodes):\n\t\tfor node_name, node_obj in top_level_nodes.items():\n\t\t\tif node_obj.component_type in [\"class\", \"struct\"]:\n\t\t\t\tif self._class_has_method(node_obj, method_name):\n\t\t\t\t\treturn node_name\n\t\treturn None\n\n\tdef _class_has_method(self, class_node, method_name):\n\t\tlines = class_node.source_code.split('\\n')\n\t\tfor line in lines:\n\t\t\tif f'{method_name}(' in line and ('void' in line or 'int' in line or 'bool' in line or class_node.name in line):\n\t\t\t\treturn True\n\t\treturn False",
    "start_line": 13,
    "end_line": 364,
    "has_docstring": false,
    "docstring": "",
    "parameters": null,
    "node_type": "class",
    "base_classes": null,
    "class_name": null,
    "display_name": "class TreeSitterCppAnalyzer",
    "component_id": "codewiki.src.be.dependency_analyzer.analyzers.cpp.TreeSitterCppAnalyzer"
  },
  "codewiki.src.be.dependency_analyzer.analyzers.cpp.analyze_cpp_file": {
    "id": "codewiki.src.be.dependency_analyzer.analyzers.cpp.analyze_cpp_file",
    "name": "analyze_cpp_file",
    "component_type": "function",
    "file_path": "/Users/wufagang/project/aiopen/CodeWiki/codewiki/src/be/dependency_analyzer/analyzers/cpp.py",
    "relative_path": "codewiki/src/be/dependency_analyzer/analyzers/cpp.py",
    "depends_on": [
      "codewiki.src.be.dependency_analyzer.analyzers.cpp.TreeSitterCppAnalyzer"
    ],
    "source_code": "def analyze_cpp_file(file_path: str, content: str, repo_path: str = None) -> Tuple[List[Node], List[CallRelationship]]:\n\tanalyzer = TreeSitterCppAnalyzer(file_path, content, repo_path)\n\treturn analyzer.nodes, analyzer.call_relationships",
    "start_line": 366,
    "end_line": 368,
    "has_docstring": false,
    "docstring": "",
    "parameters": [
      "file_path",
      "content",
      "repo_path"
    ],
    "node_type": "function",
    "base_classes": null,
    "class_name": null,
    "display_name": "function analyze_cpp_file",
    "component_id": "codewiki.src.be.dependency_analyzer.analyzers.cpp.analyze_cpp_file"
  },
  "codewiki.src.be.dependency_analyzer.analyzers.csharp.TreeSitterCSharpAnalyzer": {
    "id": "codewiki.src.be.dependency_analyzer.analyzers.csharp.TreeSitterCSharpAnalyzer",
    "name": "TreeSitterCSharpAnalyzer",
    "component_type": "class",
    "file_path": "/Users/wufagang/project/aiopen/CodeWiki/codewiki/src/be/dependency_analyzer/analyzers/csharp.py",
    "relative_path": "codewiki/src/be/dependency_analyzer/analyzers/csharp.py",
    "depends_on": [
      "codewiki.src.be.dependency_analyzer.models.core.CallRelationship",
      "codewiki.src.be.dependency_analyzer.models.core.Node"
    ],
    "source_code": "class TreeSitterCSharpAnalyzer:\n\tdef __init__(self, file_path: str, content: str, repo_path: str = None):\n\t\tself.file_path = Path(file_path)\n\t\tself.content = content\n\t\tself.repo_path = repo_path or \"\"\n\t\tself.nodes: List[Node] = []\n\t\tself.call_relationships: List[CallRelationship] = []\n\t\tself._analyze()\n\t\n\tdef _get_module_path(self) -> str:\n\t\tif self.repo_path:\n\t\t\ttry:\n\t\t\t\trel_path = os.path.relpath(str(self.file_path), self.repo_path)\n\t\t\texcept ValueError:\n\t\t\t\trel_path = str(self.file_path)\n\t\telse:\n\t\t\trel_path = str(self.file_path)\n\t\t\n\t\tfor ext in ['.cs']:\n\t\t\tif rel_path.endswith(ext):\n\t\t\t\trel_path = rel_path[:-len(ext)]\n\t\t\t\tbreak\n\t\treturn rel_path.replace('/', '.').replace('\\\\', '.')\n\t\n\tdef _get_relative_path(self) -> str:\n\t\tif self.repo_path:\n\t\t\ttry:\n\t\t\t\treturn os.path.relpath(str(self.file_path), self.repo_path)\n\t\t\texcept ValueError:\n\t\t\t\treturn str(self.file_path)\n\t\telse:\n\t\t\treturn str(self.file_path)\n\t\n\tdef _get_component_id(self, name: str) -> str:\n\t\tmodule_path = self._get_module_path()\n\t\treturn f\"{module_path}.{name}\" if module_path else name\n\n\tdef _analyze(self):\n\t\tlanguage_capsule = tree_sitter_c_sharp.language()\n\t\tcs_language = Language(language_capsule)\n\t\tparser = Parser(cs_language)\n\t\ttree = parser.parse(bytes(self.content, \"utf8\"))\n\t\troot = tree.root_node\n\t\tlines = self.content.splitlines()\n\t\t\n\t\ttop_level_nodes = {}\n\t\n\t\tself._extract_nodes(root, top_level_nodes, lines)\n\t\t\n\t\tself._extract_relationships(root, top_level_nodes)\n\t\n\tdef _extract_nodes(self, node, top_level_nodes, lines):\n\t\tnode_type = None\n\t\tnode_name = None\n\t\t\n\t\tif node.type == \"class_declaration\":\n\t\t\t# modifiers + class + identifier + body\n\t\t\tis_abstract = any(c.type == \"modifier\" and \"abstract\" in c.text.decode() for c in node.children)\n\t\t\tis_static = any(c.type == \"modifier\" and \"static\" in c.text.decode() for c in node.children)\n\t\t\tif is_static:\n\t\t\t\tnode_type = \"static class\"\n\t\t\telif is_abstract:\n\t\t\t\tnode_type = \"abstract class\"\n\t\t\telse:\n\t\t\t\tnode_type = \"class\"\n\t\t\t# find identifier that comes after class keyword\n\t\t\tfound_class_keyword = False\n\t\t\tfor child in node.children:\n\t\t\t\tif child.type == \"class\":\n\t\t\t\t\tfound_class_keyword = True\n\t\t\t\telif found_class_keyword and child.type == \"identifier\":\n\t\t\t\t\tnode_name = child.text.decode()\n\t\t\t\t\tbreak\n\t\telif node.type == \"interface_declaration\":\n\t\t\tnode_type = \"interface\"\n\t\t\t# find identifier that comes after interface keyword\n\t\t\tfound_interface_keyword = False\n\t\t\tfor child in node.children:\n\t\t\t\tif child.type == \"interface\":\n\t\t\t\t\tfound_interface_keyword = True\n\t\t\t\telif found_interface_keyword and child.type == \"identifier\":\n\t\t\t\t\tnode_name = child.text.decode()\n\t\t\t\t\tbreak\n\t\telif node.type == \"struct_declaration\":\n\t\t\tnode_type = \"struct\"\n\t\t\t# find identifier that comes after struct keyword\n\t\t\tfound_struct_keyword = False\n\t\t\tfor child in node.children:\n\t\t\t\tif child.type == \"struct\":\n\t\t\t\t\tfound_struct_keyword = True\n\t\t\t\telif found_struct_keyword and child.type == \"identifier\":\n\t\t\t\t\tnode_name = child.text.decode()\n\t\t\t\t\tbreak\n\t\telif node.type == \"enum_declaration\":\n\t\t\tnode_type = \"enum\"\n\t\t\t# find identifier that comes after enum keyword\n\t\t\tfound_enum_keyword = False\n\t\t\tfor child in node.children:\n\t\t\t\tif child.type == \"enum\":\n\t\t\t\t\tfound_enum_keyword = True\n\t\t\t\telif found_enum_keyword and child.type == \"identifier\":\n\t\t\t\t\tnode_name = child.text.decode()\n\t\t\t\t\tbreak\n\t\telif node.type == \"record_declaration\":\n\t\t\tnode_type = \"record\"\n\t\t\t# find identifier that comes after record keyword\n\t\t\tfound_record_keyword = False\n\t\t\tfor child in node.children:\n\t\t\t\tif child.type == \"record\":\n\t\t\t\t\tfound_record_keyword = True\n\t\t\t\telif found_record_keyword and child.type == \"identifier\":\n\t\t\t\t\tnode_name = child.text.decode()\n\t\t\t\t\tbreak\n\t\telif node.type == \"delegate_declaration\":\n\t\t\tnode_type = \"delegate\"\n\t\t\tfor child in node.children:\n\t\t\t\tif child.type == \"identifier\":\n\t\t\t\t\tnode_name = child.text.decode()\n\t\t\t\t\tbreak\n\t\t\n\t\tif node_type and node_name:\n\t\t\tcomponent_id = self._get_component_id(node_name)\n\t\t\trelative_path = self._get_relative_path()\n\t\t\tnode_obj = Node(\n\t\t\t\tid=component_id,\n\t\t\t\tname=node_name,\n\t\t\t\tcomponent_type=node_type,\n\t\t\t\tfile_path=str(self.file_path),\n\t\t\t\trelative_path=relative_path,\n\t\t\t\tsource_code=\"\\n\".join(lines[node.start_point[0]:node.end_point[0]+1]),\n\t\t\t\tstart_line=node.start_point[0]+1,\n\t\t\t\tend_line=node.end_point[0]+1,\n\t\t\t\thas_docstring=False,\n\t\t\t\tdocstring=\"\",\n\t\t\t\tparameters=None,\n\t\t\t\tnode_type=node_type,\n\t\t\t\tbase_classes=None,\n\t\t\t\tclass_name=None,\n\t\t\t\tdisplay_name=f\"{node_type} {node_name}\",\n\t\t\t\tcomponent_id=component_id\n\t\t\t)\n\t\t\tself.nodes.append(node_obj)\n\t\t\ttop_level_nodes[node_name] = node_obj\n\t\t\n\t\tfor child in node.children:\n\t\t\tself._extract_nodes(child, top_level_nodes, lines)\n\t\n\tdef _extract_relationships(self, node, top_level_nodes):\n\t\tcontaining_class = self._find_containing_class(node, top_level_nodes)\n\t\t\n\t\tif node.type == \"class_declaration\":\n\t\t\tclass_name = self._get_identifier_name_cs(node)\n\t\t\tif class_name:\n\t\t\t\tclass_component_id = self._get_component_id(class_name)\n\t\t\t\t\n\t\t\t\tbase_list = next((c for c in node.children if c.type == \"base_list\"), None)\n\t\t\t\tif base_list:\n\t\t\t\t\tfor child in base_list.children:\n\t\t\t\t\t\tif child.type == \"identifier\":\n\t\t\t\t\t\t\tbase_name = child.text.decode()\n\t\t\t\t\t\t\tif base_name in [n.name for n in top_level_nodes.values()]:\n\t\t\t\t\t\t\t\tbase_component_id = self._get_component_id(base_name)\n\t\t\t\t\t\t\t\tself.call_relationships.append(CallRelationship(\n\t\t\t\t\t\t\t\t\tcaller=class_component_id,\n\t\t\t\t\t\t\t\t\tcallee=base_component_id,\n\t\t\t\t\t\t\t\t\tcall_line=node.start_point[0]+1,\n\t\t\t\t\t\t\t\t\tis_resolved=True\n\t\t\t\t\t\t\t\t))\n\t\t\n\t\telif node.type == \"property_declaration\":\n\t\t\tif containing_class:\n\t\t\t\tcontaining_class_id = self._get_component_id(containing_class)\n\t\t\t\ttype_identifiers = [c for c in node.children if c.type == \"identifier\"]\n\t\t\t\tif len(type_identifiers) >= 2:\n\t\t\t\t\tproperty_type = type_identifiers[0].text.decode()\n\t\t\t\t\tif property_type and not self._is_primitive_type(property_type):\n\t\t\t\t\t\tself.call_relationships.append(CallRelationship(\n\t\t\t\t\t\t\tcaller=containing_class_id,\n\t\t\t\t\t\t\tcallee=property_type,  \n\t\t\t\t\t\t\tcall_line=node.start_point[0]+1,\n\t\t\t\t\t\t\tis_resolved=False  \n\t\t\t\t\t\t))\n\t\t\n\t\telif node.type == \"field_declaration\":\n\t\t\tif containing_class:\n\t\t\t\tcontaining_class_id = self._get_component_id(containing_class)\n\t\t\t\ttype_node = next((c for c in node.children if c.type == \"identifier\"), None)\n\t\t\t\tif type_node:\n\t\t\t\t\tfield_type = type_node.text.decode()\n\t\t\t\t\tif field_type and not self._is_primitive_type(field_type):\n\t\t\t\t\t\tself.call_relationships.append(CallRelationship(\n\t\t\t\t\t\t\tcaller=containing_class_id,\n\t\t\t\t\t\t\tcallee=field_type, \n\t\t\t\t\t\t\tcall_line=node.start_point[0]+1,\n\t\t\t\t\t\t\tis_resolved=False  \n\t\t\t\t\t\t))\n\t\t\n\t\telif node.type == \"method_declaration\":\n\t\t\tif containing_class:\n\t\t\t\tcontaining_class_id = self._get_component_id(containing_class)\n\t\t\t\tparam_list = next((c for c in node.children if c.type == \"parameter_list\"), None)\n\t\t\t\tif param_list:\n\t\t\t\t\tfor child in param_list.children:\n\t\t\t\t\t\tif child.type == \"parameter\":\n\t\t\t\t\t\t\ttype_node = next((c for c in child.children if c.type == \"identifier\"), None)\n\t\t\t\t\t\t\tif type_node:\n\t\t\t\t\t\t\t\tparam_type = type_node.text.decode()\n\t\t\t\t\t\t\t\tif param_type and not self._is_primitive_type(param_type):\n\t\t\t\t\t\t\t\t\tself.call_relationships.append(CallRelationship(\n\t\t\t\t\t\t\t\t\t\tcaller=containing_class_id,\n\t\t\t\t\t\t\t\t\t\tcallee=param_type,  \n\t\t\t\t\t\t\t\t\t\tcall_line=node.start_point[0]+1,\n\t\t\t\t\t\t\t\t\t\tis_resolved=False  \n\t\t\t\t\t\t\t\t\t))\n\t\t\n\t\t# Recursively process children\n\t\tfor child in node.children:\n\t\t\tself._extract_relationships(child, top_level_nodes)\n\t\n\tdef _is_primitive_type(self, type_name: str) -> bool:\n\t\t\"\"\"Check if type is a C# primitive or common built-in type.\"\"\"\n\t\tprimitives = {\n\t\t\t\"bool\", \"byte\", \"sbyte\", \"char\", \"decimal\", \"double\", \"float\", \"int\", \"uint\", \n\t\t\t\"long\", \"ulong\", \"short\", \"ushort\", \"string\", \"object\", \"void\",\n\t\t\t\"Boolean\", \"Byte\", \"SByte\", \"Char\", \"Decimal\", \"Double\", \"Single\", \"Int32\", \"UInt32\",\n\t\t\t\"Int64\", \"UInt64\", \"Int16\", \"UInt16\", \"String\", \"Object\", \"Void\",\n\t\t\t\"List\", \"Dictionary\", \"IList\", \"IDictionary\", \"IEnumerable\", \"ICollection\",\n\t\t\t\"Task\", \"CancellationToken\", \"DateTime\", \"TimeSpan\", \"Guid\"\n\t\t}\n\t\treturn type_name in primitives\n\t\n\tdef _get_identifier_name(self, node):\n\t\tname_node = next((c for c in node.children if c.type == \"identifier\"), None)\n\t\treturn name_node.text.decode() if name_node else None\n\t\n\tdef _get_identifier_name_cs(self, node):\n\t\tif node.type == \"class_declaration\":\n\t\t\tfound_class_keyword = False\n\t\t\tfor child in node.children:\n\t\t\t\tif child.type == \"class\":\n\t\t\t\t\tfound_class_keyword = True\n\t\t\t\telif found_class_keyword and child.type == \"identifier\":\n\t\t\t\t\treturn child.text.decode()\n\t\telif node.type == \"interface_declaration\":\n\t\t\tfound_interface_keyword = False\n\t\t\tfor child in node.children:\n\t\t\t\tif child.type == \"interface\":\n\t\t\t\t\tfound_interface_keyword = True\n\t\t\t\telif found_interface_keyword and child.type == \"identifier\":\n\t\t\t\t\treturn child.text.decode()\n\t\telif node.type == \"struct_declaration\":\n\t\t\tfound_struct_keyword = False\n\t\t\tfor child in node.children:\n\t\t\t\tif child.type == \"struct\":\n\t\t\t\t\tfound_struct_keyword = True\n\t\t\t\telif found_struct_keyword and child.type == \"identifier\":\n\t\t\t\t\treturn child.text.decode()\n\t\tname_node = next((c for c in node.children if c.type == \"identifier\"), None)\n\t\treturn name_node.text.decode() if name_node else None\n\t\n\tdef _get_type_name(self, node):\n\t\t\"\"\"Get type name from a type node.\"\"\"\n\t\tif node.type == \"identifier\":\n\t\t\treturn node.text.decode()\n\t\telif node.type == \"generic_name\":\n\t\t\ttype_node = next((c for c in node.children if c.type == \"identifier\"), None)\n\t\t\treturn type_node.text.decode() if type_node else None\n\t\telif node.type == \"predefined_type\":\n\t\t\treturn node.text.decode()\n\t\treturn None\n\t\n\tdef _find_containing_class(self, node, top_level_nodes):\n\t\tcurrent = node.parent\n\t\twhile current:\n\t\t\tif current.type in [\"class_declaration\", \"interface_declaration\", \"struct_declaration\", \"enum_declaration\", \"record_declaration\", \"delegate_declaration\"]:\n\t\t\t\tclass_name = self._get_identifier_name_cs(current)\n\t\t\t\tif class_name and class_name in top_level_nodes:\n\t\t\t\t\treturn class_name\n\t\t\tcurrent = current.parent\n\t\treturn None",
    "start_line": 13,
    "end_line": 292,
    "has_docstring": false,
    "docstring": "",
    "parameters": null,
    "node_type": "class",
    "base_classes": null,
    "class_name": null,
    "display_name": "class TreeSitterCSharpAnalyzer",
    "component_id": "codewiki.src.be.dependency_analyzer.analyzers.csharp.TreeSitterCSharpAnalyzer"
  },
  "codewiki.src.be.dependency_analyzer.analyzers.csharp.analyze_csharp_file": {
    "id": "codewiki.src.be.dependency_analyzer.analyzers.csharp.analyze_csharp_file",
    "name": "analyze_csharp_file",
    "component_type": "function",
    "file_path": "/Users/wufagang/project/aiopen/CodeWiki/codewiki/src/be/dependency_analyzer/analyzers/csharp.py",
    "relative_path": "codewiki/src/be/dependency_analyzer/analyzers/csharp.py",
    "depends_on": [
      "codewiki.src.be.dependency_analyzer.analyzers.csharp.TreeSitterCSharpAnalyzer"
    ],
    "source_code": "def analyze_csharp_file(file_path: str, content: str, repo_path: str = None) -> Tuple[List[Node], List[CallRelationship]]:\n\tanalyzer = TreeSitterCSharpAnalyzer(file_path, content, repo_path)\n\treturn analyzer.nodes, analyzer.call_relationships",
    "start_line": 294,
    "end_line": 296,
    "has_docstring": false,
    "docstring": "",
    "parameters": [
      "file_path",
      "content",
      "repo_path"
    ],
    "node_type": "function",
    "base_classes": null,
    "class_name": null,
    "display_name": "function analyze_csharp_file",
    "component_id": "codewiki.src.be.dependency_analyzer.analyzers.csharp.analyze_csharp_file"
  },
  "codewiki.src.be.dependency_analyzer.analyzers.java.TreeSitterJavaAnalyzer": {
    "id": "codewiki.src.be.dependency_analyzer.analyzers.java.TreeSitterJavaAnalyzer",
    "name": "TreeSitterJavaAnalyzer",
    "component_type": "class",
    "file_path": "/Users/wufagang/project/aiopen/CodeWiki/codewiki/src/be/dependency_analyzer/analyzers/java.py",
    "relative_path": "codewiki/src/be/dependency_analyzer/analyzers/java.py",
    "depends_on": [
      "codewiki.src.be.dependency_analyzer.models.core.CallRelationship",
      "codewiki.src.be.dependency_analyzer.models.core.Node"
    ],
    "source_code": "class TreeSitterJavaAnalyzer:\n\tdef __init__(self, file_path: str, content: str, repo_path: str = None):\n\t\tself.file_path = Path(file_path)\n\t\tself.content = content\n\t\tself.repo_path = repo_path or \"\"\n\t\tself.nodes: List[Node] = []\n\t\tself.call_relationships: List[CallRelationship] = []\n\t\tself._analyze()\n\t\n\tdef _get_module_path(self) -> str:\n\t\tif self.repo_path:\n\t\t\ttry:\n\t\t\t\trel_path = os.path.relpath(str(self.file_path), self.repo_path)\n\t\t\texcept ValueError:\n\t\t\t\trel_path = str(self.file_path)\n\t\telse:\n\t\t\trel_path = str(self.file_path)\n\t\t\n\t\tfor ext in ['.java']:\n\t\t\tif rel_path.endswith(ext):\n\t\t\t\trel_path = rel_path[:-len(ext)]\n\t\t\t\tbreak\n\t\treturn rel_path.replace('/', '.').replace('\\\\', '.')\n\t\n\tdef _get_relative_path(self) -> str:\n\t\t\"\"\"Get relative path from repo root.\"\"\"\n\t\tif self.repo_path:\n\t\t\ttry:\n\t\t\t\treturn os.path.relpath(str(self.file_path), self.repo_path)\n\t\t\texcept ValueError:\n\t\t\t\treturn str(self.file_path)\n\t\telse:\n\t\t\treturn str(self.file_path)\n\t\n\tdef _get_component_id(self, name: str, parent_class: str = None) -> str:\n\t\tmodule_path = self._get_module_path()\n\t\tif parent_class:\n\t\t\treturn f\"{module_path}.{parent_class}.{name}\"\n\t\telse:\n\t\t\treturn f\"{module_path}.{name}\"\n\n\tdef _analyze(self):\n\t\tlanguage_capsule = tree_sitter_java.language()\n\t\tjava_language = Language(language_capsule)\n\t\tparser = Parser(java_language)\n\t\ttree = parser.parse(bytes(self.content, \"utf8\"))\n\t\troot = tree.root_node\n\t\tlines = self.content.splitlines()\n\t\t\n\t\ttop_level_nodes = {}\n\t\t\n\t\tself._extract_nodes(root, top_level_nodes, lines)\n\t\t\n\t\tself._extract_relationships(root, top_level_nodes)\n\t\n\tdef _extract_nodes(self, node, top_level_nodes, lines):\n\t\tnode_type = None\n\t\tnode_name = None\n\t\t\n\t\tif node.type == \"class_declaration\":\n\t\t\tis_abstract = any(c.type == \"modifier\" and c.text.decode() == \"abstract\" for c in node.children)\n\t\t\tnode_type = \"abstract class\" if is_abstract else \"class\"\n\t\t\tname_node = next((c for c in node.children if c.type == \"identifier\"), None)\n\t\t\tnode_name = name_node.text.decode() if name_node else None\n\t\telif node.type == \"interface_declaration\":\n\t\t\tnode_type = \"interface\"\n\t\t\tname_node = next((c for c in node.children if c.type == \"identifier\"), None)\n\t\t\tnode_name = name_node.text.decode() if name_node else None\n\t\telif node.type == \"enum_declaration\":\n\t\t\tnode_type = \"enum\"\n\t\t\tname_node = next((c for c in node.children if c.type == \"identifier\"), None)\n\t\t\tnode_name = name_node.text.decode() if name_node else None\n\t\telif node.type == \"record_declaration\":\n\t\t\tnode_type = \"record\"\n\t\t\tname_node = next((c for c in node.children if c.type == \"identifier\"), None)\n\t\t\tnode_name = name_node.text.decode() if name_node else None\n\t\telif node.type == \"annotation_type_declaration\":\n\t\t\tnode_type = \"annotation\"\n\t\t\tname_node = next((c for c in node.children if c.type == \"identifier\"), None)\n\t\t\tnode_name = name_node.text.decode() if name_node else None\n\t\telif node.type == \"method_declaration\":\n\t\t\tnode_type = \"method\"\n\t\t\tname_node = next((c for c in node.children if c.type == \"identifier\"), None)\n\t\t\tif name_node:\n\t\t\t\tmethod_name = name_node.text.decode()\n\t\t\t\tcontaining_class = self._find_containing_class_name(node)\n\t\t\t\tif containing_class:\n\t\t\t\t\tnode_name = f\"{containing_class}.{method_name}\"\n\t\t\t\telse:\n\t\t\t\t\tnode_name = method_name\n\t\t\n\t\tif node_type and node_name:\n\t\t\tcomponent_id = self._get_component_id(node_name)\n\t\t\trelative_path = self._get_relative_path()\n\t\t\tnode_obj = Node(\n\t\t\t\tid=component_id,\n\t\t\t\tname=node_name,\n\t\t\t\tcomponent_type=node_type,\n\t\t\t\tfile_path=str(self.file_path),\n\t\t\t\trelative_path=relative_path,\n\t\t\t\tsource_code=\"\\n\".join(lines[node.start_point[0]:node.end_point[0]+1]),\n\t\t\t\tstart_line=node.start_point[0]+1,\n\t\t\t\tend_line=node.end_point[0]+1,\n\t\t\t\thas_docstring=False,\n\t\t\t\tdocstring=\"\",\n\t\t\t\tparameters=None,\n\t\t\t\tnode_type=node_type,\n\t\t\t\tbase_classes=None,\n\t\t\t\tclass_name=None,\n\t\t\t\tdisplay_name=f\"{node_type} {node_name}\",\n\t\t\t\tcomponent_id=component_id\n\t\t\t)\n\t\t\tself.nodes.append(node_obj)\n\t\t\ttop_level_nodes[node_name] = node_obj\n\t\t\n\t\t# Recursively process children\n\t\tfor child in node.children:\n\t\t\tself._extract_nodes(child, top_level_nodes, lines)\n\t\n\tdef _extract_relationships(self, node, top_level_nodes):\n\t\t# 1. Inheritance: Class extends another class\n\t\tif node.type == \"class_declaration\":\n\t\t\tclass_name = self._get_identifier_name(node)\n\t\t\tchildren_types = [c.type for c in node.children]\n\t\t\t\n\t\t\textends_node = next((c for c in node.children if c.type == \"superclass\"), None)\n\t\t\tif extends_node:\n\t\t\t\tbase_class_name = self._get_type_name(extends_node)\n\t\t\t\tif class_name and base_class_name and not self._is_primitive_type(base_class_name):\n\t\t\t\t\tcaller_id = self._get_component_id(class_name)\n\t\t\t\t\tcallee_id = self._get_component_id(base_class_name)  \n\t\t\t\t\tself.call_relationships.append(CallRelationship(\n\t\t\t\t\t\tcaller=caller_id,\n\t\t\t\t\t\tcallee=callee_id,  \n\t\t\t\t\t\tcall_line=node.start_point[0]+1,\n\t\t\t\t\t\tis_resolved=False  \n\t\t\t\t\t))\n\t\t\telse:\n\t\t\t\tlogger.debug(f\"   No superclass found for {class_name}\")\n\t\t\n\t\t# 2. Interface Implementation: Class/enum/record implements interface\n\t\tif node.type in [\"class_declaration\", \"enum_declaration\", \"record_declaration\"]:\n\t\t\timplementer_name = self._get_identifier_name(node)\n\t\t\timplements_node = next((c for c in node.children if c.type == \"super_interfaces\"), None)\n\t\t\tif implements_node and implementer_name:\n\t\t\t\tfor child in implements_node.children:\n\t\t\t\t\tif child.type == \"type_list\":\n\t\t\t\t\t\tfor type_child in child.children:\n\t\t\t\t\t\t\tif type_child.type in [\"type_identifier\", \"generic_type\"]:\n\t\t\t\t\t\t\t\tinterface_name = self._get_type_name(type_child)\n\t\t\t\t\t\t\t\tif interface_name and not self._is_primitive_type(interface_name):\n\t\t\t\t\t\t\t\t\tcaller_id = self._get_component_id(implementer_name)\n\t\t\t\t\t\t\t\t\tcallee_id = self._get_component_id(interface_name)  \n\t\t\t\t\t\t\t\t\tself.call_relationships.append(CallRelationship(\n\t\t\t\t\t\t\t\t\t\tcaller=caller_id,\n\t\t\t\t\t\t\t\t\t\tcallee=callee_id,  \n\t\t\t\t\t\t\t\t\t\tcall_line=node.start_point[0]+1,\n\t\t\t\t\t\t\t\t\t\tis_resolved=False\n\t\t\t\t\t\t\t\t\t))\n\t\t\n\t\t# 3. Field Type Use: Class has field of another class/interface type\n\t\tif node.type == \"field_declaration\":\n\t\t\tcontaining_class = self._find_containing_class(node, top_level_nodes)\n\t\t\ttype_node = next((c for c in node.children if c.type in [\"type_identifier\", \"generic_type\"]), None)\n\t\t\tif containing_class and type_node:\n\t\t\t\tfield_type_name = self._get_type_name(type_node)\n\t\t\t\tif field_type_name and not self._is_primitive_type(field_type_name):\n\t\t\t\t\tself.call_relationships.append(CallRelationship(\n\t\t\t\t\t\tcaller=containing_class,\n\t\t\t\t\t\tcallee=field_type_name,  \n\t\t\t\t\t\tcall_line=node.start_point[0]+1,\n\t\t\t\t\t\tis_resolved=False\n\t\t\t\t\t))\n\t\t\n\t\t# 4. Method Calls: Method calls on objects\n\t\tif node.type == \"method_invocation\":\n\t\t\tcontaining_class = self._find_containing_class(node, top_level_nodes)\n\t\t\tcontaining_method = self._find_containing_method(node)\n\t\t\tif containing_class:\n\t\t\t\tobject_name = None\n\t\t\t\tmethod_name = None\n\t\t\t\t\n\t\t\t\tif node.children:\n\t\t\t\t\tfirst_child = node.children[0]\n\t\t\t\t\tif first_child.type == \"identifier\":\n\t\t\t\t\t\tobject_name = first_child.text.decode()\n\t\t\t\t\t\tif len(node.children) >= 3:  \n\t\t\t\t\t\t\tmethod_child = node.children[2]\n\t\t\t\t\t\t\tif method_child.type == \"identifier\":\n\t\t\t\t\t\t\t\tmethod_name = method_child.text.decode()\n\t\t\t\t\n\t\t\t\tif object_name and method_name:\n\t\t\t\t\ttarget_type = None\n\t\t\t\t\t\n\t\t\t\t\tcaller_id = containing_method or containing_class\n\t\t\t\t\t\n\t\t\t\t\tif object_name in top_level_nodes:\n\t\t\t\t\t\ttarget_type = object_name\n\t\t\t\t\telse:\n\t\t\t\t\t\ttarget_type = self._find_variable_type(node, object_name, top_level_nodes)\n\n\t\t\t\t\tif target_type and not self._is_primitive_type(target_type):\n\t\t\t\t\t\tself.call_relationships.append(CallRelationship(\n\t\t\t\t\t\t\tcaller=caller_id,\n\t\t\t\t\t\t\tcallee=target_type,\n\t\t\t\t\t\t\tcall_line=node.start_point[0]+1,\n\t\t\t\t\t\t\tis_resolved=False\n\t\t\t\t\t\t))\n\t\t\n\t\t# 5. Object Creation\n\t\tif node.type == \"object_creation_expression\":\n\t\t\tcontaining_class = self._find_containing_class(node, top_level_nodes)\n\t\t\ttype_node = next((c for c in node.children if c.type in [\"type_identifier\", \"generic_type\"]), None)\n\t\t\tif containing_class and type_node:\n\t\t\t\tcreated_type = self._get_type_name(type_node)\n\t\t\t\tif created_type and not self._is_primitive_type(created_type):\n\t\t\t\t\tself.call_relationships.append(CallRelationship(\n\t\t\t\t\t\tcaller=containing_class,\n\t\t\t\t\t\tcallee=created_type,\n\t\t\t\t\t\tcall_line=node.start_point[0]+1,\n\t\t\t\t\t\tis_resolved=False\n\t\t\t\t\t))\n\t\t\n\t\t# Recursively process children\n\t\tfor child in node.children:\n\t\t\tself._extract_relationships(child, top_level_nodes)\n\t\n\tdef _is_primitive_type(self, type_name: str) -> bool:\n\t\t\"\"\"Check if type is a Java primitive or common built-in type.\"\"\"\n\t\tprimitives = {\n\t\t\t\"boolean\", \"byte\", \"char\", \"double\", \"float\", \"int\", \"long\", \"short\",\n\t\t\t\"Boolean\", \"Byte\", \"Character\", \"Double\", \"Float\", \"Integer\", \"Long\", \"Short\",\n\t\t\t\"String\", \"Object\", \"List\", \"Set\", \"Map\", \"Collection\", \"Optional\",\n\t\t\t\"void\", \"Void\"\n\t\t}\n\t\treturn type_name in primitives\n\t\n\tdef _get_identifier_name(self, node):\n\t\t\"\"\"Get identifier name from a node.\"\"\"\n\t\tname_node = next((c for c in node.children if c.type == \"identifier\"), None)\n\t\treturn name_node.text.decode() if name_node else None\n\t\n\tdef _get_type_name(self, node):\n\t\t\"\"\"Get type name from a type node.\"\"\"\n\t\tif node.type == \"type_identifier\":\n\t\t\treturn node.text.decode()\n\t\telif node.type == \"generic_type\":\n\t\t\ttype_node = next((c for c in node.children if c.type == \"type_identifier\"), None)\n\t\t\treturn type_node.text.decode() if type_node else None\n\t\telif node.type == \"superclass\":\n\t\t\ttype_node = next((c for c in node.children if c.type == \"type_identifier\"), None)\n\t\t\treturn type_node.text.decode() if type_node else None\n\t\treturn None\n\t\n\tdef _find_containing_class(self, node, top_level_nodes):\n\t\tcurrent = node.parent\n\t\twhile current:\n\t\t\tif current.type in [\"class_declaration\", \"interface_declaration\", \"enum_declaration\", \"record_declaration\", \"annotation_type_declaration\"]:\n\t\t\t\tclass_name = self._get_identifier_name(current)\n\t\t\t\tif class_name and class_name in top_level_nodes:\n\t\t\t\t\treturn self._get_component_id(class_name)  \n\t\t\tcurrent = current.parent\n\t\treturn None\n\t\n\tdef _find_variable_type(self, node, variable_name, top_level_nodes):\n\t\tmethod_node = node.parent\n\t\twhile method_node and method_node.type != \"method_declaration\":\n\t\t\tmethod_node = method_node.parent\n\t\t\n\t\tif method_node:\n\t\t\tfor child in method_node.children:\n\t\t\t\tif child.type == \"block\":\n\t\t\t\t\tvariable_type = self._search_variable_declaration(child, variable_name)\n\t\t\t\t\tif variable_type:\n\t\t\t\t\t\treturn variable_type\n\t\t\n\t\tclass_node = node.parent\n\t\twhile class_node and class_node.type != \"class_declaration\":\n\t\t\tclass_node = class_node.parent\n\t\t\t\n\t\tif class_node:\n\t\t\tfor child in class_node.children:\n\t\t\t\tif child.type == \"class_body\":\n\t\t\t\t\tfor body_child in child.children:\n\t\t\t\t\t\tif body_child.type == \"field_declaration\":\n\t\t\t\t\t\t\tidentifier_node = None\n\t\t\t\t\t\t\ttype_node = None\n\t\t\t\t\t\t\tfor field_child in body_child.children:\n\t\t\t\t\t\t\t\tif field_child.type in [\"type_identifier\", \"generic_type\"]:\n\t\t\t\t\t\t\t\t\ttype_node = field_child\n\t\t\t\t\t\t\t\telif field_child.type == \"variable_declarator\":\n\t\t\t\t\t\t\t\t\tidentifier_node = next((c for c in field_child.children if c.type == \"identifier\"), None)\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\tif identifier_node and type_node and identifier_node.text.decode() == variable_name:\n\t\t\t\t\t\t\t\tfield_type = self._get_type_name(type_node)\n\t\t\t\t\t\t\t\treturn field_type\n\t\t\n\t\treturn None\n\t\n\tdef _search_variable_declaration(self, block_node, variable_name):\n\t\tfor child in block_node.children:\n\t\t\tif child.type == \"local_variable_declaration\":\n\t\t\t\ttype_node = None\n\t\t\t\tidentifier_node = None\n\t\t\t\tfor decl_child in child.children:\n\t\t\t\t\tif decl_child.type in [\"type_identifier\", \"generic_type\"]:\n\t\t\t\t\t\ttype_node = decl_child\n\t\t\t\t\telif decl_child.type == \"variable_declarator\":\n\t\t\t\t\t\tidentifier_node = next((c for c in decl_child.children if c.type == \"identifier\"), None)\n\t\t\t\t\n\t\t\t\tif identifier_node and type_node and identifier_node.text.decode() == variable_name:\n\t\t\t\t\treturn self._get_type_name(type_node)\n\t\t\t\n\t\t\telif child.type == \"block\":\n\t\t\t\tresult = self._search_variable_declaration(child, variable_name)\n\t\t\t\tif result:\n\t\t\t\t\treturn result\n\t\t\n\t\treturn None\n\t\n\tdef _find_containing_class_name(self, node):\n\t\tcurrent = node.parent\n\t\twhile current:\n\t\t\tif current.type in [\"class_declaration\", \"interface_declaration\", \"enum_declaration\", \"record_declaration\"]:\n\t\t\t\tname_node = next((c for c in current.children if c.type == \"identifier\"), None)\n\t\t\t\tif name_node:\n\t\t\t\t\treturn name_node.text.decode()\n\t\t\tcurrent = current.parent\n\t\treturn None\n\t\n\tdef _find_containing_method(self, node):\n\t\tcurrent = node.parent\n\t\twhile current:\n\t\t\tif current.type == \"method_declaration\":\n\t\t\t\tmethod_name = self._get_identifier_name(current)\n\t\t\t\tclass_name = self._find_containing_class_name(current)\n\t\t\t\tif method_name and class_name:\n\t\t\t\t\treturn self._get_component_id(f\"{class_name}.{method_name}\")\n\t\t\tcurrent = current.parent\n\t\treturn None",
    "start_line": 13,
    "end_line": 352,
    "has_docstring": false,
    "docstring": "",
    "parameters": null,
    "node_type": "class",
    "base_classes": null,
    "class_name": null,
    "display_name": "class TreeSitterJavaAnalyzer",
    "component_id": "codewiki.src.be.dependency_analyzer.analyzers.java.TreeSitterJavaAnalyzer"
  },
  "codewiki.src.be.dependency_analyzer.analyzers.java.analyze_java_file": {
    "id": "codewiki.src.be.dependency_analyzer.analyzers.java.analyze_java_file",
    "name": "analyze_java_file",
    "component_type": "function",
    "file_path": "/Users/wufagang/project/aiopen/CodeWiki/codewiki/src/be/dependency_analyzer/analyzers/java.py",
    "relative_path": "codewiki/src/be/dependency_analyzer/analyzers/java.py",
    "depends_on": [
      "codewiki.src.be.dependency_analyzer.analyzers.java.TreeSitterJavaAnalyzer"
    ],
    "source_code": "def analyze_java_file(file_path: str, content: str, repo_path: str = None) -> Tuple[List[Node], List[CallRelationship]]:\n\tanalyzer = TreeSitterJavaAnalyzer(file_path, content, repo_path)\n\treturn analyzer.nodes, analyzer.call_relationships",
    "start_line": 354,
    "end_line": 356,
    "has_docstring": false,
    "docstring": "",
    "parameters": [
      "file_path",
      "content",
      "repo_path"
    ],
    "node_type": "function",
    "base_classes": null,
    "class_name": null,
    "display_name": "function analyze_java_file",
    "component_id": "codewiki.src.be.dependency_analyzer.analyzers.java.analyze_java_file"
  },
  "codewiki.src.be.dependency_analyzer.analyzers.javascript.TreeSitterJSAnalyzer": {
    "id": "codewiki.src.be.dependency_analyzer.analyzers.javascript.TreeSitterJSAnalyzer",
    "name": "TreeSitterJSAnalyzer",
    "component_type": "class",
    "file_path": "/Users/wufagang/project/aiopen/CodeWiki/codewiki/src/be/dependency_analyzer/analyzers/javascript.py",
    "relative_path": "codewiki/src/be/dependency_analyzer/analyzers/javascript.py",
    "depends_on": [
      "codewiki.src.be.dependency_analyzer.models.core.CallRelationship",
      "codewiki.cli.utils.errors.warning",
      "codewiki.src.be.dependency_analyzer.models.core.Node"
    ],
    "source_code": "class TreeSitterJSAnalyzer:\n    def __init__(self, file_path: str, content: str, repo_path: str = None):\n        self.file_path = Path(file_path)\n        self.content = content\n        self.repo_path = repo_path or \"\"\n        self.nodes: List[Node] = []\n        self.call_relationships: List[CallRelationship] = []\n        \n        self.top_level_nodes = {}\n        \n        self.seen_relationships = set()\n\n        try:\n            language_capsule = tree_sitter_javascript.language()\n            self.js_language = Language(language_capsule)\n            self.parser = Parser(self.js_language)\n\n        except Exception as e:\n            logger.error(f\"Failed to initialize JavaScript parser: {e}\")\n            logger.error(f\"Traceback: {traceback.format_exc()}\")\n            self.parser = None\n            self.js_language = None\n\n\n    def _add_relationship(self, relationship: CallRelationship) -> bool:\n        rel_key = (relationship.caller, relationship.callee, relationship.call_line)\n        \n        if rel_key not in self.seen_relationships:\n            self.seen_relationships.add(rel_key)\n            self.call_relationships.append(relationship)\n            return True\n        return False\n\n    def analyze(self) -> None:\n        if self.parser is None:\n            logger.warning(f\"Skipping {self.file_path} - parser initialization failed\")\n            return\n\n        try:\n            tree = self.parser.parse(bytes(self.content, \"utf8\"))\n            root_node = tree.root_node\n\n            logger.debug(f\"Parsed AST with root node type: {root_node.type}\")\n\n            self._extract_functions(root_node)\n            self._extract_call_relationships(root_node)\n\n            logger.debug(\n                f\"Analysis complete: {len(self.nodes)} nodes, {len(self.call_relationships)} relationships\"\n            )\n\n        except Exception as e:\n            logger.error(f\"Error analyzing JavaScript file {self.file_path}: {e}\", exc_info=True)\n\n    def _get_module_path(self) -> str:\n        if self.repo_path:\n            try:\n                rel_path = os.path.relpath(str(self.file_path), self.repo_path)\n            except ValueError:\n                rel_path = str(self.file_path)\n        else:\n            rel_path = str(self.file_path)\n        \n        for ext in ['.js', '.ts', '.jsx', '.tsx', '.mjs', '.cjs']:\n            if rel_path.endswith(ext):\n                rel_path = rel_path[:-len(ext)]\n                break\n        return rel_path.replace('/', '.').replace('\\\\', '.')\n    \n    def _get_relative_path(self) -> str:\n        if self.repo_path:\n            try:\n                return os.path.relpath(str(self.file_path), self.repo_path)\n            except ValueError:\n                return str(self.file_path)\n        else:\n            return str(self.file_path)\n\n    def _get_component_id(self, name: str, class_name: str = None, is_method: bool = False) -> str:\n        module_path = self._get_module_path()\n        \n        if is_method and class_name:\n            return f\"{module_path}.{class_name}.{name}\"\n        elif class_name and not is_method: \n            return f\"{module_path}.{name}\"\n        else:  \n            return f\"{module_path}.{name}\"\n\n    def _find_containing_class(self, node) -> Optional[str]:\n        parent = node.parent\n        while parent:\n            if parent.type in [\"class_declaration\", \"abstract_class_declaration\", \"interface_declaration\"]:\n                name_node = self._find_child_by_type(parent, \"type_identifier\")\n                if not name_node:\n                    name_node = self._find_child_by_type(parent, \"identifier\")\n                if name_node:\n                    return self._get_node_text(name_node)\n            parent = parent.parent\n        return None\n\n    def _extract_functions(self, node) -> None:\n        self._traverse_for_functions(node)\n        self.nodes.sort(key=lambda n: n.start_line)\n\n    def _traverse_for_functions(self, node) -> None:\n        if node.type in [\"class_declaration\", \"abstract_class_declaration\", \"interface_declaration\"]:\n            cls = self._extract_class_declaration(node)\n            if cls:\n                self.nodes.append(cls)\n                self.top_level_nodes[cls.name] = cls\n                \n                self._extract_methods_from_class(node, cls.name)\n                \n        elif node.type == \"function_declaration\":\n            containing_class = self._find_containing_class(node)\n            if containing_class is None:\n                func = self._extract_function_declaration(node)\n                if func and self._should_include_function(func):\n                    self.nodes.append(func)\n                    self.top_level_nodes[func.name] = func\n        elif node.type == \"generator_function_declaration\":\n            containing_class = self._find_containing_class(node)\n            if containing_class is None:\n                func = self._extract_function_declaration(node)\n                if func and self._should_include_function(func):\n                    self.nodes.append(func)\n                    self.top_level_nodes[func.name] = func\n        elif node.type == \"export_statement\":\n            func = self._extract_exported_function(node)\n            if func and self._should_include_function(func):\n                self.nodes.append(func)\n                self.top_level_nodes[func.name] = func\n        elif node.type == \"lexical_declaration\":\n            containing_class = self._find_containing_class(node)\n            if containing_class is None:\n                func = self._extract_arrow_function_from_declaration(node)\n                if func and self._should_include_function(func):\n                    self.nodes.append(func)\n                    self.top_level_nodes[func.name] = func\n        \n        for child in node.children:\n            self._traverse_for_functions(child)\n\n    def _extract_methods_from_class(self, class_node, class_name: str) -> None:\n        class_body = self._find_child_by_type(class_node, \"class_body\")\n        if not class_body:\n            return\n            \n        for child in class_body.children:\n            if child.type == \"method_definition\":\n                method_name = self._get_method_name(child)\n                if method_name:\n                    method_key = f\"{self._get_module_path()}.{class_name}.{method_name}\"\n                    method_node = self._create_method_node(child, method_name, class_name)\n                    if method_node:\n                        self.top_level_nodes[method_key] = method_node\n            elif child.type == \"field_definition\":\n                # Handle arrow function properties\n                field_name = self._get_field_name(child)\n                if field_name and self._is_arrow_function_field(child):\n                    method_key = f\"{self._get_module_path()}.{class_name}.{field_name}\"\n                    method_node = self._create_method_node(child, field_name, class_name)\n                    if method_node:\n                        self.top_level_nodes[method_key] = method_node\n\n    def _get_method_name(self, method_node) -> Optional[str]:\n        \"\"\"Get method name from method_definition node.\"\"\"\n        if method_node.type != \"method_definition\":\n            return None\n        \n        for child in method_node.children:\n            if child.type == \"property_identifier\":\n                return self._get_node_text(child)\n        return None\n\n    def _get_field_name(self, field_node) -> Optional[str]:\n        \"\"\"Get field name from field_definition node.\"\"\"\n        if field_node.type != \"field_definition\":\n            return None\n            \n        for child in field_node.children:\n            if child.type == \"property_identifier\":\n                return self._get_node_text(child)\n        return None\n\n    def _is_arrow_function_field(self, field_node) -> bool:\n        \"\"\"Check if field_definition contains an arrow function.\"\"\"\n        for child in field_node.children:\n            if child.type == \"arrow_function\":\n                return True\n        return False\n\n    def _create_method_node(self, node, method_name: str, class_name: str) -> Optional[Node]:\n        \"\"\"Create a method node for relationship mapping.\"\"\"\n        try:\n            line_start = node.start_point[0] + 1\n            line_end = node.end_point[0] + 1\n            component_id = self._get_component_id(method_name, class_name, is_method=True)\n            relative_path = self._get_relative_path()\n            \n            return Node(\n                id=component_id,\n                name=method_name,\n                component_type=\"method\",\n                file_path=str(self.file_path),\n                relative_path=relative_path,\n                source_code=\"\\n\".join(self.content.splitlines()[line_start - 1 : line_end]),\n                start_line=line_start,\n                end_line=line_end,\n                has_docstring=False,\n                docstring=\"\",\n                parameters=None,\n                node_type=\"method\",\n                base_classes=None,\n                class_name=class_name,\n                display_name=f\"method {method_name}\",\n                component_id=component_id\n            )\n        except Exception as e:\n            logger.debug(f\"Error creating method node for {method_name}: {e}\")\n            return None\n\n    def _extract_class_declaration(self, node) -> Optional[Node]:\n        \"\"\"Extract class/abstract class/interface declaration.\"\"\"\n        try:\n            name_node = self._find_child_by_type(node, \"type_identifier\")\n            if not name_node:\n                name_node = self._find_child_by_type(node, \"identifier\")\n            if not name_node:\n                return None\n            name = self._get_node_text(name_node)\n            line_start = node.start_point[0] + 1\n            line_end = node.end_point[0] + 1\n            docstring = None\n            base_classes = []\n            heritage_node = self._find_child_by_type(node, \"class_heritage\")\n            if heritage_node:\n                for child in heritage_node.children:\n                    if child.type in [\"identifier\", \"type_identifier\"]:\n                        base_classes.append(self._get_node_text(child))\n            code_snippet = \"\\n\".join(self.content.splitlines()[line_start - 1 : line_end])\n            \n            if node.type == \"abstract_class_declaration\":\n                node_type = \"abstract class\"\n                display_name = f\"abstract class {name}\"\n            elif node.type == \"interface_declaration\":\n                node_type = \"interface\"\n                display_name = f\"interface {name}\"\n            else:\n                node_type = \"class\"\n                display_name = f\"class {name}\"\n            \n            component_id = self._get_component_id(name, is_method=False)\n            relative_path = self._get_relative_path()\n            \n            return Node(\n                id=component_id,\n                name=name,\n                component_type=node_type,\n                file_path=str(self.file_path),\n                relative_path=relative_path,\n                source_code=code_snippet,\n                start_line=line_start,\n                end_line=line_end,\n                has_docstring=bool(docstring),\n                docstring=docstring or \"\",\n                parameters=None,\n                node_type=node_type,\n                base_classes=base_classes if base_classes else None,\n                class_name=None,\n                display_name=display_name,\n                component_id=component_id,\n            )\n        except Exception:\n            return None\n\n    def _extract_function_declaration(self, node) -> Optional[Node]:\n        try:\n            name_node = self._find_child_by_type(node, \"identifier\")\n            if not name_node:\n                return None\n\n            func_name = self._get_node_text(name_node)\n            line_start = node.start_point[0] + 1\n            line_end = node.end_point[0] + 1\n            parameters = self._extract_parameters(node)\n            code_snippet = self._get_node_text(node)\n\n            # Check for async and generator from code snippet\n            is_async = \"async function\" in code_snippet\n            is_generator = \"function*\" in code_snippet or \"*\" in func_name\n            \n            if is_async and is_generator:\n                display_name = f\"async generator {func_name}\"\n            elif is_async:\n                display_name = f\"async function {func_name}\"\n            elif is_generator:\n                display_name = f\"generator function {func_name}\"\n            else:\n                display_name = f\"function {func_name}\"\n\n            component_id = self._get_component_id(func_name, is_method=False)\n            relative_path = self._get_relative_path()\n\n            return Node(\n                id=component_id,\n                name=func_name,\n                component_type=\"function\",\n                file_path=str(self.file_path),\n                relative_path=relative_path,\n                source_code=code_snippet,\n                start_line=line_start,\n                end_line=line_end,\n                has_docstring=False,\n                docstring=\"\",\n                parameters=parameters,\n                node_type=\"function\",\n                base_classes=None,\n                class_name=None,\n                display_name=display_name,\n                component_id=component_id,\n            )\n        except Exception as e:\n            logger.debug(f\"Error extracting function declaration: {e}\")\n            return None\n    def _extract_exported_function(self, node) -> Optional[Node]:\n        \"\"\"Extract export function or export default function\"\"\"\n        try:\n            func_decl = self._find_child_by_type(node, \"function_declaration\")\n            if func_decl:\n                func = self._extract_function_declaration(func_decl)\n                if func:\n                    export_text = self._get_node_text(node)\n                    if \"export default\" in export_text and \"function (\" in export_text:\n                        func.name = \"default\"\n                return func\n        except Exception as e:\n            logger.debug(f\"Error extracting exported function: {e}\")\n        return None\n\n    def _extract_arrow_function_from_declaration(self, node) -> Optional[Node]:\n        \"\"\"Extract arrow function or function expression from const/let/var declarations.\"\"\"\n        try:\n            for child in node.children:\n                if child.type == \"variable_declarator\":\n                    name_node = self._find_child_by_type(child, \"identifier\")\n                    func_node = self._find_child_by_type(\n                        child, \"arrow_function\"\n                    ) or self._find_child_by_type(child, \"function_expression\")\n\n                    if name_node and func_node:\n                        func_name = self._get_node_text(name_node)\n                        line_start = func_node.start_point[0] + 1\n                        line_end = func_node.end_point[0] + 1\n                        parameters = self._extract_parameters(func_node)\n                        code_snippet = self._get_node_text(child)\n\n                        component_id = self._get_component_id(func_name, is_method=False)\n                        relative_path = self._get_relative_path()\n\n                        return Node(\n                            id=component_id,\n                            name=func_name,\n                            component_type=\"function\",\n                            file_path=str(self.file_path),\n                            relative_path=relative_path,\n                            source_code=code_snippet,\n                            start_line=line_start,\n                            end_line=line_end,\n                            has_docstring=False,\n                            docstring=\"\",\n                            parameters=parameters,\n                            node_type=\"function\",\n                            base_classes=None,\n                            class_name=None,\n                            display_name=f\"function {func_name}\",\n                            component_id=component_id,\n                        )\n            return None\n        except Exception as e:\n            logger.debug(f\"Error extracting function from declaration: {e}\")\n        return None\n\n    def _should_include_function(self, func: Node) -> bool:\n        excluded_names = {}\n\n        if func.name.lower() in excluded_names:\n            logger.debug(f\"Skipping excluded function: {func.name}\")\n            return False\n\n        return True\n\n    def _extract_parameters(self, node) -> List[str]:\n        parameters = []\n        params_node = self._find_child_by_type(node, \"formal_parameters\")\n        if params_node:\n            for child in params_node.children:\n                if child.type == \"identifier\":\n                    parameters.append(self._get_node_text(child))\n        return parameters\n\n    def _extract_call_relationships(self, node) -> None:\n        current_top_level = None\n        self._traverse_for_calls(node, current_top_level)\n\n    def _traverse_for_calls(self, node, current_top_level) -> None:\n        if current_top_level:\n            self._extract_jsdoc_type_dependencies(node, current_top_level)\n        \n        if node.type in [\"class_declaration\", \"abstract_class_declaration\", \"interface_declaration\"]:\n            name_node = self._find_child_by_type(node, \"type_identifier\") or self._find_child_by_type(node, \"identifier\")\n            if name_node:\n                current_top_level = self._get_node_text(name_node)\n                \n                heritage_node = self._find_child_by_type(node, \"class_heritage\")\n                if heritage_node:\n                    for child in heritage_node.children:\n                        if child.type in [\"identifier\", \"type_identifier\"]:\n                            base_class = self._get_node_text(child)\n                            caller_id = self._get_component_id(current_top_level)\n                            callee_id = f\"{self._get_module_path()}.{base_class}\" \n                            inheritance_rel = CallRelationship(\n                                caller=caller_id,\n                                callee=callee_id,\n                                call_line=node.start_point[0] + 1,\n                                is_resolved=False\n                            )\n                            self._add_relationship(inheritance_rel)\n                            \n        elif node.type == \"function_declaration\":\n            name_node = self._find_child_by_type(node, \"identifier\")\n            if name_node:\n                current_top_level = self._get_node_text(name_node)\n        elif node.type == \"generator_function_declaration\":\n            name_node = self._find_child_by_type(node, \"identifier\")\n            if name_node:\n                current_top_level = self._get_node_text(name_node)\n        elif node.type == \"lexical_declaration\":\n            for child in node.children:\n                if child.type == \"variable_declarator\":\n                    name_node = self._find_child_by_type(child, \"identifier\")\n                    func_node = self._find_child_by_type(child, \"arrow_function\") or self._find_child_by_type(child, \"function_expression\")\n                    if name_node and func_node:\n                        current_top_level = self._get_node_text(name_node)\n\n        if node.type == \"call_expression\" and current_top_level:\n            call_info = self._extract_call_from_node(node, current_top_level)\n            if call_info:\n                self._add_relationship(call_info)\n        \n        elif node.type == \"await_expression\" and current_top_level:\n            call_expr = self._find_child_by_type(node, \"call_expression\")\n            if call_expr:\n                call_info = self._extract_call_from_node(call_expr, current_top_level)\n                if call_info:\n                    self._add_relationship(call_info)\n        \n        elif node.type == \"new_expression\" and current_top_level:\n            callee_name = self._extract_callee_name(node)\n            if callee_name:\n                call_info = CallRelationship(\n                    caller=f\"{self._get_module_path()}.{current_top_level}\",\n                    callee=f\"{self._get_module_path()}.{callee_name}\",\n                    call_line=node.start_point[0] + 1,\n                    is_resolved=False\n                )\n                self._add_relationship(call_info)\n\n        for child in node.children:\n            self._traverse_for_calls(child, current_top_level)\n\n    def _extract_call_from_node(self, node, caller_name: str) -> Optional[CallRelationship]:\n        \"\"\"Extract call relationship from a call_expression node.\"\"\"\n        try:\n            call_line = node.start_point[0] + 1\n            callee_name = self._extract_callee_name(node)\n            \n            if not callee_name:\n                return None\n            \n            call_text = self._get_node_text(node)\n            is_method_call = \"this.\" in call_text or \"super.\" in call_text\n            \n            caller_id = f\"{self._get_module_path()}.{caller_name}\"\n            \n            if is_method_call:\n                current_class = None\n                for node_key, node_obj in self.top_level_nodes.items():\n                    if node_obj.component_type == \"class\" and caller_name in node_key:\n                        current_class = node_obj.name\n                        break\n                \n                if current_class:\n                    method_key = f\"{self._get_module_path()}.{current_class}.{callee_name}\"\n                    if method_key in self.top_level_nodes:\n                        return None\n            \n            callee_id = f\"{self._get_module_path()}.{callee_name}\"\n            if callee_name in self.top_level_nodes:\n                return CallRelationship(\n                    caller=caller_id,\n                    callee=callee_id,\n                    call_line=call_line,\n                    is_resolved=True,\n                )\n            \n            return CallRelationship(\n                caller=caller_id,\n                callee=callee_id,\n                call_line=call_line,\n                is_resolved=False,\n            )\n            \n        except Exception as e:\n            logger.debug(f\"Error extracting call relationship: {e}\")\n            return None\n\n    def _extract_jsdoc_type_dependencies(self, node, caller_name: str) -> None:\n        \"\"\"Extract type dependencies from JSDoc comments.\"\"\"\n        try:\n            if hasattr(node, 'prev_sibling') and node.prev_sibling:\n                prev = node.prev_sibling\n                if prev.type == \"comment\":\n                    comment_text = self._get_node_text(prev)\n                    self._parse_jsdoc_types(comment_text, caller_name, node.start_point[0] + 1)\n            \n            for child in node.children:\n                if child.type == \"comment\":\n                    comment_text = self._get_node_text(child)\n                    self._parse_jsdoc_types(comment_text, caller_name, node.start_point[0] + 1)\n                    \n        except Exception as e:\n            logger.debug(f\"Error extracting JSDoc dependencies: {e}\")\n\n    def _parse_jsdoc_types(self, comment_text: str, caller_name: str, line_number: int) -> None:\n        \"\"\"Parse JSDoc comment text and extract type references.\"\"\"\n        import re\n        try:\n            type_patterns = [\n                r'@param\\s*\\{([^}]+)\\}',     # @param {Type}\n                r'@returns?\\s*\\{([^}]+)\\}',  # @return {Type} or @returns {Type}\n                r'@type\\s*\\{([^}]+)\\}',      # @type {Type}\n                r'@typedef\\s*\\{[^}]*\\}\\s*(\\w+)', # @typedef {Object} TypeName\n                r'@interface\\s+(\\w+)',       # @interface InterfaceName\n            ]\n            \n            for pattern in type_patterns:\n                matches = re.findall(pattern, comment_text)\n                for match in matches:\n                    type_name = match.strip()\n                    \n                    base_types = self._extract_base_types_from_jsdoc(type_name)\n                    \n                    for base_type in base_types:\n                        if base_type and not self._is_builtin_type_js(base_type):\n                            caller_id = f\"{self._get_module_path()}.{caller_name}\"\n                            callee_id = f\"{self._get_module_path()}.{base_type}\"\n                            \n                            type_rel = CallRelationship(\n                                caller=caller_id,\n                                callee=callee_id,\n                                call_line=line_number,\n                                is_resolved=False \n                            )\n                            \n                            if self._add_relationship(type_rel):\n                                pass\n                                    \n        except Exception as e:\n            logger.debug(f\"Error parsing JSDoc types: {e}\")\n\n    def _extract_base_types_from_jsdoc(self, type_str: str) -> list:\n        import re\n        type_str = type_str.strip()\n        \n        base_types = []\n        \n        main_type_match = re.match(r'^(\\w+)', type_str)\n        if main_type_match:\n            base_types.append(main_type_match.group(1))\n        \n        generic_matches = re.findall(r'<([^<>]+)>', type_str)\n        for generic in generic_matches:\n            subtypes = re.findall(r'\\b(\\w+)\\b', generic)\n            base_types.extend(subtypes)\n        \n        if '|' in type_str:\n            union_types = type_str.split('|')\n            for union_type in union_types:\n                clean_type = re.match(r'\\b(\\w+)\\b', union_type.strip())\n                if clean_type:\n                    base_types.append(clean_type.group(1))\n        \n        return base_types\n\n    def _is_builtin_type_js(self, name: str) -> bool:\n        \"\"\"Check if type name is a JavaScript/JSDoc built-in type.\"\"\"\n        builtin_types = {\n            # JavaScript primitive types\n            \"string\", \"number\", \"boolean\", \"object\", \"undefined\", \"null\", \"void\", \"any\",\n            \n            # Global JavaScript types\n            \"Array\", \"Promise\", \"Date\", \"RegExp\", \"Error\", \"Map\", \"Set\", \"WeakMap\", \"WeakSet\",\n            \"Function\", \"Object\", \"String\", \"Number\", \"Boolean\", \"Symbol\", \"BigInt\",\n            \n            \"Element\", \"HTMLElement\", \"Document\", \"Window\", \"Event\", \"EventTarget\", \"Node\",\n            \"Response\", \"Request\", \"Headers\", \"URL\", \"URLSearchParams\", \"FormData\", \"Blob\", \"File\",\n            \n            # Common JSDoc generic parameters\n            \"T\", \"U\", \"V\", \"K\", \"P\", \"R\", \"E\"\n        }\n        return name in builtin_types\n\n    def _extract_callee_name(self, call_node) -> Optional[str]:\n        if not call_node.children:\n            return None\n            \n        callee_node = call_node.children[0]\n\n        if callee_node.type == \"identifier\":\n            return self._get_node_text(callee_node)\n        elif callee_node.type == \"member_expression\":\n            property_node = self._find_child_by_type(callee_node, \"property_identifier\")\n            if property_node:\n                return self._get_node_text(property_node)\n            \n            computed_property = self._find_child_by_type(callee_node, \"computed_property_name\")\n            if computed_property:\n                for child in computed_property.children:\n                    if child.type == \"identifier\":\n                        return self._get_node_text(child)\n        elif callee_node.type == \"super\":\n            return \"super\"\n        elif callee_node.type == \"this\":\n            return \"this\"\n            \n        return None\n\n    def _find_child_by_type(self, node, node_type: str):\n        \"\"\"Find first child node of specified type.\"\"\"\n        for child in node.children:\n            if child.type == node_type:\n                return child\n        return None\n\n    def _get_node_text(self, node) -> str:\n        start_byte = node.start_byte\n        end_byte = node.end_byte\n        return self.content.encode(\"utf8\")[start_byte:end_byte].decode(\"utf8\")\n\n    def _find_containing_class_name(self, method_node) -> Optional[str]:\n        current = method_node.parent\n        while current:\n            if current.type == \"class_declaration\":\n                name_node = self._find_child_by_type(current, \"identifier\")\n                if name_node:\n                    return self._get_node_text(name_node)\n            current = current.parent\n        return None\n\n    def _extract_assignment_name(self, node) -> Optional[str]:\n        if node.type == \"identifier\":\n            return self._get_node_text(node)\n        elif node.type == \"member_expression\":\n            property_node = self._find_child_by_type(node, \"property_identifier\")\n            if property_node:\n                return self._get_node_text(property_node)\n        return None",
    "start_line": 18,
    "end_line": 685,
    "has_docstring": false,
    "docstring": "",
    "parameters": null,
    "node_type": "class",
    "base_classes": null,
    "class_name": null,
    "display_name": "class TreeSitterJSAnalyzer",
    "component_id": "codewiki.src.be.dependency_analyzer.analyzers.javascript.TreeSitterJSAnalyzer"
  },
  "codewiki.src.be.dependency_analyzer.analyzers.javascript.analyze_javascript_file_treesitter": {
    "id": "codewiki.src.be.dependency_analyzer.analyzers.javascript.analyze_javascript_file_treesitter",
    "name": "analyze_javascript_file_treesitter",
    "component_type": "function",
    "file_path": "/Users/wufagang/project/aiopen/CodeWiki/codewiki/src/be/dependency_analyzer/analyzers/javascript.py",
    "relative_path": "codewiki/src/be/dependency_analyzer/analyzers/javascript.py",
    "depends_on": [
      "codewiki.src.be.dependency_analyzer.analyzers.javascript.TreeSitterJSAnalyzer"
    ],
    "source_code": "def analyze_javascript_file_treesitter(\n    file_path: str, content: str, repo_path: str = None\n) -> Tuple[List[Node], List[CallRelationship]]:\n    \"\"\"Analyze a JavaScript file using tree-sitter.\"\"\"\n    try:\n        logger.debug(f\"Tree-sitter JS analysis for {file_path}\")\n        analyzer = TreeSitterJSAnalyzer(file_path, content, repo_path)\n        analyzer.analyze()\n        logger.debug(\n            f\"Found {len(analyzer.nodes)} top-level nodes, {len(analyzer.call_relationships)} calls\"\n        )\n        return analyzer.nodes, analyzer.call_relationships\n    except Exception as e:\n        logger.error(f\"Error in tree-sitter JS analysis for {file_path}: {e}\", exc_info=True)\n        return [], []",
    "start_line": 687,
    "end_line": 701,
    "has_docstring": true,
    "docstring": "Analyze a JavaScript file using tree-sitter.",
    "parameters": [
      "file_path",
      "content",
      "repo_path"
    ],
    "node_type": "function",
    "base_classes": null,
    "class_name": null,
    "display_name": "function analyze_javascript_file_treesitter",
    "component_id": "codewiki.src.be.dependency_analyzer.analyzers.javascript.analyze_javascript_file_treesitter"
  },
  "codewiki.src.be.dependency_analyzer.analyzers.php.NamespaceResolver": {
    "id": "codewiki.src.be.dependency_analyzer.analyzers.php.NamespaceResolver",
    "name": "NamespaceResolver",
    "component_type": "class",
    "file_path": "/Users/wufagang/project/aiopen/CodeWiki/codewiki/src/be/dependency_analyzer/analyzers/php.py",
    "relative_path": "codewiki/src/be/dependency_analyzer/analyzers/php.py",
    "depends_on": [],
    "source_code": "class NamespaceResolver:\n    \"\"\"Resolves PHP class names to fully qualified names using use statements.\"\"\"\n\n    def __init__(self):\n        self.current_namespace: str = \"\"\n        self.use_map: Dict[str, str] = {}  # alias -> fully_qualified_name\n\n    def register_namespace(self, ns: str):\n        \"\"\"Set the current namespace.\"\"\"\n        self.current_namespace = ns.replace(\"\\\\\\\\\", \"\\\\\")\n\n    def register_use(self, fqn: str, alias: str = None):\n        \"\"\"Register a use statement with optional alias.\"\"\"\n        fqn = fqn.replace(\"\\\\\\\\\", \"\\\\\").lstrip(\"\\\\\")\n        alias = alias or fqn.split(\"\\\\\")[-1]\n        self.use_map[alias] = fqn\n\n    def resolve(self, name: str) -> str:\n        \"\"\"Resolve a name to its fully qualified form.\"\"\"\n        if not name:\n            return name\n\n        name = name.replace(\"\\\\\\\\\", \"\\\\\")\n\n        # Already fully qualified\n        if name.startswith(\"\\\\\"):\n            return name[1:]\n\n        # Check use map for alias\n        if name in self.use_map:\n            return self.use_map[name]\n\n        # Check if first part is an alias (for partial qualified names)\n        parts = name.split(\"\\\\\")\n        if parts[0] in self.use_map:\n            base = self.use_map[parts[0]]\n            if len(parts) > 1:\n                return f\"{base}\\\\{'\\\\'.join(parts[1:])}\"\n            return base\n\n        # Prepend current namespace\n        if self.current_namespace:\n            return f\"{self.current_namespace}\\\\{name}\"\n\n        return name",
    "start_line": 40,
    "end_line": 84,
    "has_docstring": true,
    "docstring": "Resolves PHP class names to fully qualified names using use statements.",
    "parameters": null,
    "node_type": "class",
    "base_classes": null,
    "class_name": null,
    "display_name": "class NamespaceResolver",
    "component_id": "codewiki.src.be.dependency_analyzer.analyzers.php.NamespaceResolver"
  },
  "codewiki.src.be.dependency_analyzer.analyzers.php.TreeSitterPHPAnalyzer": {
    "id": "codewiki.src.be.dependency_analyzer.analyzers.php.TreeSitterPHPAnalyzer",
    "name": "TreeSitterPHPAnalyzer",
    "component_type": "class",
    "file_path": "/Users/wufagang/project/aiopen/CodeWiki/codewiki/src/be/dependency_analyzer/analyzers/php.py",
    "relative_path": "codewiki/src/be/dependency_analyzer/analyzers/php.py",
    "depends_on": [
      "codewiki.src.be.dependency_analyzer.analyzers.php.NamespaceResolver",
      "codewiki.cli.utils.errors.warning",
      "codewiki.src.be.dependency_analyzer.models.core.Node",
      "codewiki.src.be.dependency_analyzer.models.core.CallRelationship"
    ],
    "source_code": "class TreeSitterPHPAnalyzer:\n    \"\"\"Analyzes PHP files using tree-sitter to extract nodes and relationships.\"\"\"\n\n    def __init__(self, file_path: str, content: str, repo_path: str = None):\n        self.file_path = Path(file_path)\n        self.content = content\n        self.repo_path = repo_path or \"\"\n        self.nodes: List[Node] = []\n        self.call_relationships: List[CallRelationship] = []\n        self.namespace_resolver = NamespaceResolver()\n        self._top_level_nodes: Dict[str, Node] = {}\n\n        # Check if this is a template file that should be skipped\n        if self._is_template_file():\n            logger.debug(f\"Skipping template file: {file_path}\")\n            return\n\n        self._analyze()\n\n    def _is_template_file(self) -> bool:\n        \"\"\"Check if file is a PHP template that should be skipped.\"\"\"\n        file_str = str(self.file_path)\n\n        # Check extension patterns\n        for pattern in TEMPLATE_PATTERNS:\n            if file_str.endswith(pattern):\n                return True\n\n        # Check directory patterns\n        for dir_pattern in TEMPLATE_DIRECTORIES:\n            if f\"/{dir_pattern}/\" in file_str or f\"\\\\{dir_pattern}\\\\\" in file_str:\n                return True\n\n        return False\n\n    def _get_module_path(self) -> str:\n        \"\"\"Get module path for the file.\"\"\"\n        if self.repo_path:\n            try:\n                rel_path = os.path.relpath(str(self.file_path), self.repo_path)\n            except ValueError:\n                rel_path = str(self.file_path)\n        else:\n            rel_path = str(self.file_path)\n\n        # Remove .php extension\n        for ext in ['.php', '.phtml', '.inc']:\n            if rel_path.endswith(ext):\n                rel_path = rel_path[:-len(ext)]\n                break\n\n        return rel_path.replace('/', '.').replace('\\\\', '.')\n\n    def _get_relative_path(self) -> str:\n        \"\"\"Get relative path from repo root.\"\"\"\n        if self.repo_path:\n            try:\n                return os.path.relpath(str(self.file_path), self.repo_path)\n            except ValueError:\n                return str(self.file_path)\n        return str(self.file_path)\n\n    def _get_component_id(self, name: str, parent_class: str = None) -> str:\n        \"\"\"Generate component ID for a node.\"\"\"\n        # Use namespace if available\n        if self.namespace_resolver.current_namespace:\n            ns_prefix = self.namespace_resolver.current_namespace.replace(\"\\\\\", \".\")\n            if parent_class:\n                return f\"{ns_prefix}.{parent_class}.{name}\"\n            return f\"{ns_prefix}.{name}\"\n\n        module_path = self._get_module_path()\n        if parent_class:\n            return f\"{module_path}.{parent_class}.{name}\"\n        return f\"{module_path}.{name}\"\n\n    def _analyze(self):\n        \"\"\"Parse and analyze the PHP file.\"\"\"\n        try:\n            # Use language_php for mixed PHP/HTML files (most common)\n            php_lang_capsule = tree_sitter_php.language_php()\n            php_language = Language(php_lang_capsule)\n            parser = Parser(php_language)\n\n            tree = parser.parse(bytes(self.content, \"utf8\"))\n            root = tree.root_node\n            lines = self.content.splitlines()\n\n            # First pass: extract namespace and use statements\n            self._extract_namespace_info(root)\n\n            # Second pass: extract nodes\n            self._extract_nodes(root, lines, depth=0)\n\n            # Third pass: extract relationships\n            self._extract_relationships(root, depth=0)\n\n        except RecursionError:\n            logger.warning(f\"Max recursion depth exceeded in {self.file_path}\")\n        except Exception as e:\n            logger.error(f\"Error parsing PHP file {self.file_path}: {e}\")\n\n    def _extract_namespace_info(self, node, depth: int = 0):\n        \"\"\"Extract namespace and use statements from the AST.\"\"\"\n        if depth > MAX_RECURSION_DEPTH:\n            return\n\n        if node.type == \"namespace_definition\":\n            # Get namespace name\n            name_node = self._find_child_by_type(node, \"namespace_name\")\n            if name_node:\n                self.namespace_resolver.register_namespace(name_node.text.decode())\n\n        elif node.type == \"namespace_use_declaration\":\n            self._extract_use_statement(node)\n\n        for child in node.children:\n            self._extract_namespace_info(child, depth + 1)\n\n    def _extract_use_statement(self, node):\n        \"\"\"Extract use statement(s) from a namespace_use_declaration node.\"\"\"\n        # Handle group use: use App\\{User, Post};\n        group_node = self._find_child_by_type(node, \"namespace_use_group\")\n        if group_node:\n            prefix_node = self._find_child_by_type(node, \"namespace_name\")\n            prefix = prefix_node.text.decode() if prefix_node else \"\"\n\n            for child in group_node.children:\n                if child.type == \"namespace_use_group_clause\":\n                    name_node = self._find_child_by_type(child, \"namespace_name\")\n                    alias_node = self._find_child_by_type(child, \"namespace_aliasing_clause\")\n\n                    if name_node:\n                        fqn = f\"{prefix}\\\\{name_node.text.decode()}\" if prefix else name_node.text.decode()\n                        alias = None\n                        if alias_node:\n                            alias_name = self._find_child_by_type(alias_node, \"name\")\n                            if alias_name:\n                                alias = alias_name.text.decode()\n                        self.namespace_resolver.register_use(fqn, alias)\n        else:\n            # Handle simple use: use App\\User; or use App\\User as U;\n            for child in node.children:\n                if child.type == \"namespace_use_clause\":\n                    name_node = self._find_child_by_type(child, \"qualified_name\") or \\\n                                self._find_child_by_type(child, \"namespace_name\")\n                    alias_node = self._find_child_by_type(child, \"namespace_aliasing_clause\")\n\n                    if name_node:\n                        fqn = name_node.text.decode()\n                        alias = None\n                        if alias_node:\n                            alias_name = self._find_child_by_type(alias_node, \"name\")\n                            if alias_name:\n                                alias = alias_name.text.decode()\n                        self.namespace_resolver.register_use(fqn, alias)\n\n    def _extract_nodes(self, node, lines: List[str], depth: int = 0, parent_class: str = None):\n        \"\"\"Extract class, interface, trait, enum, function, and method nodes.\"\"\"\n        if depth > MAX_RECURSION_DEPTH:\n            logger.warning(f\"Max recursion depth reached in {self.file_path}\")\n            return\n\n        node_type = None\n        node_name = None\n        docstring = \"\"\n\n        # Get preceding docstring (PHPDoc)\n        docstring = self._get_preceding_docstring(node, lines)\n\n        if node.type == \"class_declaration\":\n            # Check for abstract class\n            is_abstract = any(\n                c.type == \"abstract_modifier\" or\n                (c.type == \"modifier\" and c.text.decode() == \"abstract\")\n                for c in node.children\n            )\n            node_type = \"abstract class\" if is_abstract else \"class\"\n            name_node = self._find_child_by_type(node, \"name\")\n            node_name = name_node.text.decode() if name_node else None\n\n        elif node.type == \"interface_declaration\":\n            node_type = \"interface\"\n            name_node = self._find_child_by_type(node, \"name\")\n            node_name = name_node.text.decode() if name_node else None\n\n        elif node.type == \"trait_declaration\":\n            node_type = \"trait\"\n            name_node = self._find_child_by_type(node, \"name\")\n            node_name = name_node.text.decode() if name_node else None\n\n        elif node.type == \"enum_declaration\":\n            node_type = \"enum\"\n            name_node = self._find_child_by_type(node, \"name\")\n            node_name = name_node.text.decode() if name_node else None\n\n        elif node.type == \"function_definition\":\n            node_type = \"function\"\n            name_node = self._find_child_by_type(node, \"name\")\n            node_name = name_node.text.decode() if name_node else None\n\n        elif node.type == \"method_declaration\":\n            node_type = \"method\"\n            name_node = self._find_child_by_type(node, \"name\")\n            if name_node:\n                method_name = name_node.text.decode()\n                containing_class = parent_class or self._find_containing_class_name(node)\n                if containing_class:\n                    node_name = f\"{containing_class}.{method_name}\"\n                else:\n                    node_name = method_name\n\n        if node_type and node_name:\n            component_id = self._get_component_id(node_name)\n            relative_path = self._get_relative_path()\n\n            # Extract parameters for functions/methods\n            parameters = None\n            if node_type in (\"function\", \"method\"):\n                parameters = self._extract_parameters(node)\n\n            # Extract base classes for classes\n            base_classes = None\n            if node_type in (\"class\", \"abstract class\"):\n                base_classes = self._extract_base_classes(node)\n\n            node_obj = Node(\n                id=component_id,\n                name=node_name,\n                component_type=node_type,\n                file_path=str(self.file_path),\n                relative_path=relative_path,\n                source_code=\"\\n\".join(lines[node.start_point[0]:node.end_point[0]+1]),\n                start_line=node.start_point[0] + 1,\n                end_line=node.end_point[0] + 1,\n                has_docstring=bool(docstring),\n                docstring=docstring,\n                parameters=parameters,\n                node_type=node_type,\n                base_classes=base_classes,\n                class_name=parent_class,\n                display_name=f\"{node_type} {node_name}\",\n                component_id=component_id\n            )\n            self.nodes.append(node_obj)\n            self._top_level_nodes[node_name] = node_obj\n\n            # Track current class for method extraction\n            if node_type in (\"class\", \"abstract class\", \"interface\", \"trait\", \"enum\"):\n                parent_class = node_name\n\n        # Recursively process children\n        for child in node.children:\n            self._extract_nodes(child, lines, depth + 1, parent_class)\n\n    def _extract_relationships(self, node, depth: int = 0):\n        \"\"\"Extract dependency relationships from the AST.\"\"\"\n        if depth > MAX_RECURSION_DEPTH:\n            return\n\n        # 1. Use statements (already registered, now create relationships)\n        if node.type == \"namespace_use_declaration\":\n            self._add_use_relationships(node)\n\n        # 2. Class inheritance (extends)\n        if node.type == \"class_declaration\":\n            class_name = self._get_name_from_node(node)\n            base_clause = self._find_child_by_type(node, \"base_clause\")\n            if base_clause and class_name:\n                base_name = self._get_type_from_clause(base_clause)\n                if base_name and not self._is_primitive(base_name):\n                    resolved_base = self.namespace_resolver.resolve(base_name)\n                    self.call_relationships.append(CallRelationship(\n                        caller=self._get_component_id(class_name),\n                        callee=resolved_base.replace(\"\\\\\", \".\"),\n                        call_line=node.start_point[0] + 1,\n                        is_resolved=False\n                    ))\n\n        # 3. Interface implementation (implements)\n        if node.type in (\"class_declaration\", \"enum_declaration\"):\n            implementer_name = self._get_name_from_node(node)\n            interface_clause = self._find_child_by_type(node, \"class_interface_clause\")\n            if interface_clause and implementer_name:\n                for child in interface_clause.children:\n                    if child.type in (\"name\", \"qualified_name\"):\n                        interface_name = child.text.decode()\n                        if not self._is_primitive(interface_name):\n                            resolved_interface = self.namespace_resolver.resolve(interface_name)\n                            self.call_relationships.append(CallRelationship(\n                                caller=self._get_component_id(implementer_name),\n                                callee=resolved_interface.replace(\"\\\\\", \".\"),\n                                call_line=node.start_point[0] + 1,\n                                is_resolved=False\n                            ))\n\n        # 4. Object creation (new)\n        if node.type == \"object_creation_expression\":\n            containing_class = self._find_containing_class_name(node)\n            type_node = self._find_child_by_type(node, \"name\") or \\\n                       self._find_child_by_type(node, \"qualified_name\")\n            if type_node:\n                created_type = type_node.text.decode()\n                if not self._is_primitive(created_type) and containing_class:\n                    resolved_type = self.namespace_resolver.resolve(created_type)\n                    self.call_relationships.append(CallRelationship(\n                        caller=self._get_component_id(containing_class),\n                        callee=resolved_type.replace(\"\\\\\", \".\"),\n                        call_line=node.start_point[0] + 1,\n                        is_resolved=False\n                    ))\n\n        # 5. Static method calls (::)\n        if node.type == \"scoped_call_expression\":\n            containing_class = self._find_containing_class_name(node)\n            scope_node = self._find_child_by_type(node, \"name\") or \\\n                        self._find_child_by_type(node, \"qualified_name\")\n            if scope_node and containing_class:\n                target_class = scope_node.text.decode()\n                if not self._is_primitive(target_class):\n                    resolved_target = self.namespace_resolver.resolve(target_class)\n                    self.call_relationships.append(CallRelationship(\n                        caller=self._get_component_id(containing_class),\n                        callee=resolved_target.replace(\"\\\\\", \".\"),\n                        call_line=node.start_point[0] + 1,\n                        is_resolved=False\n                    ))\n\n        # 6. Property promotion in constructor (PHP 8+)\n        if node.type == \"property_promotion_parameter\":\n            containing_class = self._find_containing_class_name(node)\n            type_node = self._find_child_by_type(node, \"type_list\") or \\\n                       self._find_child_by_type(node, \"named_type\")\n            if type_node and containing_class:\n                type_name = self._extract_type_name(type_node)\n                if type_name and not self._is_primitive(type_name):\n                    resolved_type = self.namespace_resolver.resolve(type_name)\n                    self.call_relationships.append(CallRelationship(\n                        caller=self._get_component_id(containing_class),\n                        callee=resolved_type.replace(\"\\\\\", \".\"),\n                        call_line=node.start_point[0] + 1,\n                        is_resolved=False\n                    ))\n\n        # Recursively process children\n        for child in node.children:\n            self._extract_relationships(child, depth + 1)\n\n    def _add_use_relationships(self, node):\n        \"\"\"Add relationships for use statements.\"\"\"\n        # Get all use clauses from the declaration\n        for child in node.children:\n            if child.type == \"namespace_use_clause\":\n                name_node = self._find_child_by_type(child, \"qualified_name\") or \\\n                           self._find_child_by_type(child, \"namespace_name\")\n                if name_node:\n                    fqn = name_node.text.decode().replace(\"\\\\\", \".\")\n                    # Add relationship from file to imported class\n                    file_id = self._get_module_path()\n                    self.call_relationships.append(CallRelationship(\n                        caller=file_id,\n                        callee=fqn,\n                        call_line=node.start_point[0] + 1,\n                        is_resolved=False\n                    ))\n            elif child.type == \"namespace_use_group\":\n                prefix_node = self._find_child_by_type(node, \"namespace_name\")\n                prefix = prefix_node.text.decode() if prefix_node else \"\"\n\n                for group_child in child.children:\n                    if group_child.type == \"namespace_use_group_clause\":\n                        name_node = self._find_child_by_type(group_child, \"namespace_name\")\n                        if name_node:\n                            fqn = f\"{prefix}\\\\{name_node.text.decode()}\" if prefix else name_node.text.decode()\n                            file_id = self._get_module_path()\n                            self.call_relationships.append(CallRelationship(\n                                caller=file_id,\n                                callee=fqn.replace(\"\\\\\", \".\"),\n                                call_line=node.start_point[0] + 1,\n                                is_resolved=False\n                            ))\n\n    def _find_child_by_type(self, node, child_type: str):\n        \"\"\"Find first child of a specific type.\"\"\"\n        for child in node.children:\n            if child.type == child_type:\n                return child\n        return None\n\n    def _get_name_from_node(self, node) -> Optional[str]:\n        \"\"\"Get name from a declaration node.\"\"\"\n        name_node = self._find_child_by_type(node, \"name\")\n        return name_node.text.decode() if name_node else None\n\n    def _get_type_from_clause(self, clause_node) -> Optional[str]:\n        \"\"\"Extract type name from a base_clause or interface_clause.\"\"\"\n        for child in clause_node.children:\n            if child.type in (\"name\", \"qualified_name\"):\n                return child.text.decode()\n        return None\n\n    def _extract_type_name(self, type_node) -> Optional[str]:\n        \"\"\"Extract type name from a type node.\"\"\"\n        if type_node.type == \"named_type\":\n            name_node = self._find_child_by_type(type_node, \"name\") or \\\n                       self._find_child_by_type(type_node, \"qualified_name\")\n            if name_node:\n                return name_node.text.decode()\n        elif type_node.type in (\"name\", \"qualified_name\"):\n            return type_node.text.decode()\n        elif type_node.type == \"type_list\":\n            # Get first type from union/intersection\n            for child in type_node.children:\n                if child.type == \"named_type\":\n                    return self._extract_type_name(child)\n        return type_node.text.decode() if hasattr(type_node, 'text') else None\n\n    def _find_containing_class_name(self, node) -> Optional[str]:\n        \"\"\"Find the name of the containing class/interface/trait/enum.\"\"\"\n        current = node.parent\n        while current:\n            if current.type in (\"class_declaration\", \"interface_declaration\",\n                               \"trait_declaration\", \"enum_declaration\"):\n                name_node = self._find_child_by_type(current, \"name\")\n                if name_node:\n                    return name_node.text.decode()\n            current = current.parent\n        return None\n\n    def _get_preceding_docstring(self, node, lines: List[str]) -> str:\n        \"\"\"Extract PHPDoc comment preceding a node.\"\"\"\n        if node.start_point[0] == 0:\n            return \"\"\n\n        # Look at previous sibling or check lines before\n        prev_sibling = node.prev_named_sibling\n        if prev_sibling and prev_sibling.type == \"comment\":\n            comment_text = prev_sibling.text.decode()\n            if comment_text.startswith(\"/**\"):\n                return comment_text\n\n        # Check lines directly before the node\n        start_line = node.start_point[0]\n        if start_line > 0:\n            for i in range(start_line - 1, max(0, start_line - 10), -1):\n                line = lines[i].strip() if i < len(lines) else \"\"\n                if line.endswith(\"*/\"):\n                    # Found end of docblock, now find start\n                    docblock_lines = []\n                    for j in range(i, max(0, i - 50), -1):\n                        docblock_lines.insert(0, lines[j] if j < len(lines) else \"\")\n                        if \"/**\" in (lines[j] if j < len(lines) else \"\"):\n                            return \"\\n\".join(docblock_lines)\n                elif line and not line.startswith(\"*\") and not line.startswith(\"/**\"):\n                    break\n\n        return \"\"\n\n    def _extract_parameters(self, node) -> Optional[List[str]]:\n        \"\"\"Extract function/method parameters as list of strings.\"\"\"\n        params_node = self._find_child_by_type(node, \"formal_parameters\")\n        if params_node:\n            params = []\n            for child in params_node.children:\n                if child.type in (\"simple_parameter\", \"property_promotion_parameter\", \"variadic_parameter\"):\n                    # Get the variable name\n                    var_node = self._find_child_by_type(child, \"variable_name\")\n                    if var_node:\n                        param_text = var_node.text.decode()\n                        # Get type if present\n                        type_node = self._find_child_by_type(child, \"named_type\") or \\\n                                   self._find_child_by_type(child, \"primitive_type\")\n                        if type_node:\n                            param_text = f\"{type_node.text.decode()} {param_text}\"\n                        params.append(param_text)\n            return params if params else None\n        return None\n\n    def _extract_base_classes(self, node) -> Optional[List[str]]:\n        \"\"\"Extract base class names from a class declaration.\"\"\"\n        base_classes = []\n\n        base_clause = self._find_child_by_type(node, \"base_clause\")\n        if base_clause:\n            for child in base_clause.children:\n                if child.type in (\"name\", \"qualified_name\"):\n                    base_classes.append(child.text.decode())\n\n        interface_clause = self._find_child_by_type(node, \"class_interface_clause\")\n        if interface_clause:\n            for child in interface_clause.children:\n                if child.type in (\"name\", \"qualified_name\"):\n                    base_classes.append(child.text.decode())\n\n        return base_classes if base_classes else None\n\n    def _is_primitive(self, type_name: str) -> bool:\n        \"\"\"Check if type is a PHP primitive or built-in type.\"\"\"\n        if not type_name:\n            return True\n        # Remove leading backslash and check\n        clean_name = type_name.lstrip(\"\\\\\").split(\"\\\\\")[-1]\n        return clean_name.lower() in {p.lower() for p in PHP_PRIMITIVES}",
    "start_line": 87,
    "end_line": 589,
    "has_docstring": true,
    "docstring": "Analyzes PHP files using tree-sitter to extract nodes and relationships.",
    "parameters": null,
    "node_type": "class",
    "base_classes": null,
    "class_name": null,
    "display_name": "class TreeSitterPHPAnalyzer",
    "component_id": "codewiki.src.be.dependency_analyzer.analyzers.php.TreeSitterPHPAnalyzer"
  },
  "codewiki.src.be.dependency_analyzer.analyzers.php.analyze_php_file": {
    "id": "codewiki.src.be.dependency_analyzer.analyzers.php.analyze_php_file",
    "name": "analyze_php_file",
    "component_type": "function",
    "file_path": "/Users/wufagang/project/aiopen/CodeWiki/codewiki/src/be/dependency_analyzer/analyzers/php.py",
    "relative_path": "codewiki/src/be/dependency_analyzer/analyzers/php.py",
    "depends_on": [
      "codewiki.src.be.dependency_analyzer.analyzers.php.TreeSitterPHPAnalyzer"
    ],
    "source_code": "def analyze_php_file(file_path: str, content: str, repo_path: str = None) -> Tuple[List[Node], List[CallRelationship]]:\n    \"\"\"\n    Analyze a PHP file and extract nodes and call relationships.\n\n    Args:\n        file_path: Path to the PHP file\n        content: Content of the PHP file\n        repo_path: Optional path to the repository root\n\n    Returns:\n        Tuple of (nodes, call_relationships)\n    \"\"\"\n    analyzer = TreeSitterPHPAnalyzer(file_path, content, repo_path)\n    return analyzer.nodes, analyzer.call_relationships",
    "start_line": 592,
    "end_line": 605,
    "has_docstring": true,
    "docstring": "Analyze a PHP file and extract nodes and call relationships.\n\nArgs:\n    file_path: Path to the PHP file\n    content: Content of the PHP file\n    repo_path: Optional path to the repository root\n\nReturns:\n    Tuple of (nodes, call_relationships)",
    "parameters": [
      "file_path",
      "content",
      "repo_path"
    ],
    "node_type": "function",
    "base_classes": null,
    "class_name": null,
    "display_name": "function analyze_php_file",
    "component_id": "codewiki.src.be.dependency_analyzer.analyzers.php.analyze_php_file"
  },
  "codewiki.src.be.dependency_analyzer.analyzers.python.PythonASTAnalyzer": {
    "id": "codewiki.src.be.dependency_analyzer.analyzers.python.PythonASTAnalyzer",
    "name": "PythonASTAnalyzer",
    "component_type": "class",
    "file_path": "/Users/wufagang/project/aiopen/CodeWiki/codewiki/src/be/dependency_analyzer/analyzers/python.py",
    "relative_path": "codewiki/src/be/dependency_analyzer/analyzers/python.py",
    "depends_on": [
      "codewiki.src.be.dependency_analyzer.models.core.CallRelationship",
      "codewiki.cli.utils.errors.warning",
      "codewiki.src.be.dependency_analyzer.models.core.Node"
    ],
    "source_code": "class PythonASTAnalyzer(ast.NodeVisitor):\n\n    def __init__(self, file_path: str, content: str, repo_path: Optional[str] = None):\n        \"\"\"\n        Initialize the Python AST analyzer.\n\n        Args:\n            file_path: Path to the Python file being analyzed\n            content: Raw content of the Python file\n            repo_path: Repository root path for calculating relative paths\n        \"\"\"\n        self.file_path = file_path\n        self.repo_path = repo_path\n        self.content = content\n        self.lines = content.splitlines()\n        self.nodes: List[Node] = []\n        self.call_relationships: List[CallRelationship] = []\n        self.current_class_name: str | None = None\n        self.current_function_name: str | None = None\n        \n        self.top_level_nodes = {}\n    \n    def _get_relative_path(self) -> str:\n        \"\"\"Get relative path from repo root.\"\"\"\n        if self.repo_path:\n            return os.path.relpath(self.file_path, self.repo_path)\n        return str(self.file_path)\n\n    def _get_module_path(self) -> str:\n        try:\n            relative_path = self._get_relative_path()\n            path = relative_path\n            for ext in ['.py', '.pyx']:\n                if path.endswith(ext):\n                    path = path[:-len(ext)]\n                    break\n            return path.replace('/', '.').replace('\\\\', '.')\n        except:\n            return str(self.file_path).replace('/', '.').replace('\\\\', '.')\n    \n    def _get_component_id(self, name: str) -> str:\n        \"\"\"Generate dot-separated component ID.\"\"\"\n        module_path = self._get_module_path()\n        if self.current_class_name:\n            return f\"{module_path}.{self.current_class_name}.{name}\"\n        else:\n            return f\"{module_path}.{name}\"\n\n    def generic_visit(self, node):\n        \"\"\"Override generic_visit to continue AST traversal.\"\"\"\n        super().generic_visit(node)\n\n    def visit_ClassDef(self, node: ast.ClassDef):\n        \"\"\"Visit class definition and add to top-level nodes.\"\"\"\n\n        base_classes = [self._extract_base_class_name(base) for base in node.bases]\n        base_classes = [name for name in base_classes if name is not None]\n        \n        component_id = f\"{self._get_module_path()}.{node.name}\"\n        relative_path = self._get_relative_path()\n        \n        class_node = Node(\n            id=component_id,\n            name=node.name,\n            component_type=\"class\",\n            file_path=str(self.file_path),\n            relative_path=relative_path,\n            source_code=\"\\n\".join(self.lines[node.lineno - 1 : node.end_lineno or node.lineno]),\n            start_line=node.lineno,\n            end_line=node.end_lineno,\n            has_docstring=bool(ast.get_docstring(node)),\n            docstring=ast.get_docstring(node) or \"\",\n            parameters=None,\n            node_type=\"class\",\n            base_classes=base_classes if base_classes else None,\n            class_name=None,\n            display_name=f\"class {node.name}\",\n            component_id=component_id\n        )\n        self.nodes.append(class_node)\n        self.top_level_nodes[node.name] = class_node\n\n        for base_name in base_classes:\n            if base_name in self.top_level_nodes:\n                self.call_relationships.append(CallRelationship(\n                    caller=component_id,\n                    callee=f\"{self._get_module_path()}.{base_name}\",\n                    call_line=node.lineno,\n                    is_resolved=True\n                ))\n\n        self.current_class_name = node.name\n        self.generic_visit(node)\n        self.current_class_name = None\n    \n    def _extract_base_class_name(self, base):\n        \"\"\"Extract base class name from AST node.\"\"\"\n        if isinstance(base, ast.Name):\n            return base.id\n        elif isinstance(base, ast.Attribute):\n            parts = []\n            node = base\n            while isinstance(node, ast.Attribute):\n                parts.append(node.attr)\n                node = node.value\n            if isinstance(node, ast.Name):\n                parts.append(node.id)\n            return \".\".join(reversed(parts))\n        return None\n\n    def _process_function_node(self, node: ast.FunctionDef | ast.AsyncFunctionDef):\n        \"\"\"Process function definition - only add to nodes if it's top-level.\"\"\"\n\n        if not self.current_class_name:\n            component_id = f\"{self._get_module_path()}.{node.name}\"\n            relative_path = self._get_relative_path()\n            \n            func_node = Node(\n                id=component_id,\n                name=node.name,\n                component_type=\"function\",\n                file_path=str(self.file_path),\n                relative_path=relative_path,\n                source_code=\"\\n\".join(self.lines[node.lineno - 1 : node.end_lineno or node.lineno]),\n                start_line=node.lineno,\n                end_line=node.end_lineno,\n                has_docstring=bool(ast.get_docstring(node)),\n                docstring=ast.get_docstring(node) or \"\",\n                parameters=[arg.arg for arg in node.args.args],\n                node_type=\"function\",\n                base_classes=None,\n                class_name=None,\n                display_name=f\"function {node.name}\",\n                component_id=component_id\n            )\n            if self._should_include_function(func_node):\n                self.nodes.append(func_node)\n                self.top_level_nodes[node.name] = func_node\n\n        self.current_function_name = node.name\n        self.generic_visit(node)\n        self.current_function_name = None\n\n    def _should_include_function(self, func: Node) -> bool:\n        if func.name.startswith(\"_test_\"):\n            return False\n        return True\n\n    def visit_FunctionDef(self, node: ast.FunctionDef):\n        \"\"\"Visit function definition and extract function information.\"\"\"\n        self._process_function_node(node)\n\n    def visit_AsyncFunctionDef(self, node: ast.AsyncFunctionDef):\n        \"\"\"Visit async function definition and extract function information.\"\"\"\n        self._process_function_node(node)\n\n    def visit_Call(self, node: ast.Call):\n        \"\"\"Visit function call nodes and record relationships between top-level nodes.\"\"\"\n\n        if self.current_class_name or (self.current_function_name and not self.current_class_name):\n            call_name = self._get_call_name(node.func)\n            if call_name:\n                if self.current_class_name:\n                    caller_id = f\"{self._get_module_path()}.{self.current_class_name}\"\n                else:\n                    caller_id = f\"{self._get_module_path()}.{self.current_function_name}\"\n                \n                if call_name in self.top_level_nodes:\n                    callee_id = f\"{self._get_module_path()}.{call_name}\"\n                else:\n                    callee_id = call_name\n                \n                relationship = CallRelationship(\n                    caller=caller_id,\n                    callee=callee_id,\n                    call_line=node.lineno,\n                    is_resolved=call_name in self.top_level_nodes  \n                )\n                self.call_relationships.append(relationship)\n\n        self.generic_visit(node)\n\n    def _get_call_name(self, node) -> str | None:\n        \"\"\"\n        Extract function name from a call node.\n        Handles simple names, attributes (obj.method), and filters built-ins.\n        \"\"\"\n        PYTHON_BUILTINS = {\n            \"print\", \"len\", \"str\", \"int\", \"float\", \"bool\", \"list\", \"dict\", \"tuple\", \"set\",\n            \"range\", \"enumerate\", \"zip\", \"isinstance\", \"hasattr\", \"getattr\", \"setattr\",\n            \"open\", \"super\", \"__import__\", \"type\", \"object\", \"Exception\", \"ValueError\",\n            \"TypeError\", \"KeyError\", \"IndexError\", \"AttributeError\", \"ImportError\",\n            \"max\", \"min\", \"sum\", \"abs\", \"round\", \"sorted\", \"reversed\", \"filter\", \"map\",\n            \"any\", \"all\", \"next\", \"iter\", \"callable\", \"repr\", \"format\", \"exec\", \"eval\"\n        }\n\n        if isinstance(node, ast.Name):\n            if node.id in PYTHON_BUILTINS:\n                return None\n            return node.id\n        elif isinstance(node, ast.Attribute):\n            if isinstance(node.value, ast.Name):\n                if node.value.id in PYTHON_BUILTINS:\n                    return None\n                return f\"{node.value.id}.{node.attr}\"\n            elif isinstance(node.value, ast.Attribute):\n                base_name = self._get_call_name(node.value)\n                if base_name:\n                    return f\"{base_name}.{node.attr}\"\n            return node.attr\n        return None\n\n    def analyze(self):\n        \"\"\"Analyze the Python file and extract functions and relationships.\"\"\"\n\n        try:\n            # Suppress SyntaxWarnings about invalid escape sequences in source code\n            # These warnings come from regex patterns like '\\(' or '\\.' in the analyzed files\n            with warnings.catch_warnings():\n                warnings.filterwarnings(\"ignore\", category=SyntaxWarning)\n                tree = ast.parse(self.content)\n            self.visit(tree)\n\n            logger.debug(\n                f\"Python analysis complete for {self.file_path}: {len(self.nodes)} nodes, \"\n                f\"{len(self.call_relationships)} relationships\"\n            )\n        except SyntaxError as e:\n            logger.warning(f\"Could not parse {self.file_path}: {e}\")\n        except Exception as e:\n            logger.error(f\"Error analyzing {self.file_path}: {e}\", exc_info=True)",
    "start_line": 15,
    "end_line": 245,
    "has_docstring": false,
    "docstring": "",
    "parameters": null,
    "node_type": "class",
    "base_classes": [
      "ast.NodeVisitor"
    ],
    "class_name": null,
    "display_name": "class PythonASTAnalyzer",
    "component_id": "codewiki.src.be.dependency_analyzer.analyzers.python.PythonASTAnalyzer"
  },
  "codewiki.src.be.dependency_analyzer.analyzers.python.analyze_python_file": {
    "id": "codewiki.src.be.dependency_analyzer.analyzers.python.analyze_python_file",
    "name": "analyze_python_file",
    "component_type": "function",
    "file_path": "/Users/wufagang/project/aiopen/CodeWiki/codewiki/src/be/dependency_analyzer/analyzers/python.py",
    "relative_path": "codewiki/src/be/dependency_analyzer/analyzers/python.py",
    "depends_on": [
      "codewiki.src.be.dependency_analyzer.analyzers.python.PythonASTAnalyzer"
    ],
    "source_code": "def analyze_python_file(\n    file_path: str, content: str, repo_path: Optional[str] = None\n) -> Tuple[List[Node], List[CallRelationship]]:\n    \"\"\"\n    Analyze a Python file and return classes, functions, methods, and relationships.\n\n    Args:\n        file_path: Path to the Python file\n        content: Content of the Python file\n        repo_path: Repository root path for calculating relative paths\n\n    Returns:\n        tuple: (classes, functions, methods, call_relationships)\n    \"\"\"\n\n    analyzer = PythonASTAnalyzer(file_path, content, repo_path)\n    analyzer.analyze()\n    return analyzer.nodes, analyzer.call_relationships",
    "start_line": 248,
    "end_line": 265,
    "has_docstring": true,
    "docstring": "Analyze a Python file and return classes, functions, methods, and relationships.\n\nArgs:\n    file_path: Path to the Python file\n    content: Content of the Python file\n    repo_path: Repository root path for calculating relative paths\n\nReturns:\n    tuple: (classes, functions, methods, call_relationships)",
    "parameters": [
      "file_path",
      "content",
      "repo_path"
    ],
    "node_type": "function",
    "base_classes": null,
    "class_name": null,
    "display_name": "function analyze_python_file",
    "component_id": "codewiki.src.be.dependency_analyzer.analyzers.python.analyze_python_file"
  },
  "codewiki.src.be.dependency_analyzer.analyzers.typescript.TreeSitterTSAnalyzer": {
    "id": "codewiki.src.be.dependency_analyzer.analyzers.typescript.TreeSitterTSAnalyzer",
    "name": "TreeSitterTSAnalyzer",
    "component_type": "class",
    "file_path": "/Users/wufagang/project/aiopen/CodeWiki/codewiki/src/be/dependency_analyzer/analyzers/typescript.py",
    "relative_path": "codewiki/src/be/dependency_analyzer/analyzers/typescript.py",
    "depends_on": [
      "codewiki.src.be.dependency_analyzer.models.core.CallRelationship",
      "codewiki.src.be.dependency_analyzer.models.core.Node"
    ],
    "source_code": "class TreeSitterTSAnalyzer:\n\n    def __init__(self, file_path: str, content: str, repo_path: str = None):\n        self.file_path = Path(file_path)\n        self.content = content\n        self.repo_path = repo_path or \"\"\n        self.nodes: List[Node] = []\n        self.call_relationships: List[CallRelationship] = []\n        \n        self.top_level_nodes = {}\n\n        try:\n            language_capsule = tree_sitter_typescript.language_typescript()\n            self.ts_language = Language(language_capsule)\n            self.parser = Parser(self.ts_language)\n\n        except Exception as e:\n            logger.error(f\"Failed to initialize TypeScript parser: {e}\")\n            logger.error(f\"Traceback: {traceback.format_exc()}\")\n            self.parser = None\n            self.ts_language = None\n\n    def analyze(self) -> None:\n        if self.parser is None:\n            logger.debug(f\"Skipping {self.file_path} - parser initialization failed\")\n            return\n\n        try:\n            tree = self.parser.parse(bytes(self.content, \"utf8\"))\n            root_node = tree.root_node\n\n            logger.debug(f\"Parsed AST with root node type: {root_node.type}\")\n\n            all_entities = {}  \n            self._extract_all_entities(root_node, all_entities)\n            \n            self._filter_top_level_declarations(all_entities)\n            \n            self._extract_all_relationships(root_node, all_entities)\n\n        except Exception as e:\n            logger.error(f\"Error analyzing TypeScript file {self.file_path}: {e}\", exc_info=True)\n\n    def _extract_all_entities(self, node, all_entities: dict, depth=0) -> None:\n        entity = None\n        entity_name = None\n        \n        if node.type == \"function_declaration\":\n            entity = self._extract_function_entity(node, \"function\", depth)\n        elif node.type == \"generator_function_declaration\":\n            entity = self._extract_function_entity(node, \"generator_function\", depth)\n        elif node.type == \"arrow_function\":\n            entity = self._extract_arrow_function_entity(node, depth)\n        elif node.type == \"method_definition\":\n            entity = self._extract_method_entity(node, depth)\n        elif node.type == \"class_declaration\":\n            entity = self._extract_class_entity(node, \"class\", depth)\n        elif node.type == \"abstract_class_declaration\":\n            entity = self._extract_class_entity(node, \"abstract_class\", depth)\n        elif node.type == \"interface_declaration\":\n            entity = self._extract_interface_entity(node, depth)\n        elif node.type == \"type_alias_declaration\":\n            entity = self._extract_type_alias_entity(node, depth)\n        elif node.type == \"enum_declaration\":\n            entity = self._extract_enum_entity(node, depth)\n        elif node.type == \"variable_declarator\":\n            entity = self._extract_variable_entity(node, depth)\n        elif node.type == \"export_statement\":\n            entity = self._extract_export_statement_entity(node, depth)\n        elif node.type == \"lexical_declaration\":\n            entity = self._extract_lexical_declaration_entity(node, depth)\n        elif node.type == \"variable_declaration\":\n            entity = self._extract_variable_declaration_entity(node, depth)\n        elif node.type == \"ambient_declaration\":\n            entity = self._extract_ambient_declaration_entity(node, depth)\n        \n        if entity and entity.get('name'):\n            entity_name = entity['name']\n            entity['depth'] = depth  \n            entity['node'] = node   \n            entity['parent_context'] = self._get_parent_context(node)  \n            all_entities[entity_name] = entity\n        \n        for child in node.children:\n            self._extract_all_entities(child, all_entities, depth + 1)\n    \n    def _filter_top_level_declarations(self, all_entities: dict) -> None:\n        for entity_name, entity_data in all_entities.items():\n            if self._is_actually_top_level(entity_data):\n                node_obj = self._create_node_from_entity(entity_data)\n                if node_obj and self._should_include_node(node_obj):\n                    self.nodes.append(node_obj)\n                    self.top_level_nodes[entity_name] = node_obj\n                        \n                    if entity_data[\"type\"] in [\"class_declaration\", \"abstract_class_declaration\"]:\n                        self._extract_constructor_dependencies(entity_data[\"node\"], entity_name)\n    \n    def _is_actually_top_level(self, entity_data: dict) -> bool:\n        node = entity_data.get('node')\n        if not node or not node.parent:\n            return True\n        \n        entity_type = entity_data.get('type')\n        if self._is_inside_function_body(node):\n            return False\n        \n        current = node.parent\n        while current:\n            parent_type = current.type\n            \n            if parent_type == \"program\":\n                return True\n            \n            if parent_type == \"export_statement\":\n                return True\n                \n            if parent_type == \"ambient_declaration\":\n                return True\n                \n            if parent_type == \"module\":\n                return True\n                \n            if parent_type == \"statement_block\":\n                grandparent = current.parent\n                if grandparent and grandparent.type in [\"module\", \"ambient_declaration\"]:\n                    return True\n            \n            current = current.parent\n        \n        return False\n    \n    def _is_inside_function_body(self, node) -> bool:\n        current = node.parent\n        while current:\n            if current.type == \"statement_block\":\n                if current.parent and current.parent.type in [\n                    \"function_declaration\", \"generator_function_declaration\", \n                    \"arrow_function\", \"function_expression\", \"method_definition\"\n                ]:\n                    return True\n            current = current.parent\n        return False\n\n    def _extract_ambient_declaration_entity(self, node, depth: int) -> dict:\n        name = \"\"\n        for child in node.children:\n            if child.type == \"module\":\n                for grandchild in child.children:\n                    if grandchild.type == \"string\":\n                        name = self._get_node_text(grandchild).strip(\"'\\\"\")\n                        break\n                break\n            elif child.type == \"namespace\":\n                name = self._get_node_text(child.children[1]) if len(child.children) > 1 else \"unknown_namespace\"\n                break\n        \n        return {\n            'name': f\"{name}\",\n            'type': 'ambient_declaration',\n            'start_line': node.start_point[0] + 1,\n            'end_line': node.end_point[0] + 1,\n            'parameters': [],\n            'return_type': None,\n            'modifiers': ['ambient'],\n            'complexity': 1\n        }\n    \n    def _get_parent_context(self, node) -> str:\n        \"\"\"Get the parent context of a node for better top-level detection\"\"\"\n        if not node.parent:\n            return \"root\"\n        \n        parent_type = node.parent.type\n        if parent_type in [\"program\", \"source_file\"]:\n            return \"program\"\n        elif parent_type == \"export_statement\":\n            return \"export\"\n        elif parent_type == \"ambient_declaration\":\n            return \"ambient\"\n        elif parent_type == \"module\":\n            return \"module\"\n        elif parent_type == \"statement_block\":\n            if node.parent.parent and node.parent.parent.type in [\"module\", \"ambient_declaration\"]:\n                return \"module_block\"\n            return \"statement_block\"\n    def _extract_function_entity(self, node, func_type: str, depth: int) -> dict:\n        name_node = self._find_child_by_type(node, \"identifier\")\n        if not name_node:\n            return None\n        \n        func_name = self._get_node_text(name_node)\n        parameters = self._extract_parameters(node)\n        code_snippet = self._get_node_text(node)\n        \n        is_async = \"async\" in code_snippet.split(\"function\")[0] if \"function\" in code_snippet else False\n        display_name = f\"{'async ' if is_async else ''}{func_type} {func_name}\"\n        \n        return {\n            'name': func_name,\n            'type': 'function',\n            'subtype': func_type,\n            'parameters': parameters,\n            'code_snippet': code_snippet,\n            'display_name': display_name,\n            'start_line': node.start_point[0] + 1,\n            'end_line': node.end_point[0] + 1,\n            'is_async': is_async\n        }\n    \n    def _extract_arrow_function_entity(self, node, depth: int) -> dict:\n        \"\"\"Extract arrow function\"\"\"\n        parent = node.parent\n        if parent and parent.type == \"variable_declarator\":\n            name_node = self._find_child_by_type(parent, \"identifier\")\n            if name_node:\n                func_name = self._get_node_text(name_node)\n                parameters = self._extract_parameters(node)\n                code_snippet = self._get_node_text(parent)\n                \n                is_async = \"async\" in code_snippet.split(\"=\")[0] if \"=\" in code_snippet else False\n                display_name = f\"{'async ' if is_async else ''}arrow function {func_name}\"\n                \n                return {\n                    'name': func_name,\n                    'type': 'function',\n                    'subtype': 'arrow_function',\n                    'parameters': parameters,\n                    'code_snippet': code_snippet,\n                    'display_name': display_name,\n                    'start_line': node.start_point[0] + 1,\n                    'end_line': node.end_point[0] + 1,\n                    'is_async': is_async\n                }\n        return None\n    \n    def _extract_method_entity(self, node, depth: int) -> dict:\n        \"\"\"Extract method entity (at any depth).\"\"\"\n        name_node = self._find_child_by_type(node, \"property_identifier\")\n        if not name_node:\n            return None\n        \n        method_name = self._get_node_text(name_node)\n        parameters = self._extract_parameters(node)\n        code_snippet = self._get_node_text(node)\n        \n        is_async = \"async\" in code_snippet\n        is_static = \"static\" in code_snippet\n        \n        display_name = f\"{'static ' if is_static else ''}{'async ' if is_async else ''}method {method_name}\"\n        \n        return {\n            'name': method_name,\n            'type': 'function',\n            'subtype': 'method',\n            'parameters': parameters,\n            'code_snippet': code_snippet,\n            'display_name': display_name,\n            'start_line': node.start_point[0] + 1,\n            'end_line': node.end_point[0] + 1,\n            'is_async': is_async,\n            'is_static': is_static\n        }\n    \n    def _extract_class_entity(self, node, class_type: str, depth: int) -> dict:\n        name_node = self._find_child_by_type(node, \"type_identifier\") or self._find_child_by_type(node, \"identifier\")\n        if not name_node:\n            return None\n        \n        class_name = self._get_node_text(name_node)\n        base_classes = self._extract_inheritance(node)\n        code_snippet = self._get_node_text(node)\n        \n        display_name = f\"{class_type} {class_name}\"\n        if base_classes:\n            display_name += f\" extends {', '.join(base_classes)}\"\n        \n        return {\n            'name': class_name,\n            'type': 'class',\n            'subtype': class_type,\n            'base_classes': base_classes,\n            'code_snippet': code_snippet,\n            'display_name': display_name,\n            'start_line': node.start_point[0] + 1,\n            'end_line': node.end_point[0] + 1\n        }\n    \n    def _extract_interface_entity(self, node, depth: int) -> dict:\n        name_node = self._find_child_by_type(node, \"type_identifier\")\n        if not name_node:\n            return None\n        \n        interface_name = self._get_node_text(name_node)\n        base_classes = self._extract_inheritance(node)\n        code_snippet = self._get_node_text(node)\n        \n        display_name = f\"interface {interface_name}\"\n        if base_classes:\n            display_name += f\" extends {', '.join(base_classes)}\"\n        \n        return {\n            'name': interface_name,\n            'type': 'interface',\n            'subtype': 'interface',\n            'base_classes': base_classes,\n            'code_snippet': code_snippet,\n            'display_name': display_name,\n            'start_line': node.start_point[0] + 1,\n            'end_line': node.end_point[0] + 1\n        }\n    \n    def _extract_type_alias_entity(self, node, depth: int) -> dict:\n        name_node = self._find_child_by_type(node, \"type_identifier\")\n        if not name_node:\n            return None\n        \n        type_name = self._get_node_text(name_node)\n        code_snippet = self._get_node_text(node)\n        \n        return {\n            'name': type_name,\n            'type': 'type',\n            'subtype': 'type_alias',\n            'code_snippet': code_snippet,\n            'display_name': f\"type {type_name}\",\n            'start_line': node.start_point[0] + 1,\n            'end_line': node.end_point[0] + 1\n        }\n    \n    def _extract_enum_entity(self, node, depth: int) -> dict:\n        name_node = self._find_child_by_type(node, \"identifier\")\n        if not name_node:\n            return None\n        \n        enum_name = self._get_node_text(name_node)\n        code_snippet = self._get_node_text(node)\n        \n        return {\n            'name': enum_name,\n            'type': 'enum',\n            'subtype': 'enum',\n            'code_snippet': code_snippet,\n            'display_name': f\"enum {enum_name}\",\n            'start_line': node.start_point[0] + 1,\n            'end_line': node.end_point[0] + 1\n        }\n    \n    def _extract_variable_entity(self, node, depth: int) -> dict:\n        name_node = self._find_child_by_type(node, \"identifier\")\n        if not name_node:\n            return None\n        \n        var_name = self._get_node_text(name_node)\n        code_snippet = self._get_node_text(node)\n        \n        has_function = self._find_child_by_type(node, \"arrow_function\") or self._find_child_by_type(node, \"function_expression\")\n        \n        return {\n            'name': var_name,\n            'type': 'variable',\n            'subtype': 'variable',\n            'code_snippet': code_snippet,\n            'display_name': f\"variable {var_name}\",\n            'start_line': node.start_point[0] + 1,\n            'end_line': node.end_point[0] + 1,\n            'has_function': bool(has_function)\n        }\n    \n    def _extract_export_statement_entity(self, node, depth: int) -> dict:\n        code_snippet = self._get_node_text(node)\n        \n        func_decl = self._find_child_by_type(node, \"function_declaration\")\n        class_decl = self._find_child_by_type(node, \"class_declaration\")\n        interface_decl = self._find_child_by_type(node, \"interface_declaration\")\n        lexical_decl = self._find_child_by_type(node, \"lexical_declaration\")\n        \n        if func_decl:\n            name_node = self._find_child_by_type(func_decl, \"identifier\")\n            if name_node:\n                func_name = self._get_node_text(name_node)\n                return {\n                    'name': func_name,  \n                    'type': 'function',  \n                    'subtype': 'export_function',\n                    'code_snippet': code_snippet,\n                    'display_name': f\"export function {func_name}\",\n                    'start_line': node.start_point[0] + 1,\n                    'end_line': node.end_point[0] + 1,\n                    'parameters': self._extract_parameters(func_decl),\n                    'is_export': True\n                }\n        elif class_decl:\n            name_node = self._find_child_by_type(class_decl, \"type_identifier\")\n            if name_node:\n                class_name = self._get_node_text(name_node)\n                return {\n                    'name': class_name,  \n                    'type': 'class',  \n                    'subtype': 'export_class',\n                    'code_snippet': code_snippet,\n                    'display_name': f\"export class {class_name}\",\n                    'start_line': node.start_point[0] + 1,\n                    'end_line': node.end_point[0] + 1,\n                    'base_classes': self._extract_inheritance(class_decl),\n                    'is_export': True\n                }\n        elif interface_decl:\n            name_node = self._find_child_by_type(interface_decl, \"type_identifier\")\n            if name_node:\n                interface_name = self._get_node_text(name_node)\n                return {\n                    'name': interface_name,  \n                    'type': 'interface',  \n                    'subtype': 'export_interface',\n                    'code_snippet': code_snippet,\n                    'display_name': f\"export interface {interface_name}\",\n                    'start_line': node.start_point[0] + 1,\n                    'end_line': node.end_point[0] + 1,\n                    'base_classes': self._extract_inheritance(interface_decl),\n                    'is_export': True\n                }\n        elif lexical_decl:\n            var_declarator = self._find_child_by_type(lexical_decl, \"variable_declarator\")\n            if var_declarator:\n                name_node = self._find_child_by_type(var_declarator, \"identifier\")\n                func_expr = self._find_child_by_type(var_declarator, \"arrow_function\") or self._find_child_by_type(var_declarator, \"function_expression\")\n                if name_node and func_expr:\n                    var_name = self._get_node_text(name_node)\n                    return {\n                        'name': var_name,\n                        'type': 'function',\n                        'subtype': 'export_arrow_function',\n                        'code_snippet': code_snippet,\n                        'display_name': f\"export const {var_name}\",\n                        'start_line': node.start_point[0] + 1,\n                        'end_line': node.end_point[0] + 1,\n                        'parameters': self._extract_parameters(func_expr),\n                        'is_export': True\n                    }\n        \n        default_keyword = None\n        call_expr = None\n        for child in node.children:\n            if child.type == \"default\":\n                default_keyword = child\n            elif child.type == \"call_expression\":\n                call_expr = child\n        \n        if default_keyword and call_expr:\n            callee = call_expr.children[0] if call_expr.children else None\n            if callee:\n                callee_name = self._get_node_text(callee)\n                return {\n                    'name': callee_name,\n                    'type': 'function',\n                    'subtype': 'export_default_call',\n                    'code_snippet': code_snippet,\n                    'display_name': f\"export default {callee_name}(...)\",\n                    'start_line': node.start_point[0] + 1,\n                    'end_line': node.end_point[0] + 1,\n                    'parameters': [],\n                    'is_export': True\n                }\n        \n        return None \n    \n    def _extract_lexical_declaration_entity(self, node, depth: int) -> dict:\n        \"\"\"Extract lexical declaration entity (const/let).\"\"\"\n        # Find the variable declarator\n        var_declarator = self._find_child_by_type(node, \"variable_declarator\")\n        if not var_declarator:\n            return None\n        \n        name_node = self._find_child_by_type(var_declarator, \"identifier\")\n        if not name_node:\n            return None\n        \n        var_name = self._get_node_text(name_node)\n        code_snippet = self._get_node_text(node)\n        \n        # Check declaration type (const/let)\n        decl_type = \"const\" if \"const\" in code_snippet else \"let\"\n        \n        has_function = (self._find_child_by_type(var_declarator, \"arrow_function\") or \n                       self._find_child_by_type(var_declarator, \"function_expression\"))\n        \n        return {\n            'name': var_name,\n            'type': 'variable',\n            'subtype': f'{decl_type}_declaration',\n            'code_snippet': code_snippet,\n            'display_name': f\"{decl_type} {var_name}\",\n            'start_line': node.start_point[0] + 1,\n            'end_line': node.end_point[0] + 1,\n            'has_function': bool(has_function),\n            'declaration_type': decl_type\n        }\n    \n    def _extract_variable_declaration_entity(self, node, depth: int) -> dict:\n        var_declarator = self._find_child_by_type(node, \"variable_declarator\")\n        if not var_declarator:\n            return None\n        \n        name_node = self._find_child_by_type(var_declarator, \"identifier\")\n        if not name_node:\n            return None\n        \n        var_name = self._get_node_text(name_node)\n        code_snippet = self._get_node_text(node)\n        \n        has_function = (self._find_child_by_type(var_declarator, \"arrow_function\") or \n                       self._find_child_by_type(var_declarator, \"function_expression\"))\n        \n        return {\n            'name': var_name,\n            'type': 'variable',\n            'subtype': 'var_declaration',\n            'code_snippet': code_snippet,\n            'display_name': f\"var {var_name}\",\n            'start_line': node.start_point[0] + 1,\n            'end_line': node.end_point[0] + 1,\n            'has_function': bool(has_function),\n            'declaration_type': 'var'\n        }\n    \n    def _create_node_from_entity(self, entity_data: dict) -> Optional[Node]:\n        \"\"\"Create Node object from entity data.\"\"\"\n        try:\n            component_type = entity_data['type']\n            name = entity_data['name']\n            node_type = entity_data.get('subtype', entity_data['type'])\n            \n            component_id = self._get_component_id(name)\n            relative_path = self._get_relative_path()\n            \n            return Node(\n                id=component_id,\n                name=name,\n                component_type=component_type,\n                file_path=str(self.file_path),\n                relative_path=relative_path,\n                source_code=entity_data['code_snippet'],\n                start_line=entity_data['start_line'],\n                end_line=entity_data['end_line'],\n                has_docstring=False,\n                docstring=\"\",\n                parameters=entity_data.get('parameters', []),\n                node_type=node_type,\n                base_classes=entity_data.get('base_classes'),\n                class_name=None,\n                display_name=entity_data['display_name'],\n                component_id=component_id,\n            )\n        except Exception as e:\n            logger.debug(f\"Error creating node from entity: {e}\")\n            return None\n        \n    def _should_include_node(self, node: Node) -> bool:\n        excluded_names = {\"constructor\", \"__proto__\", \"prototype\"}\n        \n        if node.component_type == \"variable\":\n            return False\n        \n        return node.name.lower() not in excluded_names\n\n    def _extract_constructor_dependencies(self, class_node, class_name: str) -> None:\n        \"\"\"Extract dependencies from constructor parameters.\"\"\"\n        try:\n            class_body = self._find_child_by_type(class_node, \"class_body\")\n            if not class_body:\n                return\n                \n            for child in class_body.children:\n                if child.type == \"method_definition\":\n                    property_name = self._find_child_by_type(child, \"property_identifier\")\n                    if property_name and self._get_node_text(property_name) == \"constructor\":\n                        # Extract parameter types\n                        formal_params = self._find_child_by_type(child, \"formal_parameters\")\n                        if formal_params:\n                            self._extract_parameter_dependencies(formal_params, class_name)\n                        break\n        except Exception as e:\n            logger.debug(f\"Error extracting constructor dependencies: {e}\")\n\n    def _extract_parameter_dependencies(self, formal_params, caller_name: str) -> None:\n        try:\n            for child in formal_params.children:\n                if child.type in [\"required_parameter\", \"optional_parameter\"]:\n                    type_annotation = self._find_child_by_type(child, \"type_annotation\")\n                    if type_annotation:\n                        type_id = self._find_child_by_type(type_annotation, \"type_identifier\")\n                        if type_id:\n                            dependency_name = self._get_node_text(type_id)\n                            if dependency_name and dependency_name != caller_name:\n                                caller_id = f\"{self._get_module_path()}.{caller_name}\"\n                                callee_id = f\"{self._get_module_path()}.{dependency_name}\"\n                                \n                                relationship = CallRelationship(\n                                    caller=caller_id,\n                                    callee=callee_id,\n                                    call_line=child.start_point[0] + 1,\n                                    is_resolved=False\n                                )\n                                \n                                self._add_relationship(relationship)\n        except Exception as e:\n            logger.debug(f\"Error extracting parameter dependencies: {e}\")\n\n\n    def _get_module_path(self) -> str:\n        if self.repo_path:\n            try:\n                rel_path = os.path.relpath(str(self.file_path), self.repo_path)\n            except ValueError:\n                rel_path = str(self.file_path)\n        else:\n            rel_path = str(self.file_path)\n        \n        for ext in ['.ts', '.tsx', '.js', '.jsx', '.mjs', '.cjs']:\n            if rel_path.endswith(ext):\n                rel_path = rel_path[:-len(ext)]\n                break\n        return rel_path.replace('/', '.').replace('\\\\', '.')\n    \n    def _get_relative_path(self) -> str:\n        if self.repo_path:\n            try:\n                return os.path.relpath(str(self.file_path), self.repo_path)\n            except ValueError:\n                return str(self.file_path)\n        else:\n            return str(self.file_path)\n\n    def _get_component_id(self, name: str) -> str:\n        module_path = self._get_module_path()\n        return f\"{module_path}.{name}\"\n\n    def _extract_inheritance(self, node) -> List[str]:\n        \"\"\"Extract inheritance/implementation relationships.\"\"\"\n        base_classes = []\n        \n        extends_clause = self._find_child_by_type(node, \"extends_clause\")\n        if extends_clause:\n            for child in extends_clause.children:\n                if child.type in [\"identifier\", \"type_identifier\"]:\n                    base_classes.append(self._get_node_text(child))\n        \n        implements_clause = self._find_child_by_type(node, \"implements_clause\")  \n        if implements_clause:\n            for child in implements_clause.children:\n                if child.type in [\"identifier\", \"type_identifier\"]:\n                    base_classes.append(self._get_node_text(child))\n        \n        return base_classes\n\n    def _extract_parameters(self, node) -> List[str]:\n        parameters = []\n        params_node = self._find_child_by_type(node, \"formal_parameters\")\n        if params_node:\n            for child in params_node.children:\n                if child.type in [\"identifier\", \"required_parameter\", \"optional_parameter\"]:\n                    if child.type == \"identifier\":\n                        parameters.append(self._get_node_text(child))\n                    else:\n                        param_name = self._find_child_by_type(child, \"identifier\")\n                        if param_name:\n                            parameters.append(self._get_node_text(param_name))\n        return parameters\n\n    def _extract_all_relationships(self, node, all_entities: dict) -> None:\n        self._traverse_for_relationships(node, all_entities, current_top_level=None)\n\n    def _traverse_for_relationships(self, node, all_entities: dict, current_top_level: str = None) -> None:\n        if current_top_level is None or self._is_new_top_level(node):\n            new_top_level = self._get_top_level_name(node)\n            if new_top_level and new_top_level in self.top_level_nodes:\n                current_top_level = new_top_level\n\n        \n        if current_top_level:\n            if node.type == \"call_expression\":\n                self._extract_call_relationship(node, current_top_level, all_entities)\n            elif node.type == \"new_expression\":\n                self._extract_new_relationship(node, current_top_level, all_entities)\n            \n            elif node.type == \"member_expression\":\n                self._extract_member_relationship(node, current_top_level, all_entities)\n            elif node.type == \"subscript_expression\":\n                self._extract_subscript_relationship(node, current_top_level, all_entities)\n            \n            elif node.type == \"type_annotation\":\n                self._extract_type_relationship(node, current_top_level, all_entities)\n            elif node.type == \"type_arguments\":\n                self._extract_type_arguments_relationship(node, current_top_level, all_entities)\n            \n            elif node.type == \"extends_clause\":\n                self._extract_inheritance_relationship(node, current_top_level, all_entities)\n            elif node.type == \"implements_clause\":\n                self._extract_inheritance_relationship(node, current_top_level, all_entities)\n\n        for child in node.children:\n            self._traverse_for_relationships(child, all_entities, current_top_level)\n    \n    def _is_new_top_level(self, node) -> bool:\n        return node.type in [\n            \"function_declaration\", \"generator_function_declaration\", \n            \"class_declaration\", \"abstract_class_declaration\",\n            \"interface_declaration\", \"type_alias_declaration\", \"enum_declaration\",\n            \"export_statement\"\n        ]\n    \n    def _get_top_level_name(self, node) -> Optional[str]:\n        result = None\n        if node.type in [\"function_declaration\", \"generator_function_declaration\"]:\n            name_node = self._find_child_by_type(node, \"identifier\")\n            result = self._get_node_text(name_node) if name_node else None\n        elif node.type in [\"class_declaration\", \"abstract_class_declaration\", \"interface_declaration\", \"type_alias_declaration\"]:\n            name_node = self._find_child_by_type(node, \"type_identifier\") or self._find_child_by_type(node, \"identifier\")\n            result = self._get_node_text(name_node) if name_node else None\n        elif node.type == \"enum_declaration\":\n            name_node = self._find_child_by_type(node, \"identifier\")\n            result = self._get_node_text(name_node) if name_node else None\n        elif node.type == \"export_statement\":\n            if self._find_child_by_type(node, \"default\"):\n                call_expr = self._find_child_by_type(node, \"call_expression\")\n                if call_expr:\n                    identifier = self._find_child_by_type(call_expr, \"identifier\")\n                    if identifier:\n                        return self._get_node_text(identifier)\n                return \"default_export\"\n            else:\n                func_decl = self._find_child_by_type(node, \"function_declaration\")\n                class_decl = self._find_child_by_type(node, \"class_declaration\")\n                lexical_decl = self._find_child_by_type(node, \"lexical_declaration\")\n                \n                if func_decl:\n                    name_node = self._find_child_by_type(func_decl, \"identifier\")\n                    if name_node:\n                        result = self._get_node_text(name_node)  \n                elif class_decl:\n                    name_node = self._find_child_by_type(class_decl, \"type_identifier\")\n                    if name_node:\n                        result = self._get_node_text(name_node)  \n                elif lexical_decl:\n                    var_declarator = self._find_child_by_type(lexical_decl, \"variable_declarator\")\n                    if var_declarator:\n                        name_node = self._find_child_by_type(var_declarator, \"identifier\")\n                        if name_node:\n                            result = self._get_node_text(name_node)  \n                else:\n                    result = \"unnamed_export\"\n        elif node.type in [\"lexical_declaration\", \"variable_declaration\"]:\n            # const/let/var declarations\n            var_declarator = self._find_child_by_type(node, \"variable_declarator\")\n            if var_declarator:\n                name_node = self._find_child_by_type(var_declarator, \"identifier\")\n                result = self._get_node_text(name_node) if name_node else None\n            else:\n                result = None\n        else:\n            result = None\n        \n        return result\n\n    def _extract_call_relationship(self, node, caller_name: str, all_entities: dict) -> None:\n        try:\n            call_line = node.start_point[0] + 1\n            callee_name = self._extract_callee_name(node)\n            \n            if not callee_name or self._is_builtin_function(callee_name):\n                return\n            \n            call_text = self._get_node_text(node)\n            is_method_call = \"this.\" in call_text or \"super.\" in call_text\n            \n            if is_method_call:\n                current_class = None\n                for entity_name, entity_data in all_entities.items():\n                    if (entity_data.get('type') == 'class' and \n                        caller_name in entity_name): \n                        current_class = entity_name\n                        break\n                \n                if current_class and callee_name in all_entities:\n                    callee_entity = all_entities[callee_name]\n                    if (callee_entity.get('subtype') == 'method' and \n                        callee_name in current_class):\n                        return\n              \n            if callee_name in self.top_level_nodes:\n                self._add_relationship(caller_name, callee_name, call_line)\n            elif callee_name not in all_entities:\n                self._add_relationship(caller_name, callee_name, call_line)\n            elif callee_name in all_entities:\n                entity_data = all_entities[callee_name]\n                if self._is_actually_top_level(entity_data):\n                    self._add_relationship(caller_name, callee_name, call_line)\n                else:\n                    logger.debug(f\"Ignoring nested call: {caller_name} -> {callee_name} (local/nested)\")\n            else:\n                logger.debug(f\"Ignoring unknown call: {caller_name} -> {callee_name}\")\n                \n        except Exception as e:\n            logger.debug(f\"Error extracting call relationship: {e}\")\n\n    def _extract_new_relationship(self, node, caller_name: str, all_entities: dict) -> None:\n        try:\n            call_line = node.start_point[0] + 1\n            if node.children:\n                constructor_node = None\n                for child in node.children:\n                    if child.type not in ['new', 'type_arguments', 'arguments']:\n                        constructor_node = child\n                        break\n                \n                if constructor_node:\n                    constructor_name = self._get_node_text(constructor_node)\n                    \n                    if constructor_name and not self._is_builtin_function(constructor_name):\n                        self._add_relationship(caller_name, constructor_name, call_line)\n\n        except Exception as e:\n            logger.debug(f\"Error extracting new relationship: {e}\")\n\n    def _extract_member_relationship(self, node, caller_name: str, all_entities: dict) -> None:\n        try:\n            call_line = node.start_point[0] + 1\n            property_node = self._find_child_by_type(node, \"property_identifier\")\n            if property_node:\n                property_name = self._get_node_text(property_node)\n                if property_name and not self._is_builtin_function(property_name):\n                    self._add_relationship(caller_name, property_name, call_line)\n        except Exception as e:\n            logger.debug(f\"Error extracting member relationship: {e}\")\n\n    def _extract_subscript_relationship(self, node, caller_name: str, all_entities: dict) -> None:\n        pass\n\n    def _extract_type_relationship(self, node, caller_name: str, all_entities: dict) -> None:\n        try:\n            type_identifiers = []\n            self._find_all_type_identifiers(node, type_identifiers)\n            \n            call_line = node.start_point[0] + 1\n            \n            for type_node in type_identifiers:\n                type_name = self._get_node_text(type_node)\n                \n                if self._is_builtin_type(type_name):\n                    continue\n                \n                if type_name in all_entities:\n                    target_name = self._resolve_to_top_level(type_name, all_entities)\n                    if target_name and target_name in self.top_level_nodes:\n                        self._add_relationship(caller_name, target_name, call_line)\n                else:\n                    self._add_relationship(caller_name, type_name, call_line)\n                    \n        except Exception as e:\n            logger.debug(f\"Error extracting type relationship: {e}\")\n    \n    def _find_all_type_identifiers(self, node, type_identifiers: list) -> None:\n        if node.type == \"type_identifier\":\n            type_identifiers.append(node)\n        \n        for child in node.children:\n            self._find_all_type_identifiers(child, type_identifiers)\n    \n    def _extract_type_arguments_relationship(self, node, caller_name: str, all_entities: dict) -> None:\n        try:\n            for child in node.children:\n                if child.type == \"type_identifier\":\n                    type_name = self._get_node_text(child)\n                    if type_name in all_entities:\n                        target_name = self._resolve_to_top_level(type_name, all_entities)\n                        if target_name and target_name in self.top_level_nodes:\n                            call_line = node.start_point[0] + 1\n                            self._add_relationship(caller_name, target_name, call_line)\n        except Exception as e:\n            logger.debug(f\"Error extracting type arguments relationship: {e}\")\n    \n    def _extract_inheritance_relationship(self, node, caller_name: str, all_entities: dict) -> None:\n        \"\"\"Extract inheritance/implementation relationships\"\"\"\n        try:\n            for child in node.children:\n                if child.type in [\"identifier\", \"type_identifier\"]:\n                    base_name = self._get_node_text(child)\n                    if base_name in all_entities:\n                        target_name = self._resolve_to_top_level(base_name, all_entities)\n                        if target_name and target_name in self.top_level_nodes:\n                            call_line = node.start_point[0] + 1\n                            self._add_relationship(caller_name, target_name, call_line)\n        except Exception as e:\n            logger.debug(f\"Error extracting inheritance relationship: {e}\")\n\n    def _resolve_to_top_level(self, entity_name: str, all_entities: dict) -> Optional[str]:\n        if entity_name in self.top_level_nodes:\n            return entity_name\n        \n        entity_data = all_entities.get(entity_name)\n        if entity_data and entity_data.get('depth', 0) > 2:\n            return None\n        \n        return entity_name if entity_name in self.top_level_nodes else None\n\n    def _add_relationship(self, caller_name: str, callee_name: str, call_line: int) -> None:\n        caller_id = f\"{self._get_module_path()}.{caller_name}\"\n        callee_id = f\"{self._get_module_path()}.{callee_name}\"  \n        \n        relationship = CallRelationship(\n            caller=caller_id,\n            callee=callee_id,\n            call_line=call_line,\n            is_resolved=False,  \n        )\n        self.call_relationships.append(relationship)\n\n    def _extract_callee_name(self, call_node) -> Optional[str]:\n        if call_node.children:\n            callee_node = call_node.children[0]\n\n            if callee_node.type == \"identifier\":\n                return self._get_node_text(callee_node)\n            elif callee_node.type == \"member_expression\":\n                return self._get_node_text(callee_node)\n        return None\n\n    def _is_builtin_type(self, name: str) -> bool:\n        \"\"\"Check if type name is a TypeScript/JavaScript built-in type.\"\"\"\n        builtin_types = {\n            # Primitive types\n            \"string\", \"number\", \"boolean\", \"object\", \"undefined\", \"null\", \"void\", \"never\", \"any\", \"unknown\"\n        }\n        return name in builtin_types\n\n    def _is_builtin_function(self, name: str) -> bool:\n        builtins = {}\n        return name in builtins\n\n    def _find_child_by_type(self, node, node_type: str):\n        for child in node.children:\n            if child.type == node_type:\n                return child\n        return None\n\n    def _get_node_text(self, node) -> str:\n        start_byte = node.start_byte\n        end_byte = node.end_byte\n        return self.content.encode(\"utf8\")[start_byte:end_byte].decode(\"utf8\")",
    "start_line": 17,
    "end_line": 965,
    "has_docstring": false,
    "docstring": "",
    "parameters": null,
    "node_type": "class",
    "base_classes": null,
    "class_name": null,
    "display_name": "class TreeSitterTSAnalyzer",
    "component_id": "codewiki.src.be.dependency_analyzer.analyzers.typescript.TreeSitterTSAnalyzer"
  },
  "codewiki.src.be.dependency_analyzer.analyzers.typescript.analyze_typescript_file_treesitter": {
    "id": "codewiki.src.be.dependency_analyzer.analyzers.typescript.analyze_typescript_file_treesitter",
    "name": "analyze_typescript_file_treesitter",
    "component_type": "function",
    "file_path": "/Users/wufagang/project/aiopen/CodeWiki/codewiki/src/be/dependency_analyzer/analyzers/typescript.py",
    "relative_path": "codewiki/src/be/dependency_analyzer/analyzers/typescript.py",
    "depends_on": [
      "codewiki.src.be.dependency_analyzer.analyzers.typescript.TreeSitterTSAnalyzer"
    ],
    "source_code": "def analyze_typescript_file_treesitter(\n    file_path: str, content: str, repo_path: str = None\n) -> Tuple[List[Node], List[CallRelationship]]:\n    try:\n        logger.debug(f\"Tree-sitter TS analysis for {file_path}\")\n        analyzer = TreeSitterTSAnalyzer(file_path, content, repo_path)\n        analyzer.analyze()\n        logger.debug(\n            f\"Found {len(analyzer.nodes)} top-level nodes, {len(analyzer.call_relationships)} calls\"\n        )\n        return analyzer.nodes, analyzer.call_relationships\n    except Exception as e:\n        logger.error(f\"Error in tree-sitter TS analysis for {file_path}: {e}\", exc_info=True)\n        return [], []",
    "start_line": 969,
    "end_line": 982,
    "has_docstring": false,
    "docstring": "",
    "parameters": [
      "file_path",
      "content",
      "repo_path"
    ],
    "node_type": "function",
    "base_classes": null,
    "class_name": null,
    "display_name": "function analyze_typescript_file_treesitter",
    "component_id": "codewiki.src.be.dependency_analyzer.analyzers.typescript.analyze_typescript_file_treesitter"
  },
  "codewiki.src.be.dependency_analyzer.ast_parser.DependencyParser": {
    "id": "codewiki.src.be.dependency_analyzer.ast_parser.DependencyParser",
    "name": "DependencyParser",
    "component_type": "class",
    "file_path": "/Users/wufagang/project/aiopen/CodeWiki/codewiki/src/be/dependency_analyzer/ast_parser.py",
    "relative_path": "codewiki/src/be/dependency_analyzer/ast_parser.py",
    "depends_on": [
      "codewiki.src.be.dependency_analyzer.analysis.analysis_service.AnalysisService",
      "codewiki.src.be.dependency_analyzer.models.core.Node"
    ],
    "source_code": "class DependencyParser:\n    \"\"\"Parser for extracting code components from multi-language repositories.\"\"\"\n    \n    def __init__(self, repo_path: str):\n        self.repo_path = os.path.abspath(repo_path)\n        self.components: Dict[str, Node] = {}\n        self.modules: Set[str] = set()\n        \n        self.analysis_service = AnalysisService()\n\n    def parse_repository(self, filtered_folders: List[str] = None) -> Dict[str, Node]:\n        logger.debug(f\"Parsing repository at {self.repo_path}\")\n        \n        structure_result = self.analysis_service._analyze_structure(\n            self.repo_path, \n            include_patterns=None,\n            exclude_patterns=None\n        )\n        \n        call_graph_result = self.analysis_service._analyze_call_graph(\n            structure_result[\"file_tree\"], \n            self.repo_path\n        )\n        \n        self._build_components_from_analysis(call_graph_result)\n        \n        logger.debug(f\"Found {len(self.components)} components across {len(self.modules)} modules\")\n        return self.components\n    \n    def _build_components_from_analysis(self, call_graph_result: Dict):\n        functions = call_graph_result.get(\"functions\", [])\n        relationships = call_graph_result.get(\"relationships\", [])\n        \n        component_id_mapping = {}\n        \n        for func_dict in functions:\n            component_id = func_dict.get(\"id\", \"\")\n            if not component_id:\n                continue\n                \n            node = Node(\n                id=component_id,\n                name=func_dict.get(\"name\", \"\"),\n                component_type=func_dict.get(\"component_type\", func_dict.get(\"node_type\", \"function\")),\n                file_path=func_dict.get(\"file_path\", \"\"),\n                relative_path=func_dict.get(\"relative_path\", \"\"),\n                source_code=func_dict.get(\"source_code\", func_dict.get(\"code_snippet\", \"\")),\n                start_line=func_dict.get(\"start_line\", 0),\n                end_line=func_dict.get(\"end_line\", 0),\n                has_docstring=func_dict.get(\"has_docstring\", bool(func_dict.get(\"docstring\", \"\"))),\n                docstring=func_dict.get(\"docstring\", \"\") or \"\",\n                parameters=func_dict.get(\"parameters\", []),\n                node_type=func_dict.get(\"node_type\", \"function\"),\n                base_classes=func_dict.get(\"base_classes\"),\n                class_name=func_dict.get(\"class_name\"),\n                display_name=func_dict.get(\"display_name\", \"\"),\n                component_id=component_id\n            )\n            \n            self.components[component_id] = node\n            \n            component_id_mapping[component_id] = component_id\n            legacy_id = f\"{func_dict.get('file_path', '')}:{func_dict.get('name', '')}\"\n            if legacy_id and legacy_id != component_id:\n                component_id_mapping[legacy_id] = component_id\n            \n            if \".\" in component_id:\n                module_parts = component_id.split(\".\")[:-1]  \n                module_path = \".\".join(module_parts)\n                if module_path:\n                    self.modules.add(module_path)\n        \n        processed_relationships = 0\n        for rel_dict in relationships:\n            caller_id = rel_dict.get(\"caller\", \"\")\n            callee_id = rel_dict.get(\"callee\", \"\")\n            is_resolved = rel_dict.get(\"is_resolved\", False)\n            \n            caller_component_id = component_id_mapping.get(caller_id)\n            \n            callee_component_id = component_id_mapping.get(callee_id)\n            if not callee_component_id:\n                for comp_id, comp_node in self.components.items():\n                    if comp_node.name == callee_id:\n                        callee_component_id = comp_id\n                        break\n            \n            if caller_component_id and caller_component_id in self.components:\n                if callee_component_id:\n                    self.components[caller_component_id].depends_on.add(callee_component_id)\n                    processed_relationships += 1\n    \n    def _determine_component_type(self, func_dict: Dict) -> str:\n        if func_dict.get(\"is_method\", False):\n            return \"method\"\n        \n        node_type = func_dict.get(\"node_type\", \"\")\n        if node_type in [\"class\", \"interface\", \"struct\", \"enum\", \"record\", \"abstract class\", \"annotation\", \"delegate\"]:\n            return node_type\n            \n        return \"function\"\n    \n    def _file_to_module_path(self, file_path: str) -> str:\n        path = file_path\n        extensions = ['.py', '.js', '.ts', '.java', '.cs', '.cpp', '.hpp', '.h', '.c', '.tsx', '.jsx', '.cc', '.mjs', '.cxx', '.cc', '.cjs']\n        for ext in extensions:\n            if path.endswith(ext):\n                path = path[:-len(ext)]\n                break\n        return path.replace(os.path.sep, \".\")\n    \n    def save_dependency_graph(self, output_path: str):\n        result = {}\n        for component_id, component in self.components.items():\n            component_dict = component.model_dump()\n            if 'depends_on' in component_dict and isinstance(component_dict['depends_on'], set):\n                component_dict['depends_on'] = list(component_dict['depends_on'])\n            result[component_id] = component_dict\n        \n        dir_name = os.path.dirname(output_path)\n        if dir_name:\n            os.makedirs(dir_name, exist_ok=True)\n        \n        with open(output_path, 'w', encoding='utf-8') as f:\n            json.dump(result, f, indent=2, ensure_ascii=False)\n        \n        logger.debug(f\"Saved {len(self.components)} components to {output_path}\")\n        return result",
    "start_line": 18,
    "end_line": 145,
    "has_docstring": true,
    "docstring": "Parser for extracting code components from multi-language repositories.",
    "parameters": null,
    "node_type": "class",
    "base_classes": null,
    "class_name": null,
    "display_name": "class DependencyParser",
    "component_id": "codewiki.src.be.dependency_analyzer.ast_parser.DependencyParser"
  },
  "codewiki.src.be.dependency_analyzer.dependency_graphs_builder.DependencyGraphBuilder": {
    "id": "codewiki.src.be.dependency_analyzer.dependency_graphs_builder.DependencyGraphBuilder",
    "name": "DependencyGraphBuilder",
    "component_type": "class",
    "file_path": "/Users/wufagang/project/aiopen/CodeWiki/codewiki/src/be/dependency_analyzer/dependency_graphs_builder.py",
    "relative_path": "codewiki/src/be/dependency_analyzer/dependency_graphs_builder.py",
    "depends_on": [
      "codewiki.src.be.dependency_analyzer.topo_sort.get_leaf_nodes",
      "codewiki.cli.utils.errors.warning",
      "codewiki.cli.utils.fs.ensure_directory",
      "codewiki.src.be.dependency_analyzer.topo_sort.build_graph_from_components",
      "codewiki.src.be.dependency_analyzer.ast_parser.DependencyParser"
    ],
    "source_code": "class DependencyGraphBuilder:\n    \"\"\"Handles dependency analysis and graph building.\"\"\"\n    \n    def __init__(self, config: Config):\n        self.config = config\n    \n    def build_dependency_graph(self) -> tuple[Dict[str, Any], List[str]]:\n        \"\"\"\n        Build and save dependency graph, returning components and leaf nodes.\n        \n        Returns:\n            Tuple of (components, leaf_nodes)\n        \"\"\"\n        # Ensure output directory exists\n        file_manager.ensure_directory(self.config.dependency_graph_dir)\n\n        # Prepare dependency graph path\n        repo_name = os.path.basename(os.path.normpath(self.config.repo_path))\n        sanitized_repo_name = ''.join(c if c.isalnum() else '_' for c in repo_name)\n        dependency_graph_path = os.path.join(\n            self.config.dependency_graph_dir, \n            f\"{sanitized_repo_name}_dependency_graph.json\"\n        )\n        filtered_folders_path = os.path.join(\n            self.config.dependency_graph_dir, \n            f\"{sanitized_repo_name}_filtered_folders.json\"\n        )\n\n        parser = DependencyParser(self.config.repo_path)\n\n        filtered_folders = None\n        # if os.path.exists(filtered_folders_path):\n        #     logger.debug(f\"Loading filtered folders from {filtered_folders_path}\")\n        #     filtered_folders = file_manager.load_json(filtered_folders_path)\n        # else:\n        #     # Parse repository\n        #     filtered_folders = parser.filter_folders()\n        #     # Save filtered folders\n        #     file_manager.save_json(filtered_folders, filtered_folders_path)\n\n        # Parse repository\n        components = parser.parse_repository(filtered_folders)\n        \n        # Save dependency graph\n        parser.save_dependency_graph(dependency_graph_path)\n        \n        # Build graph for traversal\n        graph = build_graph_from_components(components)\n        \n        # Get leaf nodes\n        leaf_nodes = get_leaf_nodes(graph, components)\n\n        # check if leaf_nodes are in components, only keep the ones that are in components\n        # and type is one of the following: class, interface, struct (or function for C-based projects)\n        \n        # Determine if we should include functions based on available component types\n        available_types = set()\n        for comp in components.values():\n            available_types.add(comp.component_type)\n        \n        # Valid types for leaf nodes - include functions for C-based codebases\n        valid_types = {\"class\", \"interface\", \"struct\"}\n        # If no classes/interfaces/structs are found, include functions\n        if not available_types.intersection(valid_types):\n            valid_types.add(\"function\")\n        \n        keep_leaf_nodes = []\n        for leaf_node in leaf_nodes:\n            # Skip any leaf nodes that are clearly error strings or invalid identifiers\n            if not isinstance(leaf_node, str) or leaf_node.strip() == \"\" or any(err_keyword in leaf_node.lower() for err_keyword in ['error', 'exception', 'failed', 'invalid']):\n                logger.warning(f\"Skipping invalid leaf node identifier: '{leaf_node}'\")\n                continue\n                \n            if leaf_node in components:\n                if components[leaf_node].component_type in valid_types:\n                    keep_leaf_nodes.append(leaf_node)\n                else:\n                    # logger.debug(f\"Leaf node {leaf_node} is a {components[leaf_node].component_type}, removing it\")\n                    pass\n            else:\n                logger.warning(f\"Leaf node {leaf_node} not found in components, removing it\")\n        \n        return components, keep_leaf_nodes",
    "start_line": 12,
    "end_line": 94,
    "has_docstring": true,
    "docstring": "Handles dependency analysis and graph building.",
    "parameters": null,
    "node_type": "class",
    "base_classes": null,
    "class_name": null,
    "display_name": "class DependencyGraphBuilder",
    "component_id": "codewiki.src.be.dependency_analyzer.dependency_graphs_builder.DependencyGraphBuilder"
  },
  "codewiki.src.be.dependency_analyzer.models.analysis.AnalysisResult": {
    "id": "codewiki.src.be.dependency_analyzer.models.analysis.AnalysisResult",
    "name": "AnalysisResult",
    "component_type": "class",
    "file_path": "/Users/wufagang/project/aiopen/CodeWiki/codewiki/src/be/dependency_analyzer/models/analysis.py",
    "relative_path": "codewiki/src/be/dependency_analyzer/models/analysis.py",
    "depends_on": [],
    "source_code": "class AnalysisResult(BaseModel):\n    \"\"\"Result of analyzing a repository\"\"\"\n\n    repository: Repository\n    functions: List[Node]\n    relationships: List[CallRelationship]\n    file_tree: Dict[str, Any]\n    summary: Dict[str, Any]\n    visualization: Dict[str, Any] = {}\n    readme_content: Optional[str] = None",
    "start_line": 6,
    "end_line": 15,
    "has_docstring": true,
    "docstring": "Result of analyzing a repository",
    "parameters": null,
    "node_type": "class",
    "base_classes": [
      "BaseModel"
    ],
    "class_name": null,
    "display_name": "class AnalysisResult",
    "component_id": "codewiki.src.be.dependency_analyzer.models.analysis.AnalysisResult"
  },
  "codewiki.src.be.dependency_analyzer.models.analysis.NodeSelection": {
    "id": "codewiki.src.be.dependency_analyzer.models.analysis.NodeSelection",
    "name": "NodeSelection",
    "component_type": "class",
    "file_path": "/Users/wufagang/project/aiopen/CodeWiki/codewiki/src/be/dependency_analyzer/models/analysis.py",
    "relative_path": "codewiki/src/be/dependency_analyzer/models/analysis.py",
    "depends_on": [],
    "source_code": "class NodeSelection(BaseModel):\n    \"\"\"Selected nodes for partial export\"\"\"\n\n    selected_nodes: List[str] = []\n    include_relationships: bool = True\n    custom_names: Dict[str, str] = {}",
    "start_line": 18,
    "end_line": 23,
    "has_docstring": true,
    "docstring": "Selected nodes for partial export",
    "parameters": null,
    "node_type": "class",
    "base_classes": [
      "BaseModel"
    ],
    "class_name": null,
    "display_name": "class NodeSelection",
    "component_id": "codewiki.src.be.dependency_analyzer.models.analysis.NodeSelection"
  },
  "codewiki.src.be.dependency_analyzer.models.core.Node": {
    "id": "codewiki.src.be.dependency_analyzer.models.core.Node",
    "name": "Node",
    "component_type": "class",
    "file_path": "/Users/wufagang/project/aiopen/CodeWiki/codewiki/src/be/dependency_analyzer/models/core.py",
    "relative_path": "codewiki/src/be/dependency_analyzer/models/core.py",
    "depends_on": [],
    "source_code": "class Node(BaseModel):\n    id: str\n\n    name: str\n    \n    component_type: str\n    \n    file_path: str\n    \n    relative_path: str\n    \n    depends_on: Set[str] = set()\n    \n    source_code: Optional[str] = None\n    \n    start_line: int = 0\n\n    end_line: int = 0\n    \n    has_docstring: bool = False\n    \n    docstring: str = \"\"\n    \n    parameters: Optional[List[str]] = None\n\n    node_type: Optional[str] = None  \n\n    base_classes: Optional[List[str]] = None\n\n    class_name: Optional[str] = None\n\n    display_name: Optional[str] = None\n\n    component_id: Optional[str] = None\n\n    def get_display_name(self) -> str:\n        return self.display_name or self.name",
    "start_line": 7,
    "end_line": 43,
    "has_docstring": false,
    "docstring": "",
    "parameters": null,
    "node_type": "class",
    "base_classes": [
      "BaseModel"
    ],
    "class_name": null,
    "display_name": "class Node",
    "component_id": "codewiki.src.be.dependency_analyzer.models.core.Node"
  },
  "codewiki.src.be.dependency_analyzer.models.core.CallRelationship": {
    "id": "codewiki.src.be.dependency_analyzer.models.core.CallRelationship",
    "name": "CallRelationship",
    "component_type": "class",
    "file_path": "/Users/wufagang/project/aiopen/CodeWiki/codewiki/src/be/dependency_analyzer/models/core.py",
    "relative_path": "codewiki/src/be/dependency_analyzer/models/core.py",
    "depends_on": [],
    "source_code": "class CallRelationship(BaseModel):\n    caller: str\n\n    callee: str\n\n    call_line: Optional[int] = None\n\n    is_resolved: bool = False",
    "start_line": 46,
    "end_line": 53,
    "has_docstring": false,
    "docstring": "",
    "parameters": null,
    "node_type": "class",
    "base_classes": [
      "BaseModel"
    ],
    "class_name": null,
    "display_name": "class CallRelationship",
    "component_id": "codewiki.src.be.dependency_analyzer.models.core.CallRelationship"
  },
  "codewiki.src.be.dependency_analyzer.models.core.Repository": {
    "id": "codewiki.src.be.dependency_analyzer.models.core.Repository",
    "name": "Repository",
    "component_type": "class",
    "file_path": "/Users/wufagang/project/aiopen/CodeWiki/codewiki/src/be/dependency_analyzer/models/core.py",
    "relative_path": "codewiki/src/be/dependency_analyzer/models/core.py",
    "depends_on": [],
    "source_code": "class Repository(BaseModel):\n    url: str\n\n    name: str\n\n    clone_path: str\n    \n    analysis_id: str",
    "start_line": 56,
    "end_line": 63,
    "has_docstring": false,
    "docstring": "",
    "parameters": null,
    "node_type": "class",
    "base_classes": [
      "BaseModel"
    ],
    "class_name": null,
    "display_name": "class Repository",
    "component_id": "codewiki.src.be.dependency_analyzer.models.core.Repository"
  },
  "codewiki.src.be.dependency_analyzer.topo_sort.detect_cycles": {
    "id": "codewiki.src.be.dependency_analyzer.topo_sort.detect_cycles",
    "name": "detect_cycles",
    "component_type": "function",
    "file_path": "/Users/wufagang/project/aiopen/CodeWiki/codewiki/src/be/dependency_analyzer/topo_sort.py",
    "relative_path": "codewiki/src/be/dependency_analyzer/topo_sort.py",
    "depends_on": [],
    "source_code": "def detect_cycles(graph: Dict[str, Set[str]]) -> List[List[str]]:\n    \"\"\"\n    Detect cycles in a dependency graph using Tarjan's algorithm to find\n    strongly connected components.\n    \n    Args:\n        graph: A dependency graph represented as adjacency lists\n               (node -> set of dependencies)\n    \n    Returns:\n        A list of lists, where each inner list contains the nodes in a cycle\n    \"\"\"\n    # Implementation of Tarjan's algorithm\n    index_counter = [0]\n    index = {}  # node -> index\n    lowlink = {}  # node -> lowlink value\n    onstack = set()  # nodes currently on the stack\n    stack = []  # stack of nodes\n    result = []  # list of cycles (strongly connected components)\n    \n    def strongconnect(node):\n        # Set the depth index for node\n        index[node] = index_counter[0]\n        lowlink[node] = index_counter[0]\n        index_counter[0] += 1\n        stack.append(node)\n        onstack.add(node)\n        \n        # Consider successors\n        for successor in graph.get(node, set()):\n            if successor not in index:\n                # Successor has not yet been visited; recurse on it\n                strongconnect(successor)\n                lowlink[node] = min(lowlink[node], lowlink[successor])\n            elif successor in onstack:\n                # Successor is on the stack and hence in the current SCC\n                lowlink[node] = min(lowlink[node], index[successor])\n        \n        # If node is a root node, pop the stack and generate an SCC\n        if lowlink[node] == index[node]:\n            # Start a new strongly connected component\n            scc = []\n            while True:\n                successor = stack.pop()\n                onstack.remove(successor)\n                scc.append(successor)\n                if successor == node:\n                    break\n            \n            # Only include SCCs with more than one node (actual cycles)\n            if len(scc) > 1:\n                result.append(scc)\n    \n    # Visit each node\n    for node in graph:\n        if node not in index:\n            strongconnect(node)\n    \n    return result",
    "start_line": 18,
    "end_line": 76,
    "has_docstring": true,
    "docstring": "Detect cycles in a dependency graph using Tarjan's algorithm to find\nstrongly connected components.\n\nArgs:\n    graph: A dependency graph represented as adjacency lists\n           (node -> set of dependencies)\n\nReturns:\n    A list of lists, where each inner list contains the nodes in a cycle",
    "parameters": [
      "graph"
    ],
    "node_type": "function",
    "base_classes": null,
    "class_name": null,
    "display_name": "function detect_cycles",
    "component_id": "codewiki.src.be.dependency_analyzer.topo_sort.detect_cycles"
  },
  "codewiki.src.be.dependency_analyzer.topo_sort.strongconnect": {
    "id": "codewiki.src.be.dependency_analyzer.topo_sort.strongconnect",
    "name": "strongconnect",
    "component_type": "function",
    "file_path": "/Users/wufagang/project/aiopen/CodeWiki/codewiki/src/be/dependency_analyzer/topo_sort.py",
    "relative_path": "codewiki/src/be/dependency_analyzer/topo_sort.py",
    "depends_on": [
      "codewiki.src.be.dependency_analyzer.topo_sort.strongconnect"
    ],
    "source_code": "    def strongconnect(node):\n        # Set the depth index for node\n        index[node] = index_counter[0]\n        lowlink[node] = index_counter[0]\n        index_counter[0] += 1\n        stack.append(node)\n        onstack.add(node)\n        \n        # Consider successors\n        for successor in graph.get(node, set()):\n            if successor not in index:\n                # Successor has not yet been visited; recurse on it\n                strongconnect(successor)\n                lowlink[node] = min(lowlink[node], lowlink[successor])\n            elif successor in onstack:\n                # Successor is on the stack and hence in the current SCC\n                lowlink[node] = min(lowlink[node], index[successor])\n        \n        # If node is a root node, pop the stack and generate an SCC\n        if lowlink[node] == index[node]:\n            # Start a new strongly connected component\n            scc = []\n            while True:\n                successor = stack.pop()\n                onstack.remove(successor)\n                scc.append(successor)\n                if successor == node:\n                    break\n            \n            # Only include SCCs with more than one node (actual cycles)\n            if len(scc) > 1:\n                result.append(scc)",
    "start_line": 38,
    "end_line": 69,
    "has_docstring": false,
    "docstring": "",
    "parameters": [
      "node"
    ],
    "node_type": "function",
    "base_classes": null,
    "class_name": null,
    "display_name": "function strongconnect",
    "component_id": "codewiki.src.be.dependency_analyzer.topo_sort.strongconnect"
  },
  "codewiki.src.be.dependency_analyzer.topo_sort.resolve_cycles": {
    "id": "codewiki.src.be.dependency_analyzer.topo_sort.resolve_cycles",
    "name": "resolve_cycles",
    "component_type": "function",
    "file_path": "/Users/wufagang/project/aiopen/CodeWiki/codewiki/src/be/dependency_analyzer/topo_sort.py",
    "relative_path": "codewiki/src/be/dependency_analyzer/topo_sort.py",
    "depends_on": [
      "codewiki.src.be.dependency_analyzer.topo_sort.detect_cycles"
    ],
    "source_code": "def resolve_cycles(graph: Dict[str, Set[str]]) -> Dict[str, Set[str]]:\n    \"\"\"\n    Resolve cycles in a dependency graph by identifying strongly connected\n    components and breaking cycles.\n    \n    Args:\n        graph: A dependency graph represented as adjacency lists\n               (node -> set of dependencies)\n    \n    Returns:\n        A new acyclic graph with the same nodes but with cycles broken\n    \"\"\"\n    # Detect cycles (SCCs)\n    cycles = detect_cycles(graph)\n    \n    if not cycles:\n        logger.debug(\"No cycles detected in the dependency graph\")\n        return graph\n    \n    logger.debug(f\"Detected {len(cycles)} cycles in the dependency graph\")\n    \n    # Create a copy of the graph to modify\n    new_graph = {node: deps.copy() for node, deps in graph.items()}\n    \n    # Process each cycle\n    for i, cycle in enumerate(cycles):\n        logger.debug(f\"Cycle {i+1}: {' -> '.join(cycle)}\")\n        \n        # Strategy: Break the cycle by removing the \"weakest\" dependency\n        # Here, we just arbitrarily remove the last edge to make the graph acyclic\n        # In a real-world scenario, you might use heuristics to determine which edge to break\n        # For example, removing edges between different modules before edges within the same module\n        for j in range(len(cycle) - 1):\n            current = cycle[j]\n            next_node = cycle[j + 1]\n            \n            if next_node in new_graph[current]:\n                logger.debug(f\"Breaking cycle by removing dependency: {current} -> {next_node}\")\n                new_graph[current].remove(next_node)\n                break\n    \n    return new_graph",
    "start_line": 78,
    "end_line": 119,
    "has_docstring": true,
    "docstring": "Resolve cycles in a dependency graph by identifying strongly connected\ncomponents and breaking cycles.\n\nArgs:\n    graph: A dependency graph represented as adjacency lists\n           (node -> set of dependencies)\n\nReturns:\n    A new acyclic graph with the same nodes but with cycles broken",
    "parameters": [
      "graph"
    ],
    "node_type": "function",
    "base_classes": null,
    "class_name": null,
    "display_name": "function resolve_cycles",
    "component_id": "codewiki.src.be.dependency_analyzer.topo_sort.resolve_cycles"
  },
  "codewiki.src.be.dependency_analyzer.topo_sort.topological_sort": {
    "id": "codewiki.src.be.dependency_analyzer.topo_sort.topological_sort",
    "name": "topological_sort",
    "component_type": "function",
    "file_path": "/Users/wufagang/project/aiopen/CodeWiki/codewiki/src/be/dependency_analyzer/topo_sort.py",
    "relative_path": "codewiki/src/be/dependency_analyzer/topo_sort.py",
    "depends_on": [
      "codewiki.src.be.dependency_analyzer.topo_sort.resolve_cycles",
      "codewiki.cli.utils.errors.warning"
    ],
    "source_code": "def topological_sort(graph: Dict[str, Set[str]]) -> List[str]:\n    \"\"\"\n    Perform a topological sort on a dependency graph.\n    \n    Args:\n        graph: A dependency graph represented as adjacency lists\n               (node -> set of dependencies)\n    \n    Returns:\n        A list of nodes in topological order (dependencies first)\n    \"\"\"\n    # First, check for and resolve cycles\n    acyclic_graph = resolve_cycles(graph)\n    \n    # Initialize in-degree counter for all nodes\n    in_degree = {node: 0 for node in acyclic_graph}\n    \n    # Count in-degrees\n    for node, dependencies in acyclic_graph.items():\n        for dep in dependencies:\n            if dep in in_degree:\n                in_degree[dep] += 1\n    \n    # Queue of nodes with no dependencies (in-degree of 0)\n    queue = deque([node for node, degree in in_degree.items() if degree == 0])\n    \n    # Result list to store the topological order\n    result = []\n    \n    # Process nodes in topological order\n    while queue:\n        node = queue.popleft()\n        result.append(node)\n        \n        # Reduce in-degree for each node that depends on the current node\n        for dependent, deps in acyclic_graph.items():\n            if node in deps:\n                in_degree[dependent] -= 1\n                if in_degree[dependent] == 0:\n                    queue.append(dependent)\n    \n    # Check if the sort was successful (all nodes included)\n    if len(result) != len(acyclic_graph):\n        logger.warning(\"Topological sort failed: graph has cycles that weren't resolved\")\n        # Return all nodes in some order to avoid breaking the process\n        return list(acyclic_graph.keys())\n    \n    # Reverse the result to get dependencies first\n    return result[::-1]",
    "start_line": 121,
    "end_line": 169,
    "has_docstring": true,
    "docstring": "Perform a topological sort on a dependency graph.\n\nArgs:\n    graph: A dependency graph represented as adjacency lists\n           (node -> set of dependencies)\n\nReturns:\n    A list of nodes in topological order (dependencies first)",
    "parameters": [
      "graph"
    ],
    "node_type": "function",
    "base_classes": null,
    "class_name": null,
    "display_name": "function topological_sort",
    "component_id": "codewiki.src.be.dependency_analyzer.topo_sort.topological_sort"
  },
  "codewiki.src.be.dependency_analyzer.topo_sort.dependency_first_dfs": {
    "id": "codewiki.src.be.dependency_analyzer.topo_sort.dependency_first_dfs",
    "name": "dependency_first_dfs",
    "component_type": "function",
    "file_path": "/Users/wufagang/project/aiopen/CodeWiki/codewiki/src/be/dependency_analyzer/topo_sort.py",
    "relative_path": "codewiki/src/be/dependency_analyzer/topo_sort.py",
    "depends_on": [
      "codewiki.src.be.dependency_analyzer.topo_sort.resolve_cycles",
      "codewiki.cli.utils.errors.warning"
    ],
    "source_code": "def dependency_first_dfs(graph: Dict[str, Set[str]]) -> List[str]:\n    \"\"\"\n    Perform a depth-first traversal of the dependency graph, starting from root nodes\n    that have no dependencies.\n    \n    The graph uses natural dependency direction:\n    - If A depends on B, the graph has an edge A → B\n    - This means an edge from X to Y represents \"X depends on Y\"\n    - Root nodes (nodes with no incoming edges/dependencies) are processed first,\n      followed by nodes that depend on them\n    \n    Args:\n        graph: A dependency graph with natural direction (A→B if A depends on B)\n    \n    Returns:\n        A list of nodes in an order where dependencies come before their dependents\n    \"\"\"\n    # First, resolve cycles to ensure we have a DAG\n    acyclic_graph = resolve_cycles(graph)\n    \n    # Find root nodes (nodes with no dependencies)\n    root_nodes = []\n    # Create a reverse graph to easily check if a node has incoming edges\n    has_incoming_edge = {node: False for node in acyclic_graph}\n    \n    for node, deps in acyclic_graph.items():\n        for dep in deps:\n            has_incoming_edge[dep] = True\n    \n    # Nodes with no incoming edges are root nodes\n    for node in acyclic_graph:\n        if not has_incoming_edge.get(node, False) and node in acyclic_graph:\n            root_nodes.append(node)\n    \n    if not root_nodes:\n        logger.warning(\"No root nodes found in the graph, using arbitrary starting point\")\n        root_nodes = list(acyclic_graph.keys())[:1]  # Use the first node as starting point\n    \n    # Track visited nodes\n    visited = set()\n    result = []\n    \n    # DFS function that processes dependencies first\n    def dfs(node):\n        if node in visited:\n            return\n        visited.add(node)\n        \n        # Visit all dependencies first\n        for dep in sorted(acyclic_graph.get(node, set())):\n            dfs(dep)\n        \n        # Add this node to the result after all its dependencies\n        result.append(node)\n    \n    # Start DFS from each root node\n    for root in sorted(root_nodes):\n        dfs(root)\n    \n    # Check if all nodes were visited\n    if len(result) != len(acyclic_graph):\n        # Some nodes weren't visited - try to visit remaining nodes\n        for node in sorted(acyclic_graph.keys()):\n            if node not in visited:\n                dfs(node)\n    \n    return result",
    "start_line": 171,
    "end_line": 237,
    "has_docstring": true,
    "docstring": "Perform a depth-first traversal of the dependency graph, starting from root nodes\nthat have no dependencies.\n\nThe graph uses natural dependency direction:\n- If A depends on B, the graph has an edge A → B\n- This means an edge from X to Y represents \"X depends on Y\"\n- Root nodes (nodes with no incoming edges/dependencies) are processed first,\n  followed by nodes that depend on them\n\nArgs:\n    graph: A dependency graph with natural direction (A→B if A depends on B)\n\nReturns:\n    A list of nodes in an order where dependencies come before their dependents",
    "parameters": [
      "graph"
    ],
    "node_type": "function",
    "base_classes": null,
    "class_name": null,
    "display_name": "function dependency_first_dfs",
    "component_id": "codewiki.src.be.dependency_analyzer.topo_sort.dependency_first_dfs"
  },
  "codewiki.src.be.dependency_analyzer.topo_sort.dfs": {
    "id": "codewiki.src.be.dependency_analyzer.topo_sort.dfs",
    "name": "dfs",
    "component_type": "function",
    "file_path": "/Users/wufagang/project/aiopen/CodeWiki/codewiki/src/be/dependency_analyzer/topo_sort.py",
    "relative_path": "codewiki/src/be/dependency_analyzer/topo_sort.py",
    "depends_on": [
      "codewiki.src.be.dependency_analyzer.topo_sort.dfs"
    ],
    "source_code": "    def dfs(node):\n        if node in visited:\n            return\n        visited.add(node)\n        \n        # Visit all dependencies first\n        for dep in sorted(acyclic_graph.get(node, set())):\n            dfs(dep)\n        \n        # Add this node to the result after all its dependencies\n        result.append(node)",
    "start_line": 214,
    "end_line": 224,
    "has_docstring": false,
    "docstring": "",
    "parameters": [
      "node"
    ],
    "node_type": "function",
    "base_classes": null,
    "class_name": null,
    "display_name": "function dfs",
    "component_id": "codewiki.src.be.dependency_analyzer.topo_sort.dfs"
  },
  "codewiki.src.be.dependency_analyzer.topo_sort.build_graph_from_components": {
    "id": "codewiki.src.be.dependency_analyzer.topo_sort.build_graph_from_components",
    "name": "build_graph_from_components",
    "component_type": "function",
    "file_path": "/Users/wufagang/project/aiopen/CodeWiki/codewiki/src/be/dependency_analyzer/topo_sort.py",
    "relative_path": "codewiki/src/be/dependency_analyzer/topo_sort.py",
    "depends_on": [],
    "source_code": "def build_graph_from_components(components: Dict[str, Any]) -> Dict[str, Set[str]]:\n    \"\"\"\n    Build a dependency graph from a collection of code components.\n    \n    The graph uses the natural dependency direction:\n    - If A depends on B, we create an edge A → B\n    - This means an edge from node X to node Y represents \"X depends on Y\"\n    - Root nodes (nodes with no dependencies) are components that don't depend on anything\n    \n    Args:\n        components: A dictionary of code components, where each component\n                   has a 'depends_on' attribute\n    \n    Returns:\n        A dependency graph with natural dependency direction\n    \"\"\"\n    graph = {}\n    \n    for comp_id, component in components.items():\n        # Initialize the node's adjacency list\n        if comp_id not in graph:\n            graph[comp_id] = set()\n        \n        # Add dependencies\n        for dep_id in component.depends_on:\n            # Only include dependencies that are actual components in our repository\n            if dep_id in components:\n                graph[comp_id].add(dep_id)\n    \n    return graph ",
    "start_line": 239,
    "end_line": 268,
    "has_docstring": true,
    "docstring": "Build a dependency graph from a collection of code components.\n\nThe graph uses the natural dependency direction:\n- If A depends on B, we create an edge A → B\n- This means an edge from node X to node Y represents \"X depends on Y\"\n- Root nodes (nodes with no dependencies) are components that don't depend on anything\n\nArgs:\n    components: A dictionary of code components, where each component\n               has a 'depends_on' attribute\n\nReturns:\n    A dependency graph with natural dependency direction",
    "parameters": [
      "components"
    ],
    "node_type": "function",
    "base_classes": null,
    "class_name": null,
    "display_name": "function build_graph_from_components",
    "component_id": "codewiki.src.be.dependency_analyzer.topo_sort.build_graph_from_components"
  },
  "codewiki.src.be.dependency_analyzer.topo_sort.get_leaf_nodes": {
    "id": "codewiki.src.be.dependency_analyzer.topo_sort.get_leaf_nodes",
    "name": "get_leaf_nodes",
    "component_type": "function",
    "file_path": "/Users/wufagang/project/aiopen/CodeWiki/codewiki/src/be/dependency_analyzer/topo_sort.py",
    "relative_path": "codewiki/src/be/dependency_analyzer/topo_sort.py",
    "depends_on": [
      "codewiki.src.be.dependency_analyzer.topo_sort.resolve_cycles"
    ],
    "source_code": "def get_leaf_nodes(graph: Dict[str, Set[str]], components: Dict[str, Node]) -> List[str]:\n    \"\"\"\n    Find leaf nodes (nodes that no other nodes depend on) and build dependency trees\n    showing the full dependency chain from each leaf back to the ultimate dependencies.\n    \n    The graph uses natural dependency direction:\n    - If A depends on B, the graph has an edge A → B\n    - Leaf nodes are nodes that appear in no other node's dependency set\n    - Each tree shows the dependency chain: leaf → its dependencies → their dependencies, etc.\n    \n    Args:\n        graph: A dependency graph with natural direction (A→B if A depends on B)\n    \n    Returns:\n        A list of leaf nodes\n    \"\"\"\n    # First, resolve cycles to ensure we have a DAG\n    acyclic_graph = resolve_cycles(graph)\n    \n    # Find leaf nodes (nodes that no other nodes depend on)\n    leaf_nodes = set(acyclic_graph.keys())\n\n    \n    \n    def concise_node(leaf_nodes: Set[str]) -> Set[str]:\n        concise_leaf_nodes = set()\n        for node in leaf_nodes:\n            if node.endswith(\"__init__\"):\n                # replace by class name\n                concise_leaf_nodes.add(node.replace(\".__init__\", \"\"))\n            else:\n                concise_leaf_nodes.add(node)\n        \n        keep_leaf_nodes = []\n        \n        # Determine if we should include functions based on available component types\n        # For C-based projects, we need to include functions since they don't have classes\n        available_types = set()\n        for comp in components.values():\n            available_types.add(comp.component_type)\n        \n        # Valid types for leaf nodes - include functions for C-based codebases\n        valid_types = {\"class\", \"interface\", \"struct\"}\n        # If no classes/interfaces/structs are found, include functions\n        if not available_types.intersection(valid_types):\n            valid_types.add(\"function\")\n\n        for leaf_node in leaf_nodes:\n            # Skip any leaf nodes that are clearly error strings or invalid identifiers\n            if not isinstance(leaf_node, str) or leaf_node.strip() == \"\" or any(err_keyword in leaf_node.lower() for err_keyword in ['error', 'exception', 'failed', 'invalid']):\n                logger.debug(f\"Skipping invalid leaf node identifier: '{leaf_node}'\")\n                continue\n                \n            if leaf_node in components:\n                if components[leaf_node].component_type in valid_types:\n                    keep_leaf_nodes.append(leaf_node)\n                else:\n                    # logger.debug(f\"Leaf node {leaf_node} is a {components[leaf_node].component_type}, removing it\")\n                    pass\n            else:\n                # logger.debug(f\"Leaf node {leaf_node} not found in components, removing it\")\n                pass\n\n        return keep_leaf_nodes\n\n    concise_leaf_nodes = concise_node(leaf_nodes)\n    if len(concise_leaf_nodes) >= 400:\n        logger.debug(f\"Leaf nodes are too many ({len(concise_leaf_nodes)}), removing dependencies of other nodes\")\n        # Remove nodes that are dependencies of other nodes\n        for node, deps in acyclic_graph.items():\n            for dep in deps:\n                leaf_nodes.discard(dep)\n        \n        concise_leaf_nodes = concise_node(leaf_nodes)\n    \n    if not leaf_nodes:\n        logger.warning(\"No leaf nodes found in the graph\")\n        return []\n    \n    return concise_leaf_nodes ",
    "start_line": 271,
    "end_line": 350,
    "has_docstring": true,
    "docstring": "Find leaf nodes (nodes that no other nodes depend on) and build dependency trees\nshowing the full dependency chain from each leaf back to the ultimate dependencies.\n\nThe graph uses natural dependency direction:\n- If A depends on B, the graph has an edge A → B\n- Leaf nodes are nodes that appear in no other node's dependency set\n- Each tree shows the dependency chain: leaf → its dependencies → their dependencies, etc.\n\nArgs:\n    graph: A dependency graph with natural direction (A→B if A depends on B)\n\nReturns:\n    A list of leaf nodes",
    "parameters": [
      "graph",
      "components"
    ],
    "node_type": "function",
    "base_classes": null,
    "class_name": null,
    "display_name": "function get_leaf_nodes",
    "component_id": "codewiki.src.be.dependency_analyzer.topo_sort.get_leaf_nodes"
  },
  "codewiki.src.be.dependency_analyzer.topo_sort.concise_node": {
    "id": "codewiki.src.be.dependency_analyzer.topo_sort.concise_node",
    "name": "concise_node",
    "component_type": "function",
    "file_path": "/Users/wufagang/project/aiopen/CodeWiki/codewiki/src/be/dependency_analyzer/topo_sort.py",
    "relative_path": "codewiki/src/be/dependency_analyzer/topo_sort.py",
    "depends_on": [],
    "source_code": "    def concise_node(leaf_nodes: Set[str]) -> Set[str]:\n        concise_leaf_nodes = set()\n        for node in leaf_nodes:\n            if node.endswith(\"__init__\"):\n                # replace by class name\n                concise_leaf_nodes.add(node.replace(\".__init__\", \"\"))\n            else:\n                concise_leaf_nodes.add(node)\n        \n        keep_leaf_nodes = []\n        \n        # Determine if we should include functions based on available component types\n        # For C-based projects, we need to include functions since they don't have classes\n        available_types = set()\n        for comp in components.values():\n            available_types.add(comp.component_type)\n        \n        # Valid types for leaf nodes - include functions for C-based codebases\n        valid_types = {\"class\", \"interface\", \"struct\"}\n        # If no classes/interfaces/structs are found, include functions\n        if not available_types.intersection(valid_types):\n            valid_types.add(\"function\")\n\n        for leaf_node in leaf_nodes:\n            # Skip any leaf nodes that are clearly error strings or invalid identifiers\n            if not isinstance(leaf_node, str) or leaf_node.strip() == \"\" or any(err_keyword in leaf_node.lower() for err_keyword in ['error', 'exception', 'failed', 'invalid']):\n                logger.debug(f\"Skipping invalid leaf node identifier: '{leaf_node}'\")\n                continue\n                \n            if leaf_node in components:\n                if components[leaf_node].component_type in valid_types:\n                    keep_leaf_nodes.append(leaf_node)\n                else:\n                    # logger.debug(f\"Leaf node {leaf_node} is a {components[leaf_node].component_type}, removing it\")\n                    pass\n            else:\n                # logger.debug(f\"Leaf node {leaf_node} not found in components, removing it\")\n                pass\n\n        return keep_leaf_nodes",
    "start_line": 295,
    "end_line": 334,
    "has_docstring": false,
    "docstring": "",
    "parameters": [
      "leaf_nodes"
    ],
    "node_type": "function",
    "base_classes": null,
    "class_name": null,
    "display_name": "function concise_node",
    "component_id": "codewiki.src.be.dependency_analyzer.topo_sort.concise_node"
  },
  "codewiki.src.be.dependency_analyzer.utils.logging_config.ColoredFormatter": {
    "id": "codewiki.src.be.dependency_analyzer.utils.logging_config.ColoredFormatter",
    "name": "ColoredFormatter",
    "component_type": "class",
    "file_path": "/Users/wufagang/project/aiopen/CodeWiki/codewiki/src/be/dependency_analyzer/utils/logging_config.py",
    "relative_path": "codewiki/src/be/dependency_analyzer/utils/logging_config.py",
    "depends_on": [],
    "source_code": "class ColoredFormatter(logging.Formatter):\n    \"\"\"Custom formatter with colored output for better readability.\n    \n    This formatter adds colors to different log levels and components:\n    - Log levels are colored based on severity\n    - Timestamps are shown in blue\n    - Module names are shown in magenta\n    - Messages are shown in the default terminal color\n    \"\"\"\n    \n    # Define colors for different log levels\n    COLORS = {\n        'DEBUG': Fore.BLUE,\n        'INFO': Fore.CYAN,\n        'WARNING': Fore.YELLOW,\n        'ERROR': Fore.RED,\n        'CRITICAL': Fore.RED + Style.BRIGHT,\n    }\n    \n    # Define colors for different components\n    COMPONENT_COLORS = {\n        'timestamp': Fore.BLUE,\n        'module': Fore.MAGENTA,\n        'reset': Style.RESET_ALL,\n    }\n    \n    def format(self, record):\n        \"\"\"Format log record with colors.\"\"\"\n        # Get the color for this log level\n        level_color = self.COLORS.get(record.levelname, '')\n        \n        # Format timestamp\n        timestamp = self.formatTime(record, '%H:%M:%S')\n        colored_timestamp = f\"{self.COMPONENT_COLORS['timestamp']}[{timestamp}]{self.COMPONENT_COLORS['reset']}\"\n        \n        # Format log level with color\n        colored_level = f\"{level_color}{record.levelname:8}{self.COMPONENT_COLORS['reset']}\"\n        \n        # Format the message with the same color as the log level\n        message = record.getMessage()\n        colored_message = f\"{level_color}{message}{self.COMPONENT_COLORS['reset']}\"\n        \n        # Combine all parts (without module name column)\n        log_line = f\"{colored_timestamp} {colored_level} {colored_message}\"\n        \n        # Handle exceptions\n        if record.exc_info:\n            log_line += \"\\n\" + self.formatException(record.exc_info)\n        \n        return log_line",
    "start_line": 35,
    "end_line": 84,
    "has_docstring": true,
    "docstring": "Custom formatter with colored output for better readability.\n\nThis formatter adds colors to different log levels and components:\n- Log levels are colored based on severity\n- Timestamps are shown in blue\n- Module names are shown in magenta\n- Messages are shown in the default terminal color",
    "parameters": null,
    "node_type": "class",
    "base_classes": [
      "logging.Formatter"
    ],
    "class_name": null,
    "display_name": "class ColoredFormatter",
    "component_id": "codewiki.src.be.dependency_analyzer.utils.logging_config.ColoredFormatter"
  },
  "codewiki.src.be.dependency_analyzer.utils.logging_config.setup_logging": {
    "id": "codewiki.src.be.dependency_analyzer.utils.logging_config.setup_logging",
    "name": "setup_logging",
    "component_type": "function",
    "file_path": "/Users/wufagang/project/aiopen/CodeWiki/codewiki/src/be/dependency_analyzer/utils/logging_config.py",
    "relative_path": "codewiki/src/be/dependency_analyzer/utils/logging_config.py",
    "depends_on": [
      "codewiki.src.be.dependency_analyzer.utils.logging_config.ColoredFormatter"
    ],
    "source_code": "def setup_logging(level=logging.INFO):\n    \"\"\"\n    Set up logging configuration with colored output.\n    \n    Args:\n        level: Logging level (default: logging.INFO)\n    \"\"\"\n    # Create console handler\n    console_handler = logging.StreamHandler(sys.stdout)\n    console_handler.setLevel(level)\n    \n    # Set colored formatter\n    colored_formatter = ColoredFormatter()\n    console_handler.setFormatter(colored_formatter)\n    \n    # Configure root logger\n    root_logger = logging.getLogger()\n    root_logger.setLevel(level)\n    \n    # Remove existing handlers to avoid duplicates\n    root_logger.handlers.clear()\n    \n    # Add our console handler\n    root_logger.addHandler(console_handler)",
    "start_line": 87,
    "end_line": 110,
    "has_docstring": true,
    "docstring": "Set up logging configuration with colored output.\n\nArgs:\n    level: Logging level (default: logging.INFO)",
    "parameters": [
      "level"
    ],
    "node_type": "function",
    "base_classes": null,
    "class_name": null,
    "display_name": "function setup_logging",
    "component_id": "codewiki.src.be.dependency_analyzer.utils.logging_config.setup_logging"
  },
  "codewiki.src.be.dependency_analyzer.utils.logging_config.setup_module_logging": {
    "id": "codewiki.src.be.dependency_analyzer.utils.logging_config.setup_module_logging",
    "name": "setup_module_logging",
    "component_type": "function",
    "file_path": "/Users/wufagang/project/aiopen/CodeWiki/codewiki/src/be/dependency_analyzer/utils/logging_config.py",
    "relative_path": "codewiki/src/be/dependency_analyzer/utils/logging_config.py",
    "depends_on": [
      "codewiki.src.be.dependency_analyzer.utils.logging_config.ColoredFormatter"
    ],
    "source_code": "def setup_module_logging(module_name: str, level=logging.INFO):\n    \"\"\"\n    Set up logging for a specific module with colored output.\n    \n    Args:\n        module_name: Name of the module to configure logging for\n        level: Logging level (default: logging.INFO)\n    \"\"\"\n    logger = logging.getLogger(module_name)\n    logger.setLevel(level)\n    \n    # Create console handler\n    console_handler = logging.StreamHandler(sys.stdout)\n    console_handler.setLevel(level)\n    \n    # Set colored formatter\n    colored_formatter = ColoredFormatter()\n    console_handler.setFormatter(colored_formatter)\n    \n    # Remove existing handlers\n    logger.handlers.clear()\n    \n    # Add console handler\n    logger.addHandler(console_handler)\n    \n    # Prevent propagation to avoid duplicate logs\n    logger.propagate = False\n    \n    return logger",
    "start_line": 113,
    "end_line": 141,
    "has_docstring": true,
    "docstring": "Set up logging for a specific module with colored output.\n\nArgs:\n    module_name: Name of the module to configure logging for\n    level: Logging level (default: logging.INFO)",
    "parameters": [
      "module_name",
      "level"
    ],
    "node_type": "function",
    "base_classes": null,
    "class_name": null,
    "display_name": "function setup_module_logging",
    "component_id": "codewiki.src.be.dependency_analyzer.utils.logging_config.setup_module_logging"
  },
  "codewiki.src.be.dependency_analyzer.utils.patterns.get_function_patterns_for_language": {
    "id": "codewiki.src.be.dependency_analyzer.utils.patterns.get_function_patterns_for_language",
    "name": "get_function_patterns_for_language",
    "component_type": "function",
    "file_path": "/Users/wufagang/project/aiopen/CodeWiki/codewiki/src/be/dependency_analyzer/utils/patterns.py",
    "relative_path": "codewiki/src/be/dependency_analyzer/utils/patterns.py",
    "depends_on": [],
    "source_code": "def get_function_patterns_for_language(language: str) -> list:\n    \"\"\"\n    Get function definition patterns for a specific language.\n\n    Args:\n        language: Programming language name\n\n    Returns:\n        List of function definition patterns for the language\n    \"\"\"\n    return FUNCTION_DEFINITION_PATTERNS.get(\n        language.lower(), FUNCTION_DEFINITION_PATTERNS[\"general\"]\n    )",
    "start_line": 447,
    "end_line": 459,
    "has_docstring": true,
    "docstring": "Get function definition patterns for a specific language.\n\nArgs:\n    language: Programming language name\n\nReturns:\n    List of function definition patterns for the language",
    "parameters": [
      "language"
    ],
    "node_type": "function",
    "base_classes": null,
    "class_name": null,
    "display_name": "function get_function_patterns_for_language",
    "component_id": "codewiki.src.be.dependency_analyzer.utils.patterns.get_function_patterns_for_language"
  },
  "codewiki.src.be.dependency_analyzer.utils.patterns.is_entry_point_file": {
    "id": "codewiki.src.be.dependency_analyzer.utils.patterns.is_entry_point_file",
    "name": "is_entry_point_file",
    "component_type": "function",
    "file_path": "/Users/wufagang/project/aiopen/CodeWiki/codewiki/src/be/dependency_analyzer/utils/patterns.py",
    "relative_path": "codewiki/src/be/dependency_analyzer/utils/patterns.py",
    "depends_on": [],
    "source_code": "def is_entry_point_file(filename: str) -> bool:\n    \"\"\"\n    Check if a filename matches entry point patterns.\n\n    Args:\n        filename: Name of the file to check\n\n    Returns:\n        True if the file is likely an entry point\n    \"\"\"\n    filename_lower = filename.lower()\n\n    # Exact match\n    if filename_lower in ENTRY_POINT_PATTERNS:\n        return True\n\n    # Partial name matching for flexibility\n    for pattern in ENTRY_POINT_NAME_PATTERNS:\n        if pattern in filename_lower and any(\n            ext in filename_lower for ext in [\".py\", \".js\", \".ts\", \".go\", \".rs\", \".c\", \".cpp\"]\n        ):\n            return True\n\n    return False",
    "start_line": 462,
    "end_line": 485,
    "has_docstring": true,
    "docstring": "Check if a filename matches entry point patterns.\n\nArgs:\n    filename: Name of the file to check\n\nReturns:\n    True if the file is likely an entry point",
    "parameters": [
      "filename"
    ],
    "node_type": "function",
    "base_classes": null,
    "class_name": null,
    "display_name": "function is_entry_point_file",
    "component_id": "codewiki.src.be.dependency_analyzer.utils.patterns.is_entry_point_file"
  },
  "codewiki.src.be.dependency_analyzer.utils.patterns.is_entry_point_path": {
    "id": "codewiki.src.be.dependency_analyzer.utils.patterns.is_entry_point_path",
    "name": "is_entry_point_path",
    "component_type": "function",
    "file_path": "/Users/wufagang/project/aiopen/CodeWiki/codewiki/src/be/dependency_analyzer/utils/patterns.py",
    "relative_path": "codewiki/src/be/dependency_analyzer/utils/patterns.py",
    "depends_on": [],
    "source_code": "def is_entry_point_path(filepath: str) -> bool:\n    \"\"\"\n    Check if a file path matches entry point path patterns.\n\n    Args:\n        filepath: Full path of the file to check\n\n    Returns:\n        True if the path suggests an entry point\n    \"\"\"\n    filepath_lower = filepath.lower()\n\n    for pattern in ENTRY_POINT_PATH_PATTERNS:\n        if pattern in filepath_lower:\n            return True\n\n    return False",
    "start_line": 488,
    "end_line": 504,
    "has_docstring": true,
    "docstring": "Check if a file path matches entry point path patterns.\n\nArgs:\n    filepath: Full path of the file to check\n\nReturns:\n    True if the path suggests an entry point",
    "parameters": [
      "filepath"
    ],
    "node_type": "function",
    "base_classes": null,
    "class_name": null,
    "display_name": "function is_entry_point_path",
    "component_id": "codewiki.src.be.dependency_analyzer.utils.patterns.is_entry_point_path"
  },
  "codewiki.src.be.dependency_analyzer.utils.patterns.has_high_connectivity_potential": {
    "id": "codewiki.src.be.dependency_analyzer.utils.patterns.has_high_connectivity_potential",
    "name": "has_high_connectivity_potential",
    "component_type": "function",
    "file_path": "/Users/wufagang/project/aiopen/CodeWiki/codewiki/src/be/dependency_analyzer/utils/patterns.py",
    "relative_path": "codewiki/src/be/dependency_analyzer/utils/patterns.py",
    "depends_on": [],
    "source_code": "def has_high_connectivity_potential(filename: str, filepath: str) -> bool:\n    \"\"\"\n    Check if a file has high connectivity potential based on name and path.\n\n    Args:\n        filename: Name of the file\n        filepath: Full path of the file\n\n    Returns:\n        True if the file likely has high connectivity\n    \"\"\"\n    filename_lower = filename.lower()\n    filepath_lower = filepath.lower()\n\n    # Check filename patterns\n    if any(pattern in filename_lower for pattern in HIGH_CONNECTIVITY_PATTERNS):\n        return True\n\n    # Check filepath patterns\n    if any(pattern in filepath_lower for pattern in HIGH_CONNECTIVITY_PATTERNS):\n        return True\n\n    # Check source directory patterns\n    if any(pattern in filepath_lower for pattern in SOURCE_DIRECTORY_PATTERNS):\n        return True\n\n    return False",
    "start_line": 507,
    "end_line": 533,
    "has_docstring": true,
    "docstring": "Check if a file has high connectivity potential based on name and path.\n\nArgs:\n    filename: Name of the file\n    filepath: Full path of the file\n\nReturns:\n    True if the file likely has high connectivity",
    "parameters": [
      "filename",
      "filepath"
    ],
    "node_type": "function",
    "base_classes": null,
    "class_name": null,
    "display_name": "function has_high_connectivity_potential",
    "component_id": "codewiki.src.be.dependency_analyzer.utils.patterns.has_high_connectivity_potential"
  },
  "codewiki.src.be.dependency_analyzer.utils.patterns.is_critical_function": {
    "id": "codewiki.src.be.dependency_analyzer.utils.patterns.is_critical_function",
    "name": "is_critical_function",
    "component_type": "function",
    "file_path": "/Users/wufagang/project/aiopen/CodeWiki/codewiki/src/be/dependency_analyzer/utils/patterns.py",
    "relative_path": "codewiki/src/be/dependency_analyzer/utils/patterns.py",
    "depends_on": [],
    "source_code": "def is_critical_function(func_name: str, code_snippet: str = None) -> bool:\n    \"\"\"\n    Check if a function is critical based on name and code patterns.\n\n    Args:\n        func_name: Name of the function\n        code_snippet: Optional code snippet to analyze\n\n    Returns:\n        True if the function is considered critical\n    \"\"\"\n    # Check critical function names\n    if func_name.lower() in CRITICAL_FUNCTION_NAMES:\n        return True\n\n    # Check export patterns in code snippet\n    if code_snippet:\n        snippet_lower = code_snippet.lower()\n        if any(pattern in snippet_lower for pattern in EXPORT_PATTERNS):\n            return True\n\n    return False",
    "start_line": 536,
    "end_line": 557,
    "has_docstring": true,
    "docstring": "Check if a function is critical based on name and code patterns.\n\nArgs:\n    func_name: Name of the function\n    code_snippet: Optional code snippet to analyze\n\nReturns:\n    True if the function is considered critical",
    "parameters": [
      "func_name",
      "code_snippet"
    ],
    "node_type": "function",
    "base_classes": null,
    "class_name": null,
    "display_name": "function is_critical_function",
    "component_id": "codewiki.src.be.dependency_analyzer.utils.patterns.is_critical_function"
  },
  "codewiki.src.be.dependency_analyzer.utils.patterns.find_fallback_entry_points": {
    "id": "codewiki.src.be.dependency_analyzer.utils.patterns.find_fallback_entry_points",
    "name": "find_fallback_entry_points",
    "component_type": "function",
    "file_path": "/Users/wufagang/project/aiopen/CodeWiki/codewiki/src/be/dependency_analyzer/utils/patterns.py",
    "relative_path": "codewiki/src/be/dependency_analyzer/utils/patterns.py",
    "depends_on": [
      "codewiki.src.be.dependency_analyzer.utils.patterns.is_entry_point_path"
    ],
    "source_code": "def find_fallback_entry_points(code_files: List[Dict], max_files: int = 5) -> List[Dict]:\n    \"\"\"\n    Find fallback entry points when standard patterns don't match.\n\n    Args:\n        code_files: List of all code files\n        max_files: Maximum number of fallback files to return\n\n    Returns:\n        List of files that could serve as entry points\n    \"\"\"\n    fallback_files = []\n\n    # Try fallback name patterns\n    for file_info in code_files:\n        filename = file_info[\"name\"].lower()\n        filepath = file_info[\"path\"].lower()\n\n        # Check for any main-like files\n        if any(pattern in filename for pattern in [\"main\", \"app\", \"server\", \"start\", \"index\"]):\n            fallback_files.append(file_info)\n\n        # Check for entry point paths\n        elif is_entry_point_path(filepath):\n            fallback_files.append(file_info)\n\n    # If still nothing, try files in root or common directories\n    if not fallback_files:\n        for file_info in code_files:\n            filepath = file_info[\"path\"]\n            # Files in root directory or immediate subdirectories\n            if filepath.count(\"/\") <= 1:\n                fallback_files.append(file_info)\n\n    # Sort by likelihood (prefer shorter paths, common names)\n    def fallback_priority(file_info):\n        path = file_info[\"path\"].lower()\n        name = file_info[\"name\"].lower()\n\n        score = 0\n        # Prefer shorter paths (closer to root)\n        score -= path.count(\"/\")\n        # Prefer common entry point names\n        if any(pattern in name for pattern in [\"main\", \"app\", \"index\"]):\n            score -= 10\n        # Prefer certain extensions\n        if any(ext in name for ext in [\".py\", \".js\", \".go\", \".rs\"]):\n            score -= 5\n\n        return score\n\n    fallback_files.sort(key=fallback_priority)\n    return fallback_files[:max_files]",
    "start_line": 560,
    "end_line": 612,
    "has_docstring": true,
    "docstring": "Find fallback entry points when standard patterns don't match.\n\nArgs:\n    code_files: List of all code files\n    max_files: Maximum number of fallback files to return\n\nReturns:\n    List of files that could serve as entry points",
    "parameters": [
      "code_files",
      "max_files"
    ],
    "node_type": "function",
    "base_classes": null,
    "class_name": null,
    "display_name": "function find_fallback_entry_points",
    "component_id": "codewiki.src.be.dependency_analyzer.utils.patterns.find_fallback_entry_points"
  },
  "codewiki.src.be.dependency_analyzer.utils.patterns.fallback_priority": {
    "id": "codewiki.src.be.dependency_analyzer.utils.patterns.fallback_priority",
    "name": "fallback_priority",
    "component_type": "function",
    "file_path": "/Users/wufagang/project/aiopen/CodeWiki/codewiki/src/be/dependency_analyzer/utils/patterns.py",
    "relative_path": "codewiki/src/be/dependency_analyzer/utils/patterns.py",
    "depends_on": [],
    "source_code": "    def fallback_priority(file_info):\n        path = file_info[\"path\"].lower()\n        name = file_info[\"name\"].lower()\n\n        score = 0\n        # Prefer shorter paths (closer to root)\n        score -= path.count(\"/\")\n        # Prefer common entry point names\n        if any(pattern in name for pattern in [\"main\", \"app\", \"index\"]):\n            score -= 10\n        # Prefer certain extensions\n        if any(ext in name for ext in [\".py\", \".js\", \".go\", \".rs\"]):\n            score -= 5\n\n        return score",
    "start_line": 595,
    "end_line": 609,
    "has_docstring": false,
    "docstring": "",
    "parameters": [
      "file_info"
    ],
    "node_type": "function",
    "base_classes": null,
    "class_name": null,
    "display_name": "function fallback_priority",
    "component_id": "codewiki.src.be.dependency_analyzer.utils.patterns.fallback_priority"
  },
  "codewiki.src.be.dependency_analyzer.utils.patterns.find_fallback_connectivity_files": {
    "id": "codewiki.src.be.dependency_analyzer.utils.patterns.find_fallback_connectivity_files",
    "name": "find_fallback_connectivity_files",
    "component_type": "function",
    "file_path": "/Users/wufagang/project/aiopen/CodeWiki/codewiki/src/be/dependency_analyzer/utils/patterns.py",
    "relative_path": "codewiki/src/be/dependency_analyzer/utils/patterns.py",
    "depends_on": [],
    "source_code": "def find_fallback_connectivity_files(code_files: List[Dict], max_files: int = 10) -> List[Dict]:\n    \"\"\"\n    Find fallback high-connectivity files when standard patterns don't match.\n\n    Args:\n        code_files: List of all code files\n        max_files: Maximum number of fallback files to return\n\n    Returns:\n        List of files that likely have good connectivity\n    \"\"\"\n    fallback_files = []\n\n    # Include all files from common source directories\n    for file_info in code_files:\n        filepath = file_info[\"path\"].lower()\n\n        # Any file in src, lib, or similar directories\n        if any(pattern in filepath for pattern in [\"src/\", \"lib/\", \"app/\", \"pkg/\", \"core/\"]):\n            fallback_files.append(file_info)\n\n    # If still not enough, include files with certain extensions\n    if len(fallback_files) < max_files:\n        for file_info in code_files:\n            if file_info not in fallback_files:\n                name = file_info[\"name\"].lower()\n                # Include common source file extensions\n                if any(ext in name for ext in [\".py\", \".js\", \".ts\", \".go\", \".rs\", \".c\", \".cpp\"]):\n                    # Skip test files\n                    if not any(test_pattern in name for test_pattern in [\"test\", \"spec\", \"_test\"]):\n                        fallback_files.append(file_info)\n\n    return fallback_files[:max_files]",
    "start_line": 615,
    "end_line": 647,
    "has_docstring": true,
    "docstring": "Find fallback high-connectivity files when standard patterns don't match.\n\nArgs:\n    code_files: List of all code files\n    max_files: Maximum number of fallback files to return\n\nReturns:\n    List of files that likely have good connectivity",
    "parameters": [
      "code_files",
      "max_files"
    ],
    "node_type": "function",
    "base_classes": null,
    "class_name": null,
    "display_name": "function find_fallback_connectivity_files",
    "component_id": "codewiki.src.be.dependency_analyzer.utils.patterns.find_fallback_connectivity_files"
  },
  "codewiki.src.be.dependency_analyzer.utils.security._inside": {
    "id": "codewiki.src.be.dependency_analyzer.utils.security._inside",
    "name": "_inside",
    "component_type": "function",
    "file_path": "/Users/wufagang/project/aiopen/CodeWiki/codewiki/src/be/dependency_analyzer/utils/security.py",
    "relative_path": "codewiki/src/be/dependency_analyzer/utils/security.py",
    "depends_on": [],
    "source_code": "def _inside(base: Path, target: Path) -> bool:\n    base_r = base.resolve()\n    try:\n        target_r = target.resolve()\n        return target_r.is_relative_to(base_r)  # py>=3.9\n    except AttributeError:\n        return str(target.resolve()).startswith(str(base_r))",
    "start_line": 4,
    "end_line": 10,
    "has_docstring": false,
    "docstring": "",
    "parameters": [
      "base",
      "target"
    ],
    "node_type": "function",
    "base_classes": null,
    "class_name": null,
    "display_name": "function _inside",
    "component_id": "codewiki.src.be.dependency_analyzer.utils.security._inside"
  },
  "codewiki.src.be.dependency_analyzer.utils.security.assert_safe_path": {
    "id": "codewiki.src.be.dependency_analyzer.utils.security.assert_safe_path",
    "name": "assert_safe_path",
    "component_type": "function",
    "file_path": "/Users/wufagang/project/aiopen/CodeWiki/codewiki/src/be/dependency_analyzer/utils/security.py",
    "relative_path": "codewiki/src/be/dependency_analyzer/utils/security.py",
    "depends_on": [
      "codewiki.src.be.dependency_analyzer.utils.security._inside"
    ],
    "source_code": "def assert_safe_path(base_dir: Path, target: Path):\n    # Block symlinks (file or dir)\n    if target.is_symlink():\n        raise PermissionError(f\"Symlink blocked: {target}\")\n    # Block paths that escape repo\n    if not _inside(base_dir, target):\n        raise PermissionError(f\"Path escapes repo: {target} -> {target.resolve()}\")",
    "start_line": 12,
    "end_line": 18,
    "has_docstring": false,
    "docstring": "",
    "parameters": [
      "base_dir",
      "target"
    ],
    "node_type": "function",
    "base_classes": null,
    "class_name": null,
    "display_name": "function assert_safe_path",
    "component_id": "codewiki.src.be.dependency_analyzer.utils.security.assert_safe_path"
  },
  "codewiki.src.be.dependency_analyzer.utils.security.safe_open_text": {
    "id": "codewiki.src.be.dependency_analyzer.utils.security.safe_open_text",
    "name": "safe_open_text",
    "component_type": "function",
    "file_path": "/Users/wufagang/project/aiopen/CodeWiki/codewiki/src/be/dependency_analyzer/utils/security.py",
    "relative_path": "codewiki/src/be/dependency_analyzer/utils/security.py",
    "depends_on": [
      "codewiki.src.be.dependency_analyzer.utils.security.assert_safe_path"
    ],
    "source_code": "def safe_open_text(base_dir: Path, target: Path, encoding=\"utf-8\"):\n    assert_safe_path(base_dir, target)\n    flags = os.O_RDONLY\n    if hasattr(os, \"O_NOFOLLOW\"):\n        flags |= os.O_NOFOLLOW\n    fd = os.open(str(target), flags)\n    try:\n        with os.fdopen(fd, \"r\", encoding=encoding, errors=\"replace\") as f:\n            return f.read()\n    finally:\n        try:\n            os.close(fd)\n        except OSError:\n            pass",
    "start_line": 20,
    "end_line": 33,
    "has_docstring": false,
    "docstring": "",
    "parameters": [
      "base_dir",
      "target",
      "encoding"
    ],
    "node_type": "function",
    "base_classes": null,
    "class_name": null,
    "display_name": "function safe_open_text",
    "component_id": "codewiki.src.be.dependency_analyzer.utils.security.safe_open_text"
  },
  "codewiki.src.be.documentation_generator.DocumentationGenerator": {
    "id": "codewiki.src.be.documentation_generator.DocumentationGenerator",
    "name": "DocumentationGenerator",
    "component_type": "class",
    "file_path": "/Users/wufagang/project/aiopen/CodeWiki/codewiki/src/be/documentation_generator.py",
    "relative_path": "codewiki/src/be/documentation_generator.py",
    "depends_on": [
      "codewiki.cli.utils.errors.warning",
      "codewiki.src.be.agent_orchestrator.AgentOrchestrator",
      "codewiki.cli.utils.fs.ensure_directory",
      "codewiki.src.be.cluster_modules.cluster_modules",
      "codewiki.src.be.dependency_analyzer.dependency_graphs_builder.DependencyGraphBuilder",
      "codewiki.src.be.llm_services.call_llm",
      "codewiki.cli.utils.errors.info"
    ],
    "source_code": "class DocumentationGenerator:\n    \"\"\"Main documentation generation orchestrator.\"\"\"\n    \n    def __init__(self, config: Config, commit_id: str = None):\n        self.config = config\n        self.commit_id = commit_id\n        self.graph_builder = DependencyGraphBuilder(config)\n        self.agent_orchestrator = AgentOrchestrator(config)\n    \n    def create_documentation_metadata(self, working_dir: str, components: Dict[str, Any], num_leaf_nodes: int):\n        \"\"\"Create a metadata file with documentation generation information.\"\"\"\n        from datetime import datetime\n        \n        metadata = {\n            \"generation_info\": {\n                \"timestamp\": datetime.now().isoformat(),\n                \"main_model\": self.config.main_model,\n                \"generator_version\": \"1.0.1\",\n                \"repo_path\": self.config.repo_path,\n                \"commit_id\": self.commit_id\n            },\n            \"statistics\": {\n                \"total_components\": len(components),\n                \"leaf_nodes\": num_leaf_nodes,\n                \"max_depth\": self.config.max_depth\n            },\n            \"files_generated\": [\n                \"overview.md\",\n                \"module_tree.json\",\n                \"first_module_tree.json\"\n            ]\n        }\n        \n        # Add generated markdown files to the metadata\n        try:\n            for file_path in os.listdir(working_dir):\n                if file_path.endswith('.md') and file_path not in metadata[\"files_generated\"]:\n                    metadata[\"files_generated\"].append(file_path)\n        except Exception as e:\n            logger.warning(f\"Could not list generated files: {e}\")\n        \n        metadata_path = os.path.join(working_dir, \"metadata.json\")\n        file_manager.save_json(metadata, metadata_path)\n\n    \n    def get_processing_order(self, module_tree: Dict[str, Any], parent_path: List[str] = []) -> List[tuple[List[str], str]]:\n        \"\"\"Get the processing order using topological sort (leaf modules first).\"\"\"\n        processing_order = []\n        \n        def collect_modules(tree: Dict[str, Any], path: List[str]):\n            for module_name, module_info in tree.items():\n                current_path = path + [module_name]\n                \n                # If this module has children, process them first\n                if module_info.get(\"children\") and isinstance(module_info[\"children\"], dict) and module_info[\"children\"]:\n                    collect_modules(module_info[\"children\"], current_path)\n                    # Add this parent module after its children\n                    processing_order.append((current_path, module_name))\n                else:\n                    # This is a leaf module, add it immediately\n                    processing_order.append((current_path, module_name))\n        \n        collect_modules(module_tree, parent_path)\n        return processing_order\n\n    def is_leaf_module(self, module_info: Dict[str, Any]) -> bool:\n        \"\"\"Check if a module is a leaf module (has no children or empty children).\"\"\"\n        children = module_info.get(\"children\", {})\n        return not children or (isinstance(children, dict) and len(children) == 0)\n\n    def build_overview_structure(self, module_tree: Dict[str, Any], module_path: List[str],\n                                 working_dir: str) -> Dict[str, Any]:\n        \"\"\"Build structure for overview generation with 1-depth children docs and target indicator.\"\"\"\n        \n        processed_module_tree = deepcopy(module_tree)\n        module_info = processed_module_tree\n        for path_part in module_path:\n            module_info = module_info[path_part]\n            if path_part != module_path[-1]:\n                module_info = module_info.get(\"children\", {})\n            else:\n                module_info[\"is_target_for_overview_generation\"] = True\n\n        if \"children\" in module_info:\n            module_info = module_info[\"children\"]\n\n        for child_name, child_info in module_info.items():\n            if os.path.exists(os.path.join(working_dir, f\"{child_name}.md\")):\n                child_info[\"docs\"] = file_manager.load_text(os.path.join(working_dir, f\"{child_name}.md\"))\n            else:\n                logger.warning(f\"Module docs not found at {os.path.join(working_dir, f\"{child_name}.md\")}\")\n                child_info[\"docs\"] = \"\"\n\n        return processed_module_tree\n\n    async def generate_module_documentation(self, components: Dict[str, Any], leaf_nodes: List[str]) -> str:\n        \"\"\"Generate documentation for all modules using dynamic programming approach.\"\"\"\n        # Prepare output directory\n        working_dir = os.path.abspath(self.config.docs_dir)\n        file_manager.ensure_directory(working_dir)\n\n        module_tree_path = os.path.join(working_dir, MODULE_TREE_FILENAME)\n        first_module_tree_path = os.path.join(working_dir, FIRST_MODULE_TREE_FILENAME)\n        module_tree = file_manager.load_json(module_tree_path)\n        first_module_tree = file_manager.load_json(first_module_tree_path)\n        \n        # Get processing order (leaf modules first)\n        processing_order = self.get_processing_order(first_module_tree)\n\n        \n        # Process modules in dependency order\n        final_module_tree = module_tree\n        processed_modules = set()\n\n        if len(module_tree) > 0:\n            for module_path, module_name in processing_order:\n                try:\n                    # Get the module info from the tree\n                    module_info = module_tree\n                    for path_part in module_path:\n                        module_info = module_info[path_part]\n                        if path_part != module_path[-1]:  # Not the last part\n                            module_info = module_info.get(\"children\", {})\n                    \n                    # Skip if already processed\n                    module_key = \"/\".join(module_path)\n                    if module_key in processed_modules:\n                        continue\n                    \n                    # Process the module\n                    if self.is_leaf_module(module_info):\n                        logger.info(f\"📄 Processing leaf module: {module_key}\")\n                        final_module_tree = await self.agent_orchestrator.process_module(\n                            module_name, components, module_info[\"components\"], module_path, working_dir\n                        )\n                    else:\n                        logger.info(f\"📁 Processing parent module: {module_key}\")\n                        final_module_tree = await self.generate_parent_module_docs(\n                            module_path, working_dir\n                        )\n                    \n                    processed_modules.add(module_key)\n                    \n                except Exception as e:\n                    logger.error(f\"Failed to process module {module_key}: {str(e)}\")\n                    logger.error(f\"Traceback: {traceback.format_exc()}\")\n                    continue\n\n            # Generate repo overview\n            logger.info(f\"📚 Generating repository overview\")\n            final_module_tree = await self.generate_parent_module_docs(\n                [], working_dir\n            )\n        else:\n            logger.info(f\"Processing whole repo because repo can fit in the context window\")\n            repo_name = os.path.basename(os.path.normpath(self.config.repo_path))\n            final_module_tree = await self.agent_orchestrator.process_module(\n                repo_name, components, leaf_nodes, [], working_dir\n            )\n\n            # save final_module_tree to module_tree.json\n            file_manager.save_json(final_module_tree, os.path.join(working_dir, MODULE_TREE_FILENAME))\n\n            # rename repo_name.md to overview.md\n            repo_overview_path = os.path.join(working_dir, f\"{repo_name}.md\")\n            if os.path.exists(repo_overview_path):\n                os.rename(repo_overview_path, os.path.join(working_dir, OVERVIEW_FILENAME))\n        \n        return working_dir\n\n    async def generate_parent_module_docs(self, module_path: List[str], \n                                        working_dir: str) -> Dict[str, Any]:\n        \"\"\"Generate documentation for a parent module based on its children's documentation.\"\"\"\n        module_name = module_path[-1] if len(module_path) >= 1 else os.path.basename(os.path.normpath(self.config.repo_path))\n\n        logger.info(f\"Generating parent documentation for: {module_name}\")\n        \n        # Load module tree\n        module_tree_path = os.path.join(working_dir, MODULE_TREE_FILENAME)\n        module_tree = file_manager.load_json(module_tree_path)\n\n        # check if overview docs already exists\n        overview_docs_path = os.path.join(working_dir, OVERVIEW_FILENAME)\n        if os.path.exists(overview_docs_path):\n            logger.info(f\"✓ Overview docs already exists at {overview_docs_path}\")\n            return module_tree\n\n        # check if parent docs already exists\n        parent_docs_path = os.path.join(working_dir, f\"{module_name if len(module_path) >= 1 else OVERVIEW_FILENAME.replace('.md', '')}.md\")\n        if os.path.exists(parent_docs_path):\n            logger.info(f\"✓ Parent docs already exists at {parent_docs_path}\")\n            return module_tree\n\n        # Create repo structure with 1-depth children docs and target indicator\n        repo_structure = self.build_overview_structure(module_tree, module_path, working_dir)\n\n        prompt = MODULE_OVERVIEW_PROMPT.format(\n            module_name=module_name,\n            repo_structure=json.dumps(repo_structure, indent=4)\n        ) if len(module_path) >= 1 else REPO_OVERVIEW_PROMPT.format(\n            repo_name=module_name,\n            repo_structure=json.dumps(repo_structure, indent=4)\n        )\n        \n        try:\n            parent_docs = call_llm(prompt, self.config)\n            \n            # Parse and save parent documentation\n            parent_content = parent_docs.split(\"<OVERVIEW>\")[1].split(\"</OVERVIEW>\")[0].strip()\n            # parent_content = prompt\n            file_manager.save_text(parent_content, parent_docs_path)\n            \n            logger.debug(f\"Successfully generated parent documentation for: {module_name}\")\n            return module_tree\n            \n        except Exception as e:\n            logger.error(f\"Error generating parent documentation for {module_name}: {str(e)}\")\n            logger.error(f\"Traceback: {traceback.format_exc()}\")\n            raise\n    \n    async def run(self) -> None:\n        \"\"\"Run the complete documentation generation process using dynamic programming.\"\"\"\n        try:\n            # Build dependency graph\n            components, leaf_nodes = self.graph_builder.build_dependency_graph()\n\n            logger.debug(f\"Found {len(leaf_nodes)} leaf nodes\")\n            # logger.debug(f\"Leaf nodes:\\n{'\\n'.join(sorted(leaf_nodes)[:200])}\")\n            # exit()\n            \n            # Cluster modules\n            working_dir = os.path.abspath(self.config.docs_dir)\n            file_manager.ensure_directory(working_dir)\n            first_module_tree_path = os.path.join(working_dir, FIRST_MODULE_TREE_FILENAME)\n            module_tree_path = os.path.join(working_dir, MODULE_TREE_FILENAME)\n            \n            # Check if module tree exists\n            if os.path.exists(first_module_tree_path):\n                logger.debug(f\"Module tree found at {first_module_tree_path}\")\n                module_tree = file_manager.load_json(first_module_tree_path)\n            else:\n                logger.debug(f\"Module tree not found at {module_tree_path}, clustering modules\")\n                module_tree = cluster_modules(leaf_nodes, components, self.config)\n                file_manager.save_json(module_tree, first_module_tree_path)\n            \n            file_manager.save_json(module_tree, module_tree_path)\n            \n            logger.debug(f\"Grouped components into {len(module_tree)} modules\")\n            \n            # Generate module documentation using dynamic programming approach\n            # This processes leaf modules first, then parent modules\n            working_dir = await self.generate_module_documentation(components, leaf_nodes)\n            \n            # Create documentation metadata\n            self.create_documentation_metadata(working_dir, components, len(leaf_nodes))\n            \n            logger.debug(f\"Documentation generation completed successfully using dynamic programming!\")\n            logger.debug(f\"Processing order: leaf modules → parent modules → repository overview\")\n            logger.debug(f\"Documentation saved to: {working_dir}\")\n            \n        except Exception as e:\n            logger.error(f\"Documentation generation failed: {str(e)}\")\n            logger.error(f\"Traceback: {traceback.format_exc()}\")\n            raise",
    "start_line": 29,
    "end_line": 292,
    "has_docstring": true,
    "docstring": "Main documentation generation orchestrator.",
    "parameters": null,
    "node_type": "class",
    "base_classes": null,
    "class_name": null,
    "display_name": "class DocumentationGenerator",
    "component_id": "codewiki.src.be.documentation_generator.DocumentationGenerator"
  },
  "codewiki.src.be.jdcloud_adapter.is_jdcloud_api": {
    "id": "codewiki.src.be.jdcloud_adapter.is_jdcloud_api",
    "name": "is_jdcloud_api",
    "component_type": "function",
    "file_path": "/Users/wufagang/project/aiopen/CodeWiki/codewiki/src/be/jdcloud_adapter.py",
    "relative_path": "codewiki/src/be/jdcloud_adapter.py",
    "depends_on": [],
    "source_code": "def is_jdcloud_api(base_url: str) -> bool:\n    \"\"\"Check if the base URL is for JDCloud API.\"\"\"\n    jdcloud_domains = [\n        'ai-api.jdcloud.com',\n        'jdcloud.com',\n        'jd.com'\n    ]\n    return any(domain in base_url.lower() for domain in jdcloud_domains)",
    "start_line": 13,
    "end_line": 20,
    "has_docstring": true,
    "docstring": "Check if the base URL is for JDCloud API.",
    "parameters": [
      "base_url"
    ],
    "node_type": "function",
    "base_classes": null,
    "class_name": null,
    "display_name": "function is_jdcloud_api",
    "component_id": "codewiki.src.be.jdcloud_adapter.is_jdcloud_api"
  },
  "codewiki.src.be.jdcloud_adapter.normalize_jdcloud_response": {
    "id": "codewiki.src.be.jdcloud_adapter.normalize_jdcloud_response",
    "name": "normalize_jdcloud_response",
    "component_type": "function",
    "file_path": "/Users/wufagang/project/aiopen/CodeWiki/codewiki/src/be/jdcloud_adapter.py",
    "relative_path": "codewiki/src/be/jdcloud_adapter.py",
    "depends_on": [],
    "source_code": "def normalize_jdcloud_response(response: ChatCompletion) -> ChatCompletion:\n    \"\"\"\n    Normalize JDCloud Claude API response to be fully compatible with OpenAI format.\n\n    Fixes:\n    1. content field: converts [\"text\"] to \"text\"\n    2. finish_reason field: converts \"end_turn\" to \"stop\", \"tool_use\" to \"tool_calls\"\n    3. tool_calls format: converts tool_use format to function format\n\n    Args:\n        response: Raw response from JDCloud API\n\n    Returns:\n        Normalized response compatible with pydantic-ai\n    \"\"\"\n    import logging\n    logger = logging.getLogger(__name__)\n\n    try:\n        # Create a copy of the response to avoid modifying the original\n        response_dict = response.model_dump()\n        logger.debug(f\"Original JDCloud response structure: {response_dict}\")\n    except Exception as e:\n        logger.error(f\"Failed to dump response model: {e}\")\n        logger.error(f\"Response type: {type(response)}\")\n        logger.error(f\"Response content: {response}\")\n        raise\n\n    # Fix choices\n    for choice in response_dict.get('choices', []):\n        # Fix content field (convert list to string)\n        message = choice.get('message', {})\n        content = message.get('content')\n\n        if isinstance(content, list) and len(content) > 0:\n            # Convert list to string by joining elements\n            message['content'] = ''.join(str(item) for item in content)\n        elif isinstance(content, list) and len(content) == 0:\n            # Handle empty list\n            message['content'] = \"\"\n\n        # Fix finish_reason field\n        finish_reason = choice.get('finish_reason')\n        if finish_reason == 'end_turn':\n            choice['finish_reason'] = 'stop'\n        elif finish_reason == 'max_tokens':\n            choice['finish_reason'] = 'length'\n        elif finish_reason == 'tool_use':\n            choice['finish_reason'] = 'tool_calls'\n\n        # Fix tool_calls format\n        tool_calls = message.get('tool_calls', [])\n        if tool_calls:\n            for tool_call in tool_calls:\n                # Convert tool_use type to function type\n                if tool_call.get('type') == 'tool_use':\n                    tool_call['type'] = 'function'\n\n                # Ensure function structure is correct\n                if 'function' not in tool_call and 'name' in tool_call:\n                    # Restructure tool call to OpenAI format\n                    tool_call['function'] = {\n                        'name': tool_call.get('name', ''),\n                        'arguments': tool_call.get('input', {})\n                    }\n                    # Remove JDCloud-specific fields\n                    tool_call.pop('name', None)\n                    tool_call.pop('input', None)\n\n    # Reconstruct the ChatCompletion object\n    return ChatCompletion.model_validate(response_dict)",
    "start_line": 23,
    "end_line": 93,
    "has_docstring": true,
    "docstring": "Normalize JDCloud Claude API response to be fully compatible with OpenAI format.\n\nFixes:\n1. content field: converts [\"text\"] to \"text\"\n2. finish_reason field: converts \"end_turn\" to \"stop\", \"tool_use\" to \"tool_calls\"\n3. tool_calls format: converts tool_use format to function format\n\nArgs:\n    response: Raw response from JDCloud API\n\nReturns:\n    Normalized response compatible with pydantic-ai",
    "parameters": [
      "response"
    ],
    "node_type": "function",
    "base_classes": null,
    "class_name": null,
    "display_name": "function normalize_jdcloud_response",
    "component_id": "codewiki.src.be.jdcloud_adapter.normalize_jdcloud_response"
  },
  "codewiki.src.be.jdcloud_adapter.extract_content_from_jdcloud_response": {
    "id": "codewiki.src.be.jdcloud_adapter.extract_content_from_jdcloud_response",
    "name": "extract_content_from_jdcloud_response",
    "component_type": "function",
    "file_path": "/Users/wufagang/project/aiopen/CodeWiki/codewiki/src/be/jdcloud_adapter.py",
    "relative_path": "codewiki/src/be/jdcloud_adapter.py",
    "depends_on": [],
    "source_code": "def extract_content_from_jdcloud_response(response: ChatCompletion) -> str:\n    \"\"\"\n    Extract content from JDCloud response, handling both list and string formats.\n\n    Args:\n        response: ChatCompletion response from JDCloud API\n\n    Returns:\n        Content as string\n    \"\"\"\n\n    if not response.choices:\n        return \"\"\n\n    content = response.choices[0].message.content\n\n    # Handle list format (JDCloud specific)\n    if isinstance(content, list):\n        return ''.join(str(item) for item in content)\n\n    # Handle standard string format\n    return content or \"\"",
    "start_line": 96,
    "end_line": 117,
    "has_docstring": true,
    "docstring": "Extract content from JDCloud response, handling both list and string formats.\n\nArgs:\n    response: ChatCompletion response from JDCloud API\n\nReturns:\n    Content as string",
    "parameters": [
      "response"
    ],
    "node_type": "function",
    "base_classes": null,
    "class_name": null,
    "display_name": "function extract_content_from_jdcloud_response",
    "component_id": "codewiki.src.be.jdcloud_adapter.extract_content_from_jdcloud_response"
  },
  "codewiki.src.be.jdcloud_adapter.JDCloudOpenAIClient": {
    "id": "codewiki.src.be.jdcloud_adapter.JDCloudOpenAIClient",
    "name": "JDCloudOpenAIClient",
    "component_type": "class",
    "file_path": "/Users/wufagang/project/aiopen/CodeWiki/codewiki/src/be/jdcloud_adapter.py",
    "relative_path": "codewiki/src/be/jdcloud_adapter.py",
    "depends_on": [
      "codewiki.src.be.jdcloud_adapter.JDCloudChatCompletions"
    ],
    "source_code": "class JDCloudOpenAIClient:\n    \"\"\"\n    Wrapper around OpenAI client that normalizes JDCloud responses.\n    \"\"\"\n\n    def __init__(self, client):\n        self.client = client\n\n    def __getattr__(self, name):\n        \"\"\"Delegate all other attributes to the wrapped client.\"\"\"\n        return getattr(self.client, name)\n\n    @property\n    def chat(self):\n        \"\"\"Return wrapped chat completions.\"\"\"\n        return JDCloudChatCompletions(self.client.chat)",
    "start_line": 120,
    "end_line": 135,
    "has_docstring": true,
    "docstring": "Wrapper around OpenAI client that normalizes JDCloud responses.",
    "parameters": null,
    "node_type": "class",
    "base_classes": null,
    "class_name": null,
    "display_name": "class JDCloudOpenAIClient",
    "component_id": "codewiki.src.be.jdcloud_adapter.JDCloudOpenAIClient"
  },
  "codewiki.src.be.jdcloud_adapter.JDCloudChatCompletions": {
    "id": "codewiki.src.be.jdcloud_adapter.JDCloudChatCompletions",
    "name": "JDCloudChatCompletions",
    "component_type": "class",
    "file_path": "/Users/wufagang/project/aiopen/CodeWiki/codewiki/src/be/jdcloud_adapter.py",
    "relative_path": "codewiki/src/be/jdcloud_adapter.py",
    "depends_on": [
      "codewiki.src.be.jdcloud_adapter.JDCloudCompletions"
    ],
    "source_code": "class JDCloudChatCompletions:\n    \"\"\"Wrapper for chat completions that normalizes responses.\"\"\"\n\n    def __init__(self, chat_completions):\n        self.completions = JDCloudCompletions(chat_completions.completions)\n\n    def __getattr__(self, name):\n        \"\"\"Delegate all other attributes.\"\"\"\n        return getattr(self.completions.chat_completions, name)",
    "start_line": 138,
    "end_line": 146,
    "has_docstring": true,
    "docstring": "Wrapper for chat completions that normalizes responses.",
    "parameters": null,
    "node_type": "class",
    "base_classes": null,
    "class_name": null,
    "display_name": "class JDCloudChatCompletions",
    "component_id": "codewiki.src.be.jdcloud_adapter.JDCloudChatCompletions"
  },
  "codewiki.src.be.jdcloud_adapter.JDCloudCompletions": {
    "id": "codewiki.src.be.jdcloud_adapter.JDCloudCompletions",
    "name": "JDCloudCompletions",
    "component_type": "class",
    "file_path": "/Users/wufagang/project/aiopen/CodeWiki/codewiki/src/be/jdcloud_adapter.py",
    "relative_path": "codewiki/src/be/jdcloud_adapter.py",
    "depends_on": [
      "codewiki.cli.utils.errors.warning",
      "codewiki.src.be.jdcloud_adapter.normalize_jdcloud_response"
    ],
    "source_code": "class JDCloudCompletions:\n    \"\"\"Wrapper for completions that normalizes responses.\"\"\"\n\n    def __init__(self, completions):\n        self.chat_completions = completions\n\n    def create(self, **kwargs) -> ChatCompletion:\n        \"\"\"\n        Create chat completion with JDCloud response normalization.\n\n        Returns:\n            Normalized ChatCompletion object\n        \"\"\"\n        import logging\n        logger = logging.getLogger(__name__)\n\n        # Log the outgoing request for debugging\n        logger.debug(f\"JDCloud API request: {kwargs}\")\n\n        # Call original API\n        try:\n            response = self.chat_completions.create(**kwargs)\n            logger.debug(f\"JDCloud API response received: {type(response)}\")\n        except Exception as e:\n            logger.error(f\"JDCloud API request failed: {e}\")\n            logger.error(f\"Request parameters: {kwargs}\")\n\n            # Log specific error details for HTTP errors\n            if hasattr(e, 'response') and hasattr(e.response, 'status_code'):\n                logger.error(f\"HTTP response status: {e.response.status_code}\")\n                logger.error(f\"HTTP response headers: {dict(e.response.headers)}\")\n\n                # Try to get the error response body\n                try:\n                    error_body = e.response.text\n                    logger.error(f\"HTTP response body: {error_body}\")\n\n                    # Try to parse JSON error details\n                    import json\n                    try:\n                        error_json = json.loads(error_body)\n                        logger.error(f\"Parsed error details: {error_json}\")\n                    except:\n                        pass\n                except Exception as body_error:\n                    logger.error(f\"Could not read response body: {body_error}\")\n\n            # Log the exception type and message\n            logger.error(f\"Exception type: {type(e)}\")\n            logger.error(f\"Exception message: {str(e)}\")\n\n            # For debugging, also log a simplified version of the request\n            safe_kwargs = kwargs.copy()\n            if 'messages' in safe_kwargs and len(safe_kwargs['messages']) > 0:\n                # Only log the first few messages to avoid too much log spam\n                safe_kwargs['messages'] = safe_kwargs['messages'][:2] + ['... (truncated)']\n            logger.error(f\"Simplified request: {safe_kwargs}\")\n\n            raise\n\n        # Normalize the response for compatibility\n        try:\n            normalized_response = normalize_jdcloud_response(response)\n            logger.debug(f\"Response normalization successful\")\n            return normalized_response\n        except Exception as e:\n            # If normalization fails, log and return original\n            logger.warning(f\"Failed to normalize JDCloud response: {e}\")\n            logger.warning(f\"Using original response format\")\n            return response\n\n    def __getattr__(self, name):\n        \"\"\"Delegate all other attributes.\"\"\"\n        return getattr(self.chat_completions, name)",
    "start_line": 149,
    "end_line": 222,
    "has_docstring": true,
    "docstring": "Wrapper for completions that normalizes responses.",
    "parameters": null,
    "node_type": "class",
    "base_classes": null,
    "class_name": null,
    "display_name": "class JDCloudCompletions",
    "component_id": "codewiki.src.be.jdcloud_adapter.JDCloudCompletions"
  },
  "codewiki.src.be.jdcloud_pydantic_ai.JDCloudCompatibleOpenAIProvider": {
    "id": "codewiki.src.be.jdcloud_pydantic_ai.JDCloudCompatibleOpenAIProvider",
    "name": "JDCloudCompatibleOpenAIProvider",
    "component_type": "class",
    "file_path": "/Users/wufagang/project/aiopen/CodeWiki/codewiki/src/be/jdcloud_pydantic_ai.py",
    "relative_path": "codewiki/src/be/jdcloud_pydantic_ai.py",
    "depends_on": [
      "codewiki.src.be.jdcloud_adapter.is_jdcloud_api",
      "codewiki.src.be.jdcloud_adapter.normalize_jdcloud_response"
    ],
    "source_code": "class JDCloudCompatibleOpenAIProvider(OpenAIProvider):\n    \"\"\"\n    JDCloud compatible OpenAI provider that normalizes responses.\n    \"\"\"\n\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self._is_jdcloud = is_jdcloud_api(self.base_url or \"\")\n\n    def _make_client(self) -> OpenAI:\n        \"\"\"Create OpenAI client with response patching.\"\"\"\n        client = super()._make_client()\n\n        if self._is_jdcloud:\n            # Store original methods\n            original_create = client.chat.completions.create\n\n            def patched_create(*args, **kwargs) -> ChatCompletion:\n                logger.debug(f\"JDCloud sync API call with args: {args}, kwargs: {kwargs}\")\n                try:\n                    response = original_create(*args, **kwargs)\n                    logger.debug(f\"Original response type: {type(response)}\")\n\n                    # Log original content for debugging\n                    if hasattr(response, 'choices') and response.choices:\n                        original_content = response.choices[0].message.content\n                        original_finish_reason = response.choices[0].finish_reason\n                        logger.debug(f\"Original content: {original_content} (type: {type(original_content)})\")\n                        logger.debug(f\"Original finish_reason: {original_finish_reason}\")\n\n                    normalized_response = normalize_jdcloud_response(response)\n                    logger.debug(f\"Normalized response type: {type(normalized_response)}\")\n\n                    if hasattr(normalized_response, 'choices') and normalized_response.choices:\n                        new_content = normalized_response.choices[0].message.content\n                        new_finish_reason = normalized_response.choices[0].finish_reason\n                        logger.debug(f\"Normalized content: {new_content} (type: {type(new_content)})\")\n                        logger.debug(f\"Normalized finish_reason: {new_finish_reason}\")\n\n                    return normalized_response\n                except Exception as e:\n                    logger.error(f\"Error in patched_create: {e}\")\n                    raise\n\n            # Replace the method\n            client.chat.completions.create = patched_create\n\n        return client\n\n    def _make_async_client(self) -> AsyncOpenAI:\n        \"\"\"Create async OpenAI client with response patching.\"\"\"\n        client = super()._make_async_client()\n\n        if self._is_jdcloud:\n            # Store original methods\n            original_create = client.chat.completions.create\n\n            async def patched_async_create(*args, **kwargs) -> ChatCompletion:\n                logger.debug(f\"JDCloud async API call with args: {args}, kwargs: {kwargs}\")\n                try:\n                    response = await original_create(*args, **kwargs)\n                    logger.debug(f\"Original async response type: {type(response)}\")\n\n                    normalized_response = normalize_jdcloud_response(response)\n                    logger.debug(f\"Normalized async response type: {type(normalized_response)}\")\n\n                    return normalized_response\n                except Exception as e:\n                    logger.error(f\"Error in patched_async_create: {e}\")\n                    raise\n\n            # Replace the method\n            client.chat.completions.create = patched_async_create\n\n        return client",
    "start_line": 21,
    "end_line": 95,
    "has_docstring": true,
    "docstring": "JDCloud compatible OpenAI provider that normalizes responses.",
    "parameters": null,
    "node_type": "class",
    "base_classes": [
      "OpenAIProvider"
    ],
    "class_name": null,
    "display_name": "class JDCloudCompatibleOpenAIProvider",
    "component_id": "codewiki.src.be.jdcloud_pydantic_ai.JDCloudCompatibleOpenAIProvider"
  },
  "codewiki.src.be.jdcloud_pydantic_ai.JDCloudCompatibleOpenAIModel": {
    "id": "codewiki.src.be.jdcloud_pydantic_ai.JDCloudCompatibleOpenAIModel",
    "name": "JDCloudCompatibleOpenAIModel",
    "component_type": "class",
    "file_path": "/Users/wufagang/project/aiopen/CodeWiki/codewiki/src/be/jdcloud_pydantic_ai.py",
    "relative_path": "codewiki/src/be/jdcloud_pydantic_ai.py",
    "depends_on": [
      "codewiki.src.be.jdcloud_pydantic_ai.JDCloudCompatibleOpenAIProvider"
    ],
    "source_code": "class JDCloudCompatibleOpenAIModel(OpenAIModel):\n    \"\"\"\n    JDCloud compatible OpenAI model that uses the compatible provider.\n    \"\"\"\n\n    def __init__(\n        self,\n        model_name: str,\n        *,\n        provider: Optional[OpenAIProvider] = None,\n        base_url: Optional[str] = None,\n        api_key: Optional[str] = None,\n        settings: Optional[OpenAIModelSettings] = None,\n        **kwargs\n    ):\n        # If no provider is given, create a JDCloud compatible one\n        if provider is None:\n            provider = JDCloudCompatibleOpenAIProvider(\n                base_url=base_url,\n                api_key=api_key\n            )\n\n        super().__init__(\n            model_name=model_name,\n            provider=provider,\n            settings=settings,\n            **kwargs\n        )",
    "start_line": 98,
    "end_line": 125,
    "has_docstring": true,
    "docstring": "JDCloud compatible OpenAI model that uses the compatible provider.",
    "parameters": null,
    "node_type": "class",
    "base_classes": [
      "OpenAIModel"
    ],
    "class_name": null,
    "display_name": "class JDCloudCompatibleOpenAIModel",
    "component_id": "codewiki.src.be.jdcloud_pydantic_ai.JDCloudCompatibleOpenAIModel"
  },
  "codewiki.src.be.jdcloud_pydantic_ai.create_jdcloud_compatible_model": {
    "id": "codewiki.src.be.jdcloud_pydantic_ai.create_jdcloud_compatible_model",
    "name": "create_jdcloud_compatible_model",
    "component_type": "function",
    "file_path": "/Users/wufagang/project/aiopen/CodeWiki/codewiki/src/be/jdcloud_pydantic_ai.py",
    "relative_path": "codewiki/src/be/jdcloud_pydantic_ai.py",
    "depends_on": [
      "codewiki.src.be.jdcloud_pydantic_ai.JDCloudCompatibleOpenAIModel"
    ],
    "source_code": "def create_jdcloud_compatible_model(\n    model_name: str,\n    base_url: str,\n    api_key: str,\n    settings: Optional[OpenAIModelSettings] = None\n) -> JDCloudCompatibleOpenAIModel:\n    \"\"\"\n    Create a JDCloud compatible model.\n\n    Args:\n        model_name: Name of the model\n        base_url: API base URL\n        api_key: API key\n        settings: Model settings\n\n    Returns:\n        JDCloud compatible model\n    \"\"\"\n    return JDCloudCompatibleOpenAIModel(\n        model_name=model_name,\n        base_url=base_url,\n        api_key=api_key,\n        settings=settings or OpenAIModelSettings(\n            temperature=0.0,\n            max_tokens=32768\n        )\n    )",
    "start_line": 128,
    "end_line": 154,
    "has_docstring": true,
    "docstring": "Create a JDCloud compatible model.\n\nArgs:\n    model_name: Name of the model\n    base_url: API base URL\n    api_key: API key\n    settings: Model settings\n\nReturns:\n    JDCloud compatible model",
    "parameters": [
      "model_name",
      "base_url",
      "api_key",
      "settings"
    ],
    "node_type": "function",
    "base_classes": null,
    "class_name": null,
    "display_name": "function create_jdcloud_compatible_model",
    "component_id": "codewiki.src.be.jdcloud_pydantic_ai.create_jdcloud_compatible_model"
  },
  "codewiki.src.be.jdcloud_pydantic_ai.create_jdcloud_compatible_fallback_models": {
    "id": "codewiki.src.be.jdcloud_pydantic_ai.create_jdcloud_compatible_fallback_models",
    "name": "create_jdcloud_compatible_fallback_models",
    "component_type": "function",
    "file_path": "/Users/wufagang/project/aiopen/CodeWiki/codewiki/src/be/jdcloud_pydantic_ai.py",
    "relative_path": "codewiki/src/be/jdcloud_pydantic_ai.py",
    "depends_on": [
      "codewiki.src.be.jdcloud_pydantic_ai.create_jdcloud_compatible_model"
    ],
    "source_code": "def create_jdcloud_compatible_fallback_models(\n    main_model: str,\n    fallback_model: str,\n    base_url: str,\n    api_key: str\n) -> FallbackModel:\n    \"\"\"\n    Create JDCloud compatible fallback models.\n\n    Args:\n        main_model: Main model name\n        fallback_model: Fallback model name\n        base_url: API base URL\n        api_key: API key\n\n    Returns:\n        Fallback model configuration\n    \"\"\"\n    settings = OpenAIModelSettings(\n        temperature=0.0,\n        max_tokens=32768\n    )\n\n    main = create_jdcloud_compatible_model(\n        model_name=main_model,\n        base_url=base_url,\n        api_key=api_key,\n        settings=settings\n    )\n\n    fallback = create_jdcloud_compatible_model(\n        model_name=fallback_model,\n        base_url=base_url,\n        api_key=api_key,\n        settings=settings\n    )\n\n    return FallbackModel(main, fallback)",
    "start_line": 157,
    "end_line": 194,
    "has_docstring": true,
    "docstring": "Create JDCloud compatible fallback models.\n\nArgs:\n    main_model: Main model name\n    fallback_model: Fallback model name\n    base_url: API base URL\n    api_key: API key\n\nReturns:\n    Fallback model configuration",
    "parameters": [
      "main_model",
      "fallback_model",
      "base_url",
      "api_key"
    ],
    "node_type": "function",
    "base_classes": null,
    "class_name": null,
    "display_name": "function create_jdcloud_compatible_fallback_models",
    "component_id": "codewiki.src.be.jdcloud_pydantic_ai.create_jdcloud_compatible_fallback_models"
  },
  "codewiki.src.be.jdcloud_pydantic_ai_v2.JDCloudCompatibleOpenAIModel": {
    "id": "codewiki.src.be.jdcloud_pydantic_ai_v2.JDCloudCompatibleOpenAIModel",
    "name": "JDCloudCompatibleOpenAIModel",
    "component_type": "class",
    "file_path": "/Users/wufagang/project/aiopen/CodeWiki/codewiki/src/be/jdcloud_pydantic_ai_v2.py",
    "relative_path": "codewiki/src/be/jdcloud_pydantic_ai_v2.py",
    "depends_on": [
      "codewiki.src.be.jdcloud_adapter.is_jdcloud_api",
      "codewiki.src.be.jdcloud_adapter.normalize_jdcloud_response"
    ],
    "source_code": "class JDCloudCompatibleOpenAIModel(OpenAIModel):\n    \"\"\"\n    JDCloud compatible OpenAI model that normalizes responses at the processing level.\n    \"\"\"\n\n    def __init__(\n        self,\n        model_name: str,\n        *,\n        base_url: Optional[str] = None,\n        api_key: Optional[str] = None,\n        settings: Optional[OpenAIModelSettings] = None,\n        **kwargs\n    ):\n        # Create a standard OpenAI provider\n        provider = OpenAIProvider(\n            base_url=base_url,\n            api_key=api_key\n        )\n\n        super().__init__(\n            model_name=model_name,\n            provider=provider,\n            settings=settings,\n            **kwargs\n        )\n\n        # Store whether this is a JDCloud API\n        self._is_jdcloud = is_jdcloud_api(base_url or \"\")\n\n    def _process_response(self, response: chat.ChatCompletion) -> messages.ModelResponse:\n        \"\"\"\n        Override response processing to normalize JDCloud format.\n        \"\"\"\n        if self._is_jdcloud:\n            logger.debug(f\"JDCloud response processing - Original: {response}\")\n            try:\n                # Normalize the response before processing\n                normalized_response = normalize_jdcloud_response(response)\n                logger.debug(f\"JDCloud response processing - Normalized: {normalized_response}\")\n\n                # Call parent with normalized response\n                return super()._process_response(normalized_response)\n            except Exception as e:\n                logger.error(f\"Error normalizing JDCloud response: {e}\")\n                # Fallback to original processing\n                return super()._process_response(response)\n        else:\n            # Standard processing for non-JDCloud APIs\n            return super()._process_response(response)\n\n    def _validate_completion(self, response: chat.ChatCompletion) -> chat.ChatCompletion:\n        \"\"\"\n        Override completion validation to normalize JDCloud format first.\n        \"\"\"\n        if self._is_jdcloud:\n            logger.debug(f\"JDCloud validation - Original: {response}\")\n            try:\n                # Normalize before validation\n                normalized_response = normalize_jdcloud_response(response)\n                logger.debug(f\"JDCloud validation - Normalized: {normalized_response}\")\n                return normalized_response\n            except Exception as e:\n                logger.error(f\"Error in JDCloud validation: {e}\")\n                # Fallback to original\n                return response\n        else:\n            # Standard validation for non-JDCloud APIs\n            return response",
    "start_line": 20,
    "end_line": 88,
    "has_docstring": true,
    "docstring": "JDCloud compatible OpenAI model that normalizes responses at the processing level.",
    "parameters": null,
    "node_type": "class",
    "base_classes": [
      "OpenAIModel"
    ],
    "class_name": null,
    "display_name": "class JDCloudCompatibleOpenAIModel",
    "component_id": "codewiki.src.be.jdcloud_pydantic_ai_v2.JDCloudCompatibleOpenAIModel"
  },
  "codewiki.src.be.jdcloud_pydantic_ai_v2.create_jdcloud_compatible_model": {
    "id": "codewiki.src.be.jdcloud_pydantic_ai_v2.create_jdcloud_compatible_model",
    "name": "create_jdcloud_compatible_model",
    "component_type": "function",
    "file_path": "/Users/wufagang/project/aiopen/CodeWiki/codewiki/src/be/jdcloud_pydantic_ai_v2.py",
    "relative_path": "codewiki/src/be/jdcloud_pydantic_ai_v2.py",
    "depends_on": [
      "codewiki.src.be.jdcloud_pydantic_ai_v2.JDCloudCompatibleOpenAIModel"
    ],
    "source_code": "def create_jdcloud_compatible_model(\n    model_name: str,\n    base_url: str,\n    api_key: str,\n    settings: Optional[OpenAIModelSettings] = None\n) -> JDCloudCompatibleOpenAIModel:\n    \"\"\"\n    Create a JDCloud compatible model.\n\n    Args:\n        model_name: Name of the model\n        base_url: API base URL\n        api_key: API key\n        settings: Model settings\n\n    Returns:\n        JDCloud compatible model\n    \"\"\"\n    return JDCloudCompatibleOpenAIModel(\n        model_name=model_name,\n        base_url=base_url,\n        api_key=api_key,\n        settings=settings or OpenAIModelSettings(\n            temperature=0.0,\n            max_tokens=32768\n        )\n    )",
    "start_line": 91,
    "end_line": 117,
    "has_docstring": true,
    "docstring": "Create a JDCloud compatible model.\n\nArgs:\n    model_name: Name of the model\n    base_url: API base URL\n    api_key: API key\n    settings: Model settings\n\nReturns:\n    JDCloud compatible model",
    "parameters": [
      "model_name",
      "base_url",
      "api_key",
      "settings"
    ],
    "node_type": "function",
    "base_classes": null,
    "class_name": null,
    "display_name": "function create_jdcloud_compatible_model",
    "component_id": "codewiki.src.be.jdcloud_pydantic_ai_v2.create_jdcloud_compatible_model"
  },
  "codewiki.src.be.jdcloud_pydantic_ai_v2.create_jdcloud_compatible_fallback_models": {
    "id": "codewiki.src.be.jdcloud_pydantic_ai_v2.create_jdcloud_compatible_fallback_models",
    "name": "create_jdcloud_compatible_fallback_models",
    "component_type": "function",
    "file_path": "/Users/wufagang/project/aiopen/CodeWiki/codewiki/src/be/jdcloud_pydantic_ai_v2.py",
    "relative_path": "codewiki/src/be/jdcloud_pydantic_ai_v2.py",
    "depends_on": [
      "codewiki.src.be.jdcloud_pydantic_ai_v2.create_jdcloud_compatible_model"
    ],
    "source_code": "def create_jdcloud_compatible_fallback_models(\n    main_model: str,\n    fallback_model: str,\n    base_url: str,\n    api_key: str\n) -> FallbackModel:\n    \"\"\"\n    Create JDCloud compatible fallback models.\n\n    Args:\n        main_model: Main model name\n        fallback_model: Fallback model name\n        base_url: API base URL\n        api_key: API key\n\n    Returns:\n        Fallback model configuration\n    \"\"\"\n    settings = OpenAIModelSettings(\n        temperature=0.0,\n        max_tokens=32768\n    )\n\n    main = create_jdcloud_compatible_model(\n        model_name=main_model,\n        base_url=base_url,\n        api_key=api_key,\n        settings=settings\n    )\n\n    fallback = create_jdcloud_compatible_model(\n        model_name=fallback_model,\n        base_url=base_url,\n        api_key=api_key,\n        settings=settings\n    )\n\n    return FallbackModel(main, fallback)",
    "start_line": 120,
    "end_line": 157,
    "has_docstring": true,
    "docstring": "Create JDCloud compatible fallback models.\n\nArgs:\n    main_model: Main model name\n    fallback_model: Fallback model name\n    base_url: API base URL\n    api_key: API key\n\nReturns:\n    Fallback model configuration",
    "parameters": [
      "main_model",
      "fallback_model",
      "base_url",
      "api_key"
    ],
    "node_type": "function",
    "base_classes": null,
    "class_name": null,
    "display_name": "function create_jdcloud_compatible_fallback_models",
    "component_id": "codewiki.src.be.jdcloud_pydantic_ai_v2.create_jdcloud_compatible_fallback_models"
  },
  "codewiki.src.be.llm_services.create_main_model": {
    "id": "codewiki.src.be.llm_services.create_main_model",
    "name": "create_main_model",
    "component_type": "function",
    "file_path": "/Users/wufagang/project/aiopen/CodeWiki/codewiki/src/be/llm_services.py",
    "relative_path": "codewiki/src/be/llm_services.py",
    "depends_on": [
      "codewiki.src.be.jdcloud_adapter.is_jdcloud_api",
      "codewiki.src.be.jdcloud_pydantic_ai_v2.create_jdcloud_compatible_model",
      "codewiki.cli.utils.errors.info"
    ],
    "source_code": "def create_main_model(config: Config) -> OpenAIModel:\n    \"\"\"Create the main LLM model from configuration.\"\"\"\n\n    logging.info(f\"Creating OpenAI client with base URL: {config.llm_base_url}\")\n    logging.info(f\"Using model: {config.main_model}\")\n    logging.info(f\"Using fallback model: {config.fallback_model}\")\n    logging.info(\n        f\"Using API key: {config.llm_api_key[:8]}...{config.llm_api_key[-4:] if len(config.llm_api_key) > 8 else '***'}\")\n    if is_jdcloud_api(config.llm_base_url):\n        return create_jdcloud_compatible_model(\n            model_name=config.main_model,\n            base_url=config.llm_base_url,\n            api_key=config.llm_api_key,\n            settings=OpenAIModelSettings(\n                temperature=0.0,\n                max_tokens=32768\n            )\n        )\n\n    return OpenAIModel(\n        model_name=config.main_model,\n        provider=OpenAIProvider(\n            base_url=config.llm_base_url,\n            api_key=config.llm_api_key\n        ),\n        settings=OpenAIModelSettings(\n            temperature=0.0,\n            max_tokens=32768\n        )\n    )",
    "start_line": 20,
    "end_line": 49,
    "has_docstring": true,
    "docstring": "Create the main LLM model from configuration.",
    "parameters": [
      "config"
    ],
    "node_type": "function",
    "base_classes": null,
    "class_name": null,
    "display_name": "function create_main_model",
    "component_id": "codewiki.src.be.llm_services.create_main_model"
  },
  "codewiki.src.be.llm_services.create_fallback_model": {
    "id": "codewiki.src.be.llm_services.create_fallback_model",
    "name": "create_fallback_model",
    "component_type": "function",
    "file_path": "/Users/wufagang/project/aiopen/CodeWiki/codewiki/src/be/llm_services.py",
    "relative_path": "codewiki/src/be/llm_services.py",
    "depends_on": [
      "codewiki.src.be.jdcloud_adapter.is_jdcloud_api",
      "codewiki.src.be.jdcloud_pydantic_ai_v2.create_jdcloud_compatible_model"
    ],
    "source_code": "def create_fallback_model(config: Config) -> OpenAIModel:\n    \"\"\"Create the fallback LLM model from configuration.\"\"\"\n    if is_jdcloud_api(config.llm_base_url):\n        return create_jdcloud_compatible_model(\n            model_name=config.fallback_model,\n            base_url=config.llm_base_url,\n            api_key=config.llm_api_key,\n            settings=OpenAIModelSettings(\n                temperature=0.0,\n                max_tokens=32768\n            )\n        )\n\n    return OpenAIModel(\n        model_name=config.fallback_model,\n        provider=OpenAIProvider(\n            base_url=config.llm_base_url,\n            api_key=config.llm_api_key\n        ),\n        settings=OpenAIModelSettings(\n            temperature=0.0,\n            max_tokens=32768\n        )\n    )",
    "start_line": 52,
    "end_line": 75,
    "has_docstring": true,
    "docstring": "Create the fallback LLM model from configuration.",
    "parameters": [
      "config"
    ],
    "node_type": "function",
    "base_classes": null,
    "class_name": null,
    "display_name": "function create_fallback_model",
    "component_id": "codewiki.src.be.llm_services.create_fallback_model"
  },
  "codewiki.src.be.llm_services.create_fallback_models": {
    "id": "codewiki.src.be.llm_services.create_fallback_models",
    "name": "create_fallback_models",
    "component_type": "function",
    "file_path": "/Users/wufagang/project/aiopen/CodeWiki/codewiki/src/be/llm_services.py",
    "relative_path": "codewiki/src/be/llm_services.py",
    "depends_on": [
      "codewiki.src.be.jdcloud_adapter.is_jdcloud_api",
      "codewiki.src.be.llm_services.create_fallback_model",
      "codewiki.src.be.jdcloud_pydantic_ai_v2.create_jdcloud_compatible_fallback_models",
      "codewiki.src.be.llm_services.create_main_model"
    ],
    "source_code": "def create_fallback_models(config: Config) -> FallbackModel:\n    \"\"\"Create fallback models chain from configuration.\"\"\"\n    if is_jdcloud_api(config.llm_base_url):\n        return create_jdcloud_compatible_fallback_models(\n            main_model=config.main_model,\n            fallback_model=config.fallback_model,\n            base_url=config.llm_base_url,\n            api_key=config.llm_api_key\n        )\n\n    main = create_main_model(config)\n    fallback = create_fallback_model(config)\n    return FallbackModel(main, fallback)",
    "start_line": 78,
    "end_line": 90,
    "has_docstring": true,
    "docstring": "Create fallback models chain from configuration.",
    "parameters": [
      "config"
    ],
    "node_type": "function",
    "base_classes": null,
    "class_name": null,
    "display_name": "function create_fallback_models",
    "component_id": "codewiki.src.be.llm_services.create_fallback_models"
  },
  "codewiki.src.be.llm_services.create_openai_client": {
    "id": "codewiki.src.be.llm_services.create_openai_client",
    "name": "create_openai_client",
    "component_type": "function",
    "file_path": "/Users/wufagang/project/aiopen/CodeWiki/codewiki/src/be/llm_services.py",
    "relative_path": "codewiki/src/be/llm_services.py",
    "depends_on": [
      "codewiki.src.be.jdcloud_adapter.is_jdcloud_api",
      "codewiki.src.be.jdcloud_adapter.JDCloudOpenAIClient"
    ],
    "source_code": "def create_openai_client(config: Config) -> OpenAI:\n    \"\"\"Create OpenAI client from configuration.\"\"\"\n    client = OpenAI(\n        base_url=config.llm_base_url,\n        api_key=config.llm_api_key\n    )\n\n    # Wrap with JDCloud adapter if using JDCloud API\n    if is_jdcloud_api(config.llm_base_url):\n        return JDCloudOpenAIClient(client)\n\n    return client",
    "start_line": 93,
    "end_line": 104,
    "has_docstring": true,
    "docstring": "Create OpenAI client from configuration.",
    "parameters": [
      "config"
    ],
    "node_type": "function",
    "base_classes": null,
    "class_name": null,
    "display_name": "function create_openai_client",
    "component_id": "codewiki.src.be.llm_services.create_openai_client"
  },
  "codewiki.src.be.llm_services.call_llm": {
    "id": "codewiki.src.be.llm_services.call_llm",
    "name": "call_llm",
    "component_type": "function",
    "file_path": "/Users/wufagang/project/aiopen/CodeWiki/codewiki/src/be/llm_services.py",
    "relative_path": "codewiki/src/be/llm_services.py",
    "depends_on": [
      "codewiki.src.be.jdcloud_adapter.is_jdcloud_api",
      "codewiki.src.be.jdcloud_adapter.extract_content_from_jdcloud_response",
      "codewiki.src.be.llm_services.create_openai_client"
    ],
    "source_code": "def call_llm(\n    prompt: str,\n    config: Config,\n    model: str = None,\n    temperature: float = 0.0\n) -> str:\n    \"\"\"\n    Call LLM with the given prompt.\n\n    Args:\n        prompt: The prompt to send\n        config: Configuration containing LLM settings\n        model: Model name (defaults to config.main_model)\n        temperature: Temperature setting\n\n    Returns:\n        LLM response text\n    \"\"\"\n    if model is None:\n        model = config.main_model\n\n    client = create_openai_client(config)\n    response = client.chat.completions.create(\n        model=model,\n        messages=[{\"role\": \"user\", \"content\": prompt}],\n        temperature=temperature,\n        max_tokens=32768\n    )\n\n    # Handle both JDCloud (list) and OpenAI (string) content formats\n    if is_jdcloud_api(config.llm_base_url):\n        return extract_content_from_jdcloud_response(response)\n    else:\n        return response.choices[0].message.content",
    "start_line": 107,
    "end_line": 140,
    "has_docstring": true,
    "docstring": "Call LLM with the given prompt.\n\nArgs:\n    prompt: The prompt to send\n    config: Configuration containing LLM settings\n    model: Model name (defaults to config.main_model)\n    temperature: Temperature setting\n\nReturns:\n    LLM response text",
    "parameters": [
      "prompt",
      "config",
      "model",
      "temperature"
    ],
    "node_type": "function",
    "base_classes": null,
    "class_name": null,
    "display_name": "function call_llm",
    "component_id": "codewiki.src.be.llm_services.call_llm"
  },
  "codewiki.src.be.main.parse_arguments": {
    "id": "codewiki.src.be.main.parse_arguments",
    "name": "parse_arguments",
    "component_type": "function",
    "file_path": "/Users/wufagang/project/aiopen/CodeWiki/codewiki/src/be/main.py",
    "relative_path": "codewiki/src/be/main.py",
    "depends_on": [],
    "source_code": "def parse_arguments() -> argparse.Namespace:\n    \"\"\"Parse command line arguments.\"\"\"\n    parser = argparse.ArgumentParser(\n        description='Generate comprehensive documentation for Python components in dependency order.'\n    )\n    parser.add_argument(\n        '--repo-path',\n        type=str,\n        required=True,\n        help='Path to the repository'\n    )\n    \n    return parser.parse_args()",
    "start_line": 31,
    "end_line": 43,
    "has_docstring": true,
    "docstring": "Parse command line arguments.",
    "parameters": [],
    "node_type": "function",
    "base_classes": null,
    "class_name": null,
    "display_name": "function parse_arguments",
    "component_id": "codewiki.src.be.main.parse_arguments"
  },
  "codewiki.src.be.main.main": {
    "id": "codewiki.src.be.main.main",
    "name": "main",
    "component_type": "function",
    "file_path": "/Users/wufagang/project/aiopen/CodeWiki/codewiki/src/be/main.py",
    "relative_path": "codewiki/src/be/main.py",
    "depends_on": [
      "codewiki.src.be.main.parse_arguments",
      "codewiki.src.be.documentation_generator.DocumentationGenerator"
    ],
    "source_code": "async def main() -> None:\n    \"\"\"Main entry point for the documentation generation process.\"\"\"\n    try:\n        # Parse arguments and create configuration\n        args = parse_arguments()\n        config = Config.from_args(args)\n        \n        # Create and run documentation generator\n        doc_generator = DocumentationGenerator(config)\n        await doc_generator.run()\n        \n    except KeyboardInterrupt:\n        logger.debug(\"Documentation generation interrupted by user\")\n    except Exception as e:\n        logger.error(f\"Unexpected error: {str(e)}\")\n        logger.error(f\"Traceback: {traceback.format_exc()}\")\n        raise",
    "start_line": 46,
    "end_line": 62,
    "has_docstring": true,
    "docstring": "Main entry point for the documentation generation process.",
    "parameters": [],
    "node_type": "function",
    "base_classes": null,
    "class_name": null,
    "display_name": "function main",
    "component_id": "codewiki.src.be.main.main"
  },
  "codewiki.src.be.prompt_template.format_user_prompt": {
    "id": "codewiki.src.be.prompt_template.format_user_prompt",
    "name": "format_user_prompt",
    "component_type": "function",
    "file_path": "/Users/wufagang/project/aiopen/CodeWiki/codewiki/src/be/prompt_template.py",
    "relative_path": "codewiki/src/be/prompt_template.py",
    "depends_on": [],
    "source_code": "def format_user_prompt(module_name: str, core_component_ids: list[str], components: Dict[str, Any], module_tree: dict[str, any]) -> str:\n    \"\"\"\n    Format the user prompt with module name and organized core component codes.\n    \n    Args:\n        module_name: Name of the module to document\n        core_component_ids: List of component IDs to include\n        components: Dictionary mapping component IDs to CodeComponent objects\n    \n    Returns:\n        Formatted user prompt string\n    \"\"\"\n\n    # format module tree\n    lines = []\n    \n    def _format_module_tree(module_tree: dict[str, any], indent: int = 0):\n        for key, value in module_tree.items():\n            if key == module_name:\n                lines.append(f\"{'  ' * indent}{key} (current module)\")\n            else:\n                lines.append(f\"{'  ' * indent}{key}\")\n            \n            lines.append(f\"{'  ' * (indent + 1)} Core components: {', '.join(value['components'])}\")\n            if isinstance(value[\"children\"], dict) and len(value[\"children\"]) > 0:\n                lines.append(f\"{'  ' * (indent + 1)} Children:\")\n                _format_module_tree(value[\"children\"], indent + 2)\n    \n    _format_module_tree(module_tree, 0)\n    formatted_module_tree = \"\\n\".join(lines)\n\n    # print(f\"Formatted module tree:\\n{formatted_module_tree}\")\n\n    # Group core component IDs by their file path\n    grouped_components: dict[str, list[str]] = {}\n    for component_id in core_component_ids:\n        if component_id not in components:\n            continue\n        component = components[component_id]\n        path = component.relative_path\n        if path not in grouped_components:\n            grouped_components[path] = []\n        grouped_components[path].append(component_id)\n\n    core_component_codes = \"\"\n    for path, component_ids_in_file in grouped_components.items():\n        core_component_codes += f\"# File: {path}\\n\\n\"\n        core_component_codes += f\"## Core Components in this file:\\n\"\n        \n        for component_id in component_ids_in_file:\n            core_component_codes += f\"- {component_id}\\n\"\n        \n        core_component_codes += f\"\\n## File Content:\\n```{EXTENSION_TO_LANGUAGE['.'+path.split('.')[-1]]}\\n\"\n        \n        # Read content of the file using the first component's file path\n        try:\n            core_component_codes += file_manager.load_text(components[component_ids_in_file[0]].file_path)\n        except (FileNotFoundError, IOError) as e:\n            core_component_codes += f\"# Error reading file: {e}\\n\"\n        \n        core_component_codes += \"```\\n\\n\"\n        \n    return USER_PROMPT.format(module_name=module_name, formatted_core_component_codes=core_component_codes, module_tree=formatted_module_tree)",
    "start_line": 259,
    "end_line": 321,
    "has_docstring": true,
    "docstring": "Format the user prompt with module name and organized core component codes.\n\nArgs:\n    module_name: Name of the module to document\n    core_component_ids: List of component IDs to include\n    components: Dictionary mapping component IDs to CodeComponent objects\n\nReturns:\n    Formatted user prompt string",
    "parameters": [
      "module_name",
      "core_component_ids",
      "components",
      "module_tree"
    ],
    "node_type": "function",
    "base_classes": null,
    "class_name": null,
    "display_name": "function format_user_prompt",
    "component_id": "codewiki.src.be.prompt_template.format_user_prompt"
  },
  "codewiki.src.be.prompt_template._format_module_tree": {
    "id": "codewiki.src.be.prompt_template._format_module_tree",
    "name": "_format_module_tree",
    "component_type": "function",
    "file_path": "/Users/wufagang/project/aiopen/CodeWiki/codewiki/src/be/prompt_template.py",
    "relative_path": "codewiki/src/be/prompt_template.py",
    "depends_on": [
      "codewiki.src.be.prompt_template._format_module_tree"
    ],
    "source_code": "    def _format_module_tree(module_tree: dict[str, any], indent: int = 0):\n        for key, value in module_tree.items():\n            if key == module_name:\n                lines.append(f\"{'  ' * indent}{key} (current module)\")\n            else:\n                lines.append(f\"{'  ' * indent}{key}\")\n            \n            lines.append(f\"{'  ' * (indent + 1)} Core components: {', '.join(value['components'])}\")\n            if (\"children\" in value) and isinstance(value[\"children\"], dict) and len(value[\"children\"]) > 0:\n                lines.append(f\"{'  ' * (indent + 1)} Children:\")\n                _format_module_tree(value[\"children\"], indent + 2)",
    "start_line": 335,
    "end_line": 345,
    "has_docstring": false,
    "docstring": "",
    "parameters": [
      "module_tree",
      "indent"
    ],
    "node_type": "function",
    "base_classes": null,
    "class_name": null,
    "display_name": "function _format_module_tree",
    "component_id": "codewiki.src.be.prompt_template._format_module_tree"
  },
  "codewiki.src.be.prompt_template.format_cluster_prompt": {
    "id": "codewiki.src.be.prompt_template.format_cluster_prompt",
    "name": "format_cluster_prompt",
    "component_type": "function",
    "file_path": "/Users/wufagang/project/aiopen/CodeWiki/codewiki/src/be/prompt_template.py",
    "relative_path": "codewiki/src/be/prompt_template.py",
    "depends_on": [],
    "source_code": "def format_cluster_prompt(potential_core_components: str, module_tree: dict[str, any] = {}, module_name: str = None) -> str:\n    \"\"\"\n    Format the cluster prompt with potential core components and module tree.\n    \"\"\"\n\n    # format module tree\n    lines = []\n\n    # print(f\"Module tree:\\n{json.dumps(module_tree, indent=2)}\")\n    \n    def _format_module_tree(module_tree: dict[str, any], indent: int = 0):\n        for key, value in module_tree.items():\n            if key == module_name:\n                lines.append(f\"{'  ' * indent}{key} (current module)\")\n            else:\n                lines.append(f\"{'  ' * indent}{key}\")\n            \n            lines.append(f\"{'  ' * (indent + 1)} Core components: {', '.join(value['components'])}\")\n            if (\"children\" in value) and isinstance(value[\"children\"], dict) and len(value[\"children\"]) > 0:\n                lines.append(f\"{'  ' * (indent + 1)} Children:\")\n                _format_module_tree(value[\"children\"], indent + 2)\n    \n    _format_module_tree(module_tree, 0)\n    formatted_module_tree = \"\\n\".join(lines)\n\n\n    if module_tree == {}:\n        return CLUSTER_REPO_PROMPT.format(potential_core_components=potential_core_components)\n    else:\n        return CLUSTER_MODULE_PROMPT.format(potential_core_components=potential_core_components, module_tree=formatted_module_tree, module_name=module_name)",
    "start_line": 325,
    "end_line": 354,
    "has_docstring": true,
    "docstring": "Format the cluster prompt with potential core components and module tree.",
    "parameters": [
      "potential_core_components",
      "module_tree",
      "module_name"
    ],
    "node_type": "function",
    "base_classes": null,
    "class_name": null,
    "display_name": "function format_cluster_prompt",
    "component_id": "codewiki.src.be.prompt_template.format_cluster_prompt"
  },
  "codewiki.src.be.utils.is_complex_module": {
    "id": "codewiki.src.be.utils.is_complex_module",
    "name": "is_complex_module",
    "component_type": "function",
    "file_path": "/Users/wufagang/project/aiopen/CodeWiki/codewiki/src/be/utils.py",
    "relative_path": "codewiki/src/be/utils.py",
    "depends_on": [],
    "source_code": "def is_complex_module(components: dict[str, any], core_component_ids: list[str]) -> bool:\n    files = set()\n    for component_id in core_component_ids:\n        if component_id in components:\n            files.add(components[component_id].file_path)\n\n    result = len(files) > 1\n\n    return result",
    "start_line": 15,
    "end_line": 23,
    "has_docstring": false,
    "docstring": "",
    "parameters": [
      "components",
      "core_component_ids"
    ],
    "node_type": "function",
    "base_classes": null,
    "class_name": null,
    "display_name": "function is_complex_module",
    "component_id": "codewiki.src.be.utils.is_complex_module"
  },
  "codewiki.src.be.utils.count_tokens": {
    "id": "codewiki.src.be.utils.count_tokens",
    "name": "count_tokens",
    "component_type": "function",
    "file_path": "/Users/wufagang/project/aiopen/CodeWiki/codewiki/src/be/utils.py",
    "relative_path": "codewiki/src/be/utils.py",
    "depends_on": [],
    "source_code": "def count_tokens(text: str) -> int:\n    \"\"\"\n    Count the number of tokens in a text.\n    \"\"\"\n    length = len(enc.encode(text))\n    # logger.debug(f\"Number of tokens: {length}\")\n    return length",
    "start_line": 32,
    "end_line": 38,
    "has_docstring": true,
    "docstring": "Count the number of tokens in a text.",
    "parameters": [
      "text"
    ],
    "node_type": "function",
    "base_classes": null,
    "class_name": null,
    "display_name": "function count_tokens",
    "component_id": "codewiki.src.be.utils.count_tokens"
  },
  "codewiki.src.be.utils.validate_mermaid_diagrams": {
    "id": "codewiki.src.be.utils.validate_mermaid_diagrams",
    "name": "validate_mermaid_diagrams",
    "component_type": "function",
    "file_path": "/Users/wufagang/project/aiopen/CodeWiki/codewiki/src/be/utils.py",
    "relative_path": "codewiki/src/be/utils.py",
    "depends_on": [
      "codewiki.src.be.utils.validate_single_diagram",
      "codewiki.src.be.utils.extract_mermaid_blocks"
    ],
    "source_code": "async def validate_mermaid_diagrams(md_file_path: str, relative_path: str) -> str:\n    \"\"\"\n    Validate all Mermaid diagrams in a markdown file.\n    \n    Args:\n        md_file_path: Path to the markdown file to check\n        relative_path: Relative path to the markdown file\n    Returns:\n        \"All mermaid diagrams are syntax correct\" if all diagrams are valid,\n        otherwise returns error message with details about invalid diagrams\n    \"\"\"\n\n    try:\n        # Read the markdown file\n        file_path = Path(md_file_path)\n        if not file_path.exists():\n            return f\"Error: File '{md_file_path}' does not exist\"\n        \n        content = file_path.read_text(encoding='utf-8')\n        \n        # Extract all mermaid code blocks\n        mermaid_blocks = extract_mermaid_blocks(content)\n        \n        if not mermaid_blocks:\n            return \"No mermaid diagrams found in the file\"\n        \n        # Validate each mermaid diagram sequentially to avoid segfaults\n        errors = []\n        for i, (line_start, diagram_content) in enumerate(mermaid_blocks, 1):\n            error_msg = await validate_single_diagram(diagram_content, i, line_start)\n            if error_msg:\n                errors.append(\"\\n\")\n                errors.append(error_msg)\n        \n        # if errors:\n        #     logger.debug(f\"Mermaid syntax errors found in file: {md_file_path}: {errors}\")\n        \n        if errors:\n            return \"Mermaid syntax errors found in file: \" + relative_path + \"\\n\" + \"\\n\".join(errors)\n        else:\n            return \"All mermaid diagrams in file: \" + relative_path + \" are syntax correct\"\n            \n    except Exception as e:\n        return f\"Error processing file: {str(e)}\"",
    "start_line": 45,
    "end_line": 88,
    "has_docstring": true,
    "docstring": "Validate all Mermaid diagrams in a markdown file.\n\nArgs:\n    md_file_path: Path to the markdown file to check\n    relative_path: Relative path to the markdown file\nReturns:\n    \"All mermaid diagrams are syntax correct\" if all diagrams are valid,\n    otherwise returns error message with details about invalid diagrams",
    "parameters": [
      "md_file_path",
      "relative_path"
    ],
    "node_type": "function",
    "base_classes": null,
    "class_name": null,
    "display_name": "function validate_mermaid_diagrams",
    "component_id": "codewiki.src.be.utils.validate_mermaid_diagrams"
  },
  "codewiki.src.be.utils.extract_mermaid_blocks": {
    "id": "codewiki.src.be.utils.extract_mermaid_blocks",
    "name": "extract_mermaid_blocks",
    "component_type": "function",
    "file_path": "/Users/wufagang/project/aiopen/CodeWiki/codewiki/src/be/utils.py",
    "relative_path": "codewiki/src/be/utils.py",
    "depends_on": [],
    "source_code": "def extract_mermaid_blocks(content: str) -> List[Tuple[int, str]]:\n    \"\"\"\n    Extract all mermaid code blocks from markdown content.\n    \n    Returns:\n        List of tuples containing (line_number, diagram_content)\n    \"\"\"\n    mermaid_blocks = []\n    lines = content.split('\\n')\n    i = 0\n    \n    while i < len(lines):\n        line = lines[i].strip()\n        \n        # Look for mermaid code block start\n        if line == '```mermaid' or line.startswith('```mermaid'):\n            start_line = i + 1\n            diagram_lines = []\n            i += 1\n            \n            # Collect lines until we find the closing ```\n            while i < len(lines):\n                if lines[i].strip() == '```':\n                    break\n                diagram_lines.append(lines[i])\n                i += 1\n            \n            if diagram_lines:  # Only add non-empty diagrams\n                diagram_content = '\\n'.join(diagram_lines)\n                mermaid_blocks.append((start_line, diagram_content))\n        \n        i += 1\n    \n    return mermaid_blocks",
    "start_line": 91,
    "end_line": 124,
    "has_docstring": true,
    "docstring": "Extract all mermaid code blocks from markdown content.\n\nReturns:\n    List of tuples containing (line_number, diagram_content)",
    "parameters": [
      "content"
    ],
    "node_type": "function",
    "base_classes": null,
    "class_name": null,
    "display_name": "function extract_mermaid_blocks",
    "component_id": "codewiki.src.be.utils.extract_mermaid_blocks"
  },
  "codewiki.src.be.utils.validate_single_diagram": {
    "id": "codewiki.src.be.utils.validate_single_diagram",
    "name": "validate_single_diagram",
    "component_type": "function",
    "file_path": "/Users/wufagang/project/aiopen/CodeWiki/codewiki/src/be/utils.py",
    "relative_path": "codewiki/src/be/utils.py",
    "depends_on": [
      "codewiki.cli.utils.errors.warning"
    ],
    "source_code": "async def validate_single_diagram(diagram_content: str, diagram_num: int, line_start: int) -> str:\n    \"\"\"\n    Validate a single mermaid diagram.\n    \n    Args:\n        diagram_content: The mermaid diagram content\n        diagram_num: Diagram number for error reporting\n        line_start: Starting line number in the file\n        \n    Returns:\n        Error message if invalid, empty string if valid\n    \"\"\"\n    import sys\n    import os\n    from io import StringIO\n\n    core_error = \"\"\n    \n    try:\n        from mermaid_parser.parser import parse_mermaid_py\n        # logger.debug(\"Using mermaid-parser-py to validate mermaid diagrams\")\n    \n        try:\n            # Redirect stderr to suppress mermaid parser JavaScript errors\n            old_stderr = sys.stderr\n            sys.stderr = open(os.devnull, 'w')\n            \n            try:\n                json_output = await parse_mermaid_py(diagram_content)\n            finally:\n                # Restore stderr\n                sys.stderr.close()\n                sys.stderr = old_stderr\n        except Exception as e:\n            error_str = str(e)\n            \n            # Extract the core error information from the exception message\n            # Look for the pattern that contains \"Parse error on line X:\"\n            error_pattern = r\"Error:(.*?)(?=Stack Trace:|$)\"\n            match = re.search(error_pattern, error_str, re.DOTALL)\n            \n            if match:\n                core_error = match.group(0).strip()\n                core_error = core_error\n            else:\n                logger.error(f\"No match found for error pattern, fallback to mermaid-py\\n{error_str}\")\n                logger.error(f\"Traceback: {traceback.format_exc()}\")\n                raise Exception(error_str)\n\n    except Exception as e:\n        logger.warning(\"Using mermaid-py to validate mermaid diagrams\")\n        try:\n            import mermaid as md\n            # Create Mermaid object and check response\n            render = md.Mermaid(diagram_content)\n            core_error = render.svg_response.text\n            \n        except Exception as e:\n            return f\"  Diagram {diagram_num}: Exception during validation - {str(e)}\"\n\n    # Check if response indicates a parse error\n    if core_error:\n        # Extract line number from parse error and calculate actual line in markdown file\n        line_match = re.search(r'line (\\d+)', core_error)\n        if line_match:\n            error_line_in_diagram = int(line_match.group(1))\n            actual_line_in_file = line_start + error_line_in_diagram\n            newline = '\\n'\n            return f\"Diagram {diagram_num}: Parse error on line {actual_line_in_file}:{newline}{newline.join(core_error.split(newline)[1:])}\"\n        else:\n            return f\"Diagram {diagram_num}: {core_error}\"\n    \n    return \"\"  # No error",
    "start_line": 127,
    "end_line": 199,
    "has_docstring": true,
    "docstring": "Validate a single mermaid diagram.\n\nArgs:\n    diagram_content: The mermaid diagram content\n    diagram_num: Diagram number for error reporting\n    line_start: Starting line number in the file\n    \nReturns:\n    Error message if invalid, empty string if valid",
    "parameters": [
      "diagram_content",
      "diagram_num",
      "line_start"
    ],
    "node_type": "function",
    "base_classes": null,
    "class_name": null,
    "display_name": "function validate_single_diagram",
    "component_id": "codewiki.src.be.utils.validate_single_diagram"
  },
  "codewiki.src.config.set_cli_context": {
    "id": "codewiki.src.config.set_cli_context",
    "name": "set_cli_context",
    "component_type": "function",
    "file_path": "/Users/wufagang/project/aiopen/CodeWiki/codewiki/src/config.py",
    "relative_path": "codewiki/src/config.py",
    "depends_on": [],
    "source_code": "def set_cli_context(enabled: bool = True):\n    \"\"\"Set whether we're running in CLI context (vs web app).\"\"\"\n    global _CLI_CONTEXT\n    _CLI_CONTEXT = enabled",
    "start_line": 22,
    "end_line": 25,
    "has_docstring": true,
    "docstring": "Set whether we're running in CLI context (vs web app).",
    "parameters": [
      "enabled"
    ],
    "node_type": "function",
    "base_classes": null,
    "class_name": null,
    "display_name": "function set_cli_context",
    "component_id": "codewiki.src.config.set_cli_context"
  },
  "codewiki.src.config.is_cli_context": {
    "id": "codewiki.src.config.is_cli_context",
    "name": "is_cli_context",
    "component_type": "function",
    "file_path": "/Users/wufagang/project/aiopen/CodeWiki/codewiki/src/config.py",
    "relative_path": "codewiki/src/config.py",
    "depends_on": [],
    "source_code": "def is_cli_context() -> bool:\n    \"\"\"Check if running in CLI context.\"\"\"\n    return _CLI_CONTEXT",
    "start_line": 27,
    "end_line": 29,
    "has_docstring": true,
    "docstring": "Check if running in CLI context.",
    "parameters": [],
    "node_type": "function",
    "base_classes": null,
    "class_name": null,
    "display_name": "function is_cli_context",
    "component_id": "codewiki.src.config.is_cli_context"
  },
  "codewiki.src.config.Config": {
    "id": "codewiki.src.config.Config",
    "name": "Config",
    "component_type": "class",
    "file_path": "/Users/wufagang/project/aiopen/CodeWiki/codewiki/src/config.py",
    "relative_path": "codewiki/src/config.py",
    "depends_on": [],
    "source_code": "class Config:\n    \"\"\"Configuration class for CodeWiki.\"\"\"\n    repo_path: str\n    output_dir: str\n    dependency_graph_dir: str\n    docs_dir: str\n    max_depth: int\n    # LLM configuration\n    llm_base_url: str\n    llm_api_key: str\n    main_model: str\n    cluster_model: str\n    fallback_model: str = FALLBACK_MODEL_1\n    \n    @classmethod\n    def from_args(cls, args: argparse.Namespace) -> 'Config':\n        \"\"\"Create configuration from parsed arguments.\"\"\"\n        repo_name = os.path.basename(os.path.normpath(args.repo_path))\n        sanitized_repo_name = ''.join(c if c.isalnum() else '_' for c in repo_name)\n        \n        return cls(\n            repo_path=args.repo_path,\n            output_dir=OUTPUT_BASE_DIR,\n            dependency_graph_dir=os.path.join(OUTPUT_BASE_DIR, DEPENDENCY_GRAPHS_DIR),\n            docs_dir=os.path.join(OUTPUT_BASE_DIR, DOCS_DIR, f\"{sanitized_repo_name}-docs\"),\n            max_depth=MAX_DEPTH,\n            llm_base_url=LLM_BASE_URL,\n            llm_api_key=LLM_API_KEY,\n            main_model=MAIN_MODEL,\n            cluster_model=CLUSTER_MODEL,\n            fallback_model=FALLBACK_MODEL_1\n        )\n    \n    @classmethod\n    def from_cli(\n        cls,\n        repo_path: str,\n        output_dir: str,\n        llm_base_url: str,\n        llm_api_key: str,\n        main_model: str,\n        cluster_model: str,\n        fallback_model: str = FALLBACK_MODEL_1\n    ) -> 'Config':\n        \"\"\"\n        Create configuration for CLI context.\n        \n        Args:\n            repo_path: Repository path\n            output_dir: Output directory for generated docs\n            llm_base_url: LLM API base URL\n            llm_api_key: LLM API key\n            main_model: Primary model\n            cluster_model: Clustering model\n            fallback_model: Fallback model\n            \n        Returns:\n            Config instance\n        \"\"\"\n        repo_name = os.path.basename(os.path.normpath(repo_path))\n        base_output_dir = os.path.join(output_dir, \"temp\")\n        \n        return cls(\n            repo_path=repo_path,\n            output_dir=base_output_dir,\n            dependency_graph_dir=os.path.join(base_output_dir, DEPENDENCY_GRAPHS_DIR),\n            docs_dir=output_dir,\n            max_depth=MAX_DEPTH,\n            llm_base_url=llm_base_url,\n            llm_api_key=llm_api_key,\n            main_model=main_model,\n            cluster_model=cluster_model,\n            fallback_model=fallback_model\n        )",
    "start_line": 41,
    "end_line": 114,
    "has_docstring": true,
    "docstring": "Configuration class for CodeWiki.",
    "parameters": null,
    "node_type": "class",
    "base_classes": null,
    "class_name": null,
    "display_name": "class Config",
    "component_id": "codewiki.src.config.Config"
  },
  "codewiki.src.fe.background_worker.BackgroundWorker": {
    "id": "codewiki.src.fe.background_worker.BackgroundWorker",
    "name": "BackgroundWorker",
    "component_type": "class",
    "file_path": "/Users/wufagang/project/aiopen/CodeWiki/codewiki/src/fe/background_worker.py",
    "relative_path": "codewiki/src/fe/background_worker.py",
    "depends_on": [
      "codewiki.src.fe.models.JobStatus",
      "codewiki.src.be.dependency_analyzer.analysis.cloning.clone_repository",
      "codewiki.src.be.documentation_generator.DocumentationGenerator"
    ],
    "source_code": "class BackgroundWorker:\n    \"\"\"Background worker for processing documentation generation jobs.\"\"\"\n    \n    def __init__(self, cache_manager: CacheManager, temp_dir: str = None):\n        self.cache_manager = cache_manager\n        self.temp_dir = temp_dir or WebAppConfig.TEMP_DIR\n        self.running = False\n        self.processing_queue = Queue(maxsize=WebAppConfig.QUEUE_SIZE)\n        self.job_status: Dict[str, JobStatus] = {}\n        self.jobs_file = Path(WebAppConfig.CACHE_DIR) / \"jobs.json\"\n        self.load_job_statuses()\n    \n    def start(self):\n        \"\"\"Start the background worker thread.\"\"\"\n        if not self.running:\n            self.running = True\n            thread = threading.Thread(target=self._worker_loop, daemon=True)\n            thread.start()\n            print(\"Background worker started\")\n    \n    def stop(self):\n        \"\"\"Stop the background worker.\"\"\"\n        self.running = False\n    \n    def add_job(self, job_id: str, job: JobStatus):\n        \"\"\"Add a job to the processing queue.\"\"\"\n        self.job_status[job_id] = job\n        self.processing_queue.put(job_id)\n    \n    def get_job_status(self, job_id: str) -> JobStatus:\n        \"\"\"Get job status by ID.\"\"\"\n        return self.job_status.get(job_id)\n    \n    def get_all_jobs(self) -> Dict[str, JobStatus]:\n        \"\"\"Get all job statuses.\"\"\"\n        return self.job_status\n    \n    def load_job_statuses(self):\n        \"\"\"Load job statuses from disk.\"\"\"\n        if not self.jobs_file.exists():\n            # Try to reconstruct from cache if no job file exists\n            self._reconstruct_jobs_from_cache()\n            return\n        \n        try:\n            data = file_manager.load_json(self.jobs_file)\n                \n            for job_id, job_data in data.items():\n                # Only load completed jobs to avoid inconsistent state\n                if job_data.get('status') == 'completed':\n                    self.job_status[job_id] = JobStatus(\n                        job_id=job_data['job_id'],\n                        repo_url=job_data['repo_url'],\n                        status=job_data['status'],\n                        created_at=datetime.fromisoformat(job_data['created_at']),\n                        started_at=datetime.fromisoformat(job_data['started_at']) if job_data.get('started_at') else None,\n                        completed_at=datetime.fromisoformat(job_data['completed_at']) if job_data.get('completed_at') else None,\n                        error_message=job_data.get('error_message'),\n                        progress=job_data.get('progress', ''),\n                        docs_path=job_data.get('docs_path')\n                    )\n            print(f\"Loaded {len([j for j in self.job_status.values() if j.status == 'completed'])} completed jobs from disk\")\n        except Exception as e:\n            print(f\"Error loading job statuses: {e}\")\n    \n    def _reconstruct_jobs_from_cache(self):\n        \"\"\"Reconstruct job statuses from cache entries for backward compatibility.\"\"\"\n        try:\n            cache_entries = self.cache_manager.cache_index\n            reconstructed_count = 0\n            \n            for repo_hash, cache_entry in cache_entries.items():\n                # Extract repo info to create job_id\n                from .repository_processors import RepositoryManager\n                try:\n                    repo_info = RepositoryManager.get_repository_info(cache_entry.repo_url)\n                    job_id = repo_info.full_name.replace('/', '--')\n                    \n                    # Only add if job doesn't already exist\n                    if job_id not in self.job_status:\n                        self.job_status[job_id] = JobStatus(\n                            job_id=job_id,\n                            repo_url=cache_entry.repo_url,\n                            status='completed',\n                            created_at=cache_entry.created_at,\n                            completed_at=cache_entry.created_at,\n                            docs_path=cache_entry.docs_path,\n                            progress=\"Reconstructed from cache\"\n                        )\n                        reconstructed_count += 1\n                except Exception as e:\n                    print(f\"Failed to reconstruct job for {cache_entry.repo_url}: {e}\")\n            \n            if reconstructed_count > 0:\n                print(f\"Reconstructed {reconstructed_count} job statuses from cache\")\n                self.save_job_statuses()\n                \n        except Exception as e:\n            print(f\"Error reconstructing jobs from cache: {e}\")\n    \n    def save_job_statuses(self):\n        \"\"\"Save job statuses to disk.\"\"\"\n        try:\n            # Ensure cache directory exists\n            self.jobs_file.parent.mkdir(parents=True, exist_ok=True)\n            \n            data = {}\n            for job_id, job in self.job_status.items():\n                data[job_id] = {\n                    'job_id': job.job_id,\n                    'repo_url': job.repo_url,\n                    'status': job.status,\n                    'created_at': job.created_at.isoformat(),\n                    'started_at': job.started_at.isoformat() if job.started_at else None,\n                    'completed_at': job.completed_at.isoformat() if job.completed_at else None,\n                    'error_message': job.error_message,\n                    'progress': job.progress,\n                    'docs_path': job.docs_path\n                }\n            \n            file_manager.save_json(data, self.jobs_file)\n        except Exception as e:\n            print(f\"Error saving job statuses: {e}\")\n    \n    def _worker_loop(self):\n        \"\"\"Main worker loop.\"\"\"\n        while self.running:\n            try:\n                if not self.processing_queue.empty():\n                    job_id = self.processing_queue.get(timeout=1)\n                    self._process_job(job_id)\n                else:\n                    time.sleep(1)\n            except Exception as e:\n                print(f\"Worker error: {e}\")\n                time.sleep(1)\n    \n    def _process_job(self, job_id: str):\n        \"\"\"Process a single documentation generation job.\"\"\"\n        if job_id not in self.job_status:\n            return\n        \n        job = self.job_status[job_id]\n        \n        try:\n            # Update job status\n            job.status = 'processing'\n            job.started_at = datetime.now()\n            job.progress = \"Starting repository clone...\"\n            job.main_model = MAIN_MODEL\n            \n            # Check cache first\n            cached_docs = self.cache_manager.get_cached_docs(job.repo_url)\n            if cached_docs and Path(cached_docs).exists():\n                job.status = 'completed'\n                job.completed_at = datetime.now()\n                job.docs_path = cached_docs\n                job.progress = \"Documentation retrieved from cache\"\n                if not job.main_model:  # Only set if not already set\n                    job.main_model = MAIN_MODEL\n                \n                # Save job status to disk\n                self.save_job_statuses()\n                \n                print(f\"Job {job_id}: Using cached documentation\")\n                return\n            \n            # Clone repository\n            repo_info = RepositoryManager.get_repository_info(job.repo_url)\n            # Use repo full name for temp directory (already URL-safe since job_id is URL-safe)\n            temp_repo_dir = os.path.join(self.temp_dir, job_id)\n            \n            job.progress = f\"Cloning repository {repo_info.full_name}...\"\n            \n            if not RepositoryManager.clone_repository(job.repo_url, temp_repo_dir, job.commit_id):\n                raise Exception(\"Failed to clone repository\")\n            \n            # Generate documentation\n            job.progress = \"Analyzing repository structure...\"\n            \n            # Create config for documentation generation (using env vars)\n            import argparse\n            args = argparse.Namespace(repo_path=temp_repo_dir)\n            config = Config.from_args(args)\n            # Override docs_dir with job-specific directory\n            config.docs_dir = os.path.join(\"output\", \"docs\", f\"{job_id}-docs\")\n            \n            job.progress = \"Generating documentation...\"\n            \n            # Generate documentation\n            doc_generator = DocumentationGenerator(config, job.commit_id)\n            \n            # Run the async documentation generation in a new event loop\n            loop = asyncio.new_event_loop()\n            asyncio.set_event_loop(loop)\n            try:\n                loop.run_until_complete(doc_generator.run())\n            finally:\n                loop.close()\n            \n            # Cache the results\n            docs_path = os.path.abspath(config.docs_dir)\n            self.cache_manager.add_to_cache(job.repo_url, docs_path)\n            \n            # Update job status\n            job.status = 'completed'\n            job.completed_at = datetime.now()\n            job.docs_path = docs_path\n            job.progress = \"Documentation generation completed\"\n            \n            # Save job status to disk\n            self.save_job_statuses()\n            \n            print(f\"Job {job_id}: Documentation generated successfully\")\n            \n        except Exception as e:\n            # Update job status with error\n            job.status = 'failed'\n            job.completed_at = datetime.now()\n            job.error_message = str(e)\n            job.progress = f\"Failed: {str(e)}\"\n            \n            print(f\"Job {job_id}: Failed with error: {e}\")\n        \n        finally:\n            # Cleanup temporary repository\n            if 'temp_repo_dir' in locals() and os.path.exists(temp_repo_dir):\n                try:\n                    subprocess.run(['rm', '-rf', temp_repo_dir], check=True)\n                except Exception as e:\n                    print(f\"Failed to cleanup temp directory: {e}\")",
    "start_line": 26,
    "end_line": 256,
    "has_docstring": true,
    "docstring": "Background worker for processing documentation generation jobs.",
    "parameters": null,
    "node_type": "class",
    "base_classes": null,
    "class_name": null,
    "display_name": "class BackgroundWorker",
    "component_id": "codewiki.src.fe.background_worker.BackgroundWorker"
  },
  "codewiki.src.fe.cache_manager.CacheManager": {
    "id": "codewiki.src.fe.cache_manager.CacheManager",
    "name": "CacheManager",
    "component_type": "class",
    "file_path": "/Users/wufagang/project/aiopen/CodeWiki/codewiki/src/fe/cache_manager.py",
    "relative_path": "codewiki/src/fe/cache_manager.py",
    "depends_on": [
      "codewiki.src.fe.models.CacheEntry"
    ],
    "source_code": "class CacheManager:\n    \"\"\"Manages documentation cache.\"\"\"\n    \n    def __init__(self, cache_dir: str = None, cache_expiry_days: int = None):\n        self.cache_dir = Path(cache_dir or WebAppConfig.CACHE_DIR)\n        self.cache_expiry_days = cache_expiry_days or WebAppConfig.CACHE_EXPIRY_DAYS\n        self.cache_dir.mkdir(parents=True, exist_ok=True)\n        self.cache_index: Dict[str, CacheEntry] = {}\n        self.load_cache_index()\n    \n    def load_cache_index(self):\n        \"\"\"Load cache index from disk.\"\"\"\n        index_file = self.cache_dir / \"cache_index.json\"\n        if index_file.exists():\n            try:\n                data = file_manager.load_json(index_file)\n                for key, value in data.items():\n                    self.cache_index[key] = CacheEntry(\n                        repo_url=value['repo_url'],\n                        repo_url_hash=value['repo_url_hash'],\n                        docs_path=value['docs_path'],\n                        created_at=datetime.fromisoformat(value['created_at']),\n                        last_accessed=datetime.fromisoformat(value['last_accessed'])\n                    )\n            except Exception as e:\n                print(f\"Error loading cache index: {e}\")\n    \n    def save_cache_index(self):\n        \"\"\"Save cache index to disk.\"\"\"\n        index_file = self.cache_dir / \"cache_index.json\"\n        try:\n            data = {}\n            for key, entry in self.cache_index.items():\n                data[key] = {\n                    'repo_url': entry.repo_url,\n                    'repo_url_hash': entry.repo_url_hash,\n                    'docs_path': entry.docs_path,\n                    'created_at': entry.created_at.isoformat(),\n                    'last_accessed': entry.last_accessed.isoformat()\n                }\n            \n            file_manager.save_json(data, index_file)\n        except Exception as e:\n            print(f\"Error saving cache index: {e}\")\n    \n    def get_repo_hash(self, repo_url: str) -> str:\n        \"\"\"Generate hash for repository URL.\"\"\"\n        return hashlib.sha256(repo_url.encode()).hexdigest()[:16]\n    \n    def get_cached_docs(self, repo_url: str) -> Optional[str]:\n        \"\"\"Get cached documentation path if available.\"\"\"\n        repo_hash = self.get_repo_hash(repo_url)\n        \n        if repo_hash in self.cache_index:\n            entry = self.cache_index[repo_hash]\n            \n            # Check if cache is still valid\n            if datetime.now() - entry.created_at < timedelta(days=self.cache_expiry_days):\n                # Update last accessed\n                entry.last_accessed = datetime.now()\n                self.save_cache_index()\n                return entry.docs_path\n            else:\n                # Cache expired, remove it\n                self.remove_from_cache(repo_url)\n        \n        return None\n    \n    def add_to_cache(self, repo_url: str, docs_path: str):\n        \"\"\"Add documentation to cache.\"\"\"\n        repo_hash = self.get_repo_hash(repo_url)\n        now = datetime.now()\n        \n        self.cache_index[repo_hash] = CacheEntry(\n            repo_url=repo_url,\n            repo_url_hash=repo_hash,\n            docs_path=docs_path,\n            created_at=now,\n            last_accessed=now\n        )\n        \n        self.save_cache_index()\n    \n    def remove_from_cache(self, repo_url: str):\n        \"\"\"Remove documentation from cache.\"\"\"\n        repo_hash = self.get_repo_hash(repo_url)\n        if repo_hash in self.cache_index:\n            del self.cache_index[repo_hash]\n            self.save_cache_index()\n    \n    def cleanup_expired_cache(self):\n        \"\"\"Remove expired cache entries.\"\"\"\n        expired_entries = []\n        cutoff = datetime.now() - timedelta(days=self.cache_expiry_days)\n        \n        for repo_hash, entry in self.cache_index.items():\n            if entry.created_at < cutoff:\n                expired_entries.append(repo_hash)\n        \n        for repo_hash in expired_entries:\n            del self.cache_index[repo_hash]\n        \n        if expired_entries:\n            self.save_cache_index()",
    "start_line": 16,
    "end_line": 119,
    "has_docstring": true,
    "docstring": "Manages documentation cache.",
    "parameters": null,
    "node_type": "class",
    "base_classes": null,
    "class_name": null,
    "display_name": "class CacheManager",
    "component_id": "codewiki.src.fe.cache_manager.CacheManager"
  },
  "codewiki.src.fe.config.WebAppConfig": {
    "id": "codewiki.src.fe.config.WebAppConfig",
    "name": "WebAppConfig",
    "component_type": "class",
    "file_path": "/Users/wufagang/project/aiopen/CodeWiki/codewiki/src/fe/config.py",
    "relative_path": "codewiki/src/fe/config.py",
    "depends_on": [],
    "source_code": "class WebAppConfig:\n    \"\"\"Configuration class for web application settings.\"\"\"\n    \n    # Directories\n    CACHE_DIR = \"./output/cache\"\n    TEMP_DIR = \"./output/temp\"\n    OUTPUT_DIR = \"./output\"\n    \n    # Queue settings\n    QUEUE_SIZE = 100\n    \n    # Cache settings\n    CACHE_EXPIRY_DAYS = 365\n    \n    # Job cleanup settings\n    JOB_CLEANUP_HOURS = 24000\n    RETRY_COOLDOWN_MINUTES = 3\n    \n    # Server settings\n    DEFAULT_HOST = \"127.0.0.1\"\n    DEFAULT_PORT = 8000\n    \n    # Git settings\n    CLONE_TIMEOUT = 300\n    CLONE_DEPTH = 1\n    \n    @classmethod\n    def ensure_directories(cls):\n        \"\"\"Ensure all required directories exist.\"\"\"\n        directories = [\n            cls.CACHE_DIR,\n            cls.TEMP_DIR,\n            cls.OUTPUT_DIR\n        ]\n        \n        for directory in directories:\n            Path(directory).mkdir(parents=True, exist_ok=True)\n    \n    @classmethod\n    def get_absolute_path(cls, path: str) -> str:\n        \"\"\"Get absolute path for a given relative path.\"\"\"\n        return os.path.abspath(path)",
    "start_line": 10,
    "end_line": 51,
    "has_docstring": true,
    "docstring": "Configuration class for web application settings.",
    "parameters": null,
    "node_type": "class",
    "base_classes": null,
    "class_name": null,
    "display_name": "class WebAppConfig",
    "component_id": "codewiki.src.fe.config.WebAppConfig"
  },
  "codewiki.src.fe.github_processor.GitHubRepoProcessor": {
    "id": "codewiki.src.fe.github_processor.GitHubRepoProcessor",
    "name": "GitHubRepoProcessor",
    "component_type": "class",
    "file_path": "/Users/wufagang/project/aiopen/CodeWiki/codewiki/src/fe/github_processor.py",
    "relative_path": "codewiki/src/fe/github_processor.py",
    "depends_on": [],
    "source_code": "class GitHubRepoProcessor:\n    \"\"\"Handles GitHub repository processing.\"\"\"\n    \n    @staticmethod\n    def is_valid_github_url(url: str) -> bool:\n        \"\"\"Validate if the URL is a valid GitHub repository URL.\"\"\"\n        try:\n            parsed = urlparse(url)\n            if parsed.netloc.lower() not in ['github.com', 'www.github.com', 'xingyun.jd.com' , 'www.xingyun.jd.com']:\n                return False\n            \n            path_parts = parsed.path.strip('/').split('/')\n            if len(path_parts) < 2:\n                return False\n            \n            # Check if it's a valid repo path (owner/repo)\n            return len(path_parts) >= 2 and all(part for part in path_parts[:2])\n        except Exception:\n            return False\n    \n    @staticmethod\n    def get_repo_info(url: str) -> Dict[str, str]:\n        \"\"\"Extract repository information from GitHub URL.\"\"\"\n        parsed = urlparse(url)\n        path_parts = parsed.path.strip('/').split('/')\n        \n        owner = path_parts[0]\n        repo = path_parts[1]\n        \n        # Remove .git suffix if present\n        if repo.endswith('.git'):\n            repo = repo[:-4]\n        \n        return {\n            'owner': owner,\n            'repo': repo,\n            'full_name': f\"{owner}/{repo}\",\n            'clone_url': f\"https://github.com/{owner}/{repo}.git\"\n        }\n    \n    @staticmethod\n    def clone_repository(clone_url: str, target_dir: str, commit_id: str = None) -> bool:\n        \"\"\"Clone a GitHub repository to the target directory, optionally checking out a specific commit.\"\"\"\n        try:\n            # Ensure target directory exists\n            os.makedirs(os.path.dirname(target_dir), exist_ok=True)\n            \n            # If specific commit is requested, don't use shallow clone\n            if commit_id:\n                # Clone full repository to access specific commit\n                result = subprocess.run([\n                    'git', 'clone', clone_url, target_dir\n                ], capture_output=True, text=True, timeout=WebAppConfig.CLONE_TIMEOUT)\n                \n                if result.returncode != 0:\n                    print(f\"Error cloning repository: {result.stderr}\")\n                    return False\n                \n                # Checkout specific commit\n                result = subprocess.run([\n                    'git', 'checkout', commit_id\n                ], cwd=target_dir, capture_output=True, text=True, timeout=30)\n                \n                if result.returncode != 0:\n                    print(f\"Error checking out commit {commit_id}: {result.stderr}\")\n                    return False\n            else:\n                # Clone repository with shallow depth (default behavior)\n                result = subprocess.run([\n                    'git', 'clone', '--depth', str(WebAppConfig.CLONE_DEPTH), clone_url, target_dir\n                ], capture_output=True, text=True, timeout=WebAppConfig.CLONE_TIMEOUT)\n                \n                if result.returncode != 0:\n                    print(f\"Error cloning repository: {result.stderr}\")\n                    return False\n            \n            return True\n        except Exception as e:\n            print(f\"Error cloning repository: {e}\")\n            return False",
    "start_line": 14,
    "end_line": 93,
    "has_docstring": true,
    "docstring": "Handles GitHub repository processing.",
    "parameters": null,
    "node_type": "class",
    "base_classes": null,
    "class_name": null,
    "display_name": "class GitHubRepoProcessor",
    "component_id": "codewiki.src.fe.github_processor.GitHubRepoProcessor"
  },
  "codewiki.src.fe.models.RepositorySubmission": {
    "id": "codewiki.src.fe.models.RepositorySubmission",
    "name": "RepositorySubmission",
    "component_type": "class",
    "file_path": "/Users/wufagang/project/aiopen/CodeWiki/codewiki/src/fe/models.py",
    "relative_path": "codewiki/src/fe/models.py",
    "depends_on": [],
    "source_code": "class RepositorySubmission(BaseModel):\n    \"\"\"Pydantic model for repository submission form.\"\"\"\n    repo_url: HttpUrl",
    "start_line": 12,
    "end_line": 14,
    "has_docstring": true,
    "docstring": "Pydantic model for repository submission form.",
    "parameters": null,
    "node_type": "class",
    "base_classes": [
      "BaseModel"
    ],
    "class_name": null,
    "display_name": "class RepositorySubmission",
    "component_id": "codewiki.src.fe.models.RepositorySubmission"
  },
  "codewiki.src.fe.models.JobStatusResponse": {
    "id": "codewiki.src.fe.models.JobStatusResponse",
    "name": "JobStatusResponse",
    "component_type": "class",
    "file_path": "/Users/wufagang/project/aiopen/CodeWiki/codewiki/src/fe/models.py",
    "relative_path": "codewiki/src/fe/models.py",
    "depends_on": [],
    "source_code": "class JobStatusResponse(BaseModel):\n    \"\"\"Pydantic model for job status API response.\"\"\"\n    job_id: str\n    repo_url: str\n    status: str\n    created_at: datetime\n    started_at: Optional[datetime] = None\n    completed_at: Optional[datetime] = None\n    error_message: Optional[str] = None\n    progress: str = \"\"\n    docs_path: Optional[str] = None\n    main_model: Optional[str] = None\n    commit_id: Optional[str] = None",
    "start_line": 17,
    "end_line": 29,
    "has_docstring": true,
    "docstring": "Pydantic model for job status API response.",
    "parameters": null,
    "node_type": "class",
    "base_classes": [
      "BaseModel"
    ],
    "class_name": null,
    "display_name": "class JobStatusResponse",
    "component_id": "codewiki.src.fe.models.JobStatusResponse"
  },
  "codewiki.src.fe.models.JobStatus": {
    "id": "codewiki.src.fe.models.JobStatus",
    "name": "JobStatus",
    "component_type": "class",
    "file_path": "/Users/wufagang/project/aiopen/CodeWiki/codewiki/src/fe/models.py",
    "relative_path": "codewiki/src/fe/models.py",
    "depends_on": [],
    "source_code": "class JobStatus:\n    \"\"\"Tracks the status of a documentation generation job.\"\"\"\n    job_id: str\n    repo_url: str\n    status: str  # 'queued', 'processing', 'completed', 'failed'\n    created_at: datetime\n    started_at: Optional[datetime] = None\n    completed_at: Optional[datetime] = None\n    error_message: Optional[str] = None\n    progress: str = \"\"\n    docs_path: Optional[str] = None\n    main_model: Optional[str] = None\n    commit_id: Optional[str] = None",
    "start_line": 33,
    "end_line": 45,
    "has_docstring": true,
    "docstring": "Tracks the status of a documentation generation job.",
    "parameters": null,
    "node_type": "class",
    "base_classes": null,
    "class_name": null,
    "display_name": "class JobStatus",
    "component_id": "codewiki.src.fe.models.JobStatus"
  },
  "codewiki.src.fe.models.CacheEntry": {
    "id": "codewiki.src.fe.models.CacheEntry",
    "name": "CacheEntry",
    "component_type": "class",
    "file_path": "/Users/wufagang/project/aiopen/CodeWiki/codewiki/src/fe/models.py",
    "relative_path": "codewiki/src/fe/models.py",
    "depends_on": [],
    "source_code": "class CacheEntry:\n    \"\"\"Represents a cached documentation result.\"\"\"\n    repo_url: str\n    repo_url_hash: str\n    docs_path: str\n    created_at: datetime\n    last_accessed: datetime",
    "start_line": 49,
    "end_line": 55,
    "has_docstring": true,
    "docstring": "Represents a cached documentation result.",
    "parameters": null,
    "node_type": "class",
    "base_classes": null,
    "class_name": null,
    "display_name": "class CacheEntry",
    "component_id": "codewiki.src.fe.models.CacheEntry"
  },
  "codewiki.src.fe.repository_processors.base_processor.RepositoryInfo": {
    "id": "codewiki.src.fe.repository_processors.base_processor.RepositoryInfo",
    "name": "RepositoryInfo",
    "component_type": "class",
    "file_path": "/Users/wufagang/project/aiopen/CodeWiki/codewiki/src/fe/repository_processors/base_processor.py",
    "relative_path": "codewiki/src/fe/repository_processors/base_processor.py",
    "depends_on": [],
    "source_code": "class RepositoryInfo:\n    \"\"\"Repository information data class.\"\"\"\n\n    def __init__(self,\n                 owner: str,\n                 repo: str,\n                 full_name: str,\n                 clone_url: str,\n                 platform: str,\n                 normalized_url: str):\n        self.owner = owner\n        self.repo = repo\n        self.full_name = full_name\n        self.clone_url = clone_url\n        self.platform = platform\n        self.normalized_url = normalized_url\n\n    def to_dict(self) -> Dict[str, str]:\n        \"\"\"Convert to dictionary for backwards compatibility.\"\"\"\n        return {\n            'owner': self.owner,\n            'repo': self.repo,\n            'full_name': self.full_name,\n            'clone_url': self.clone_url,\n            'platform': self.platform,\n            'normalized_url': self.normalized_url\n        }",
    "start_line": 13,
    "end_line": 39,
    "has_docstring": true,
    "docstring": "Repository information data class.",
    "parameters": null,
    "node_type": "class",
    "base_classes": null,
    "class_name": null,
    "display_name": "class RepositoryInfo",
    "component_id": "codewiki.src.fe.repository_processors.base_processor.RepositoryInfo"
  },
  "codewiki.src.fe.repository_processors.base_processor.BaseRepositoryProcessor": {
    "id": "codewiki.src.fe.repository_processors.base_processor.BaseRepositoryProcessor",
    "name": "BaseRepositoryProcessor",
    "component_type": "class",
    "file_path": "/Users/wufagang/project/aiopen/CodeWiki/codewiki/src/fe/repository_processors/base_processor.py",
    "relative_path": "codewiki/src/fe/repository_processors/base_processor.py",
    "depends_on": [],
    "source_code": "class BaseRepositoryProcessor(ABC):\n    \"\"\"Abstract base class for repository processors.\"\"\"\n\n    PLATFORM_NAME: str = \"\"  # To be overridden by subclasses\n\n    @classmethod\n    @abstractmethod\n    def get_supported_domains(cls) -> List[str]:\n        \"\"\"Return list of supported domain names for this processor.\"\"\"\n        pass\n\n    @classmethod\n    @abstractmethod\n    def is_valid_url(cls, url: str) -> bool:\n        \"\"\"Validate if the URL is supported by this processor.\"\"\"\n        pass\n\n    @classmethod\n    @abstractmethod\n    def get_repo_info(cls, url: str) -> RepositoryInfo:\n        \"\"\"Extract repository information from URL.\"\"\"\n        pass\n\n    @classmethod\n    @abstractmethod\n    def normalize_url(cls, url: str) -> str:\n        \"\"\"Normalize the repository URL to a standard format.\"\"\"\n        pass\n\n    @classmethod\n    def clone_repository(cls,\n                        clone_url: str,\n                        target_dir: str,\n                        commit_id: Optional[str] = None,\n                        depth: int = 1,\n                        timeout: int = 300) -> bool:\n        \"\"\"\n        Clone a repository to the target directory.\n\n        Args:\n            clone_url: Git clone URL\n            target_dir: Target directory path\n            commit_id: Optional specific commit to checkout\n            depth: Clone depth (1 for shallow clone)\n            timeout: Clone timeout in seconds\n\n        Returns:\n            bool: True if successful, False otherwise\n        \"\"\"\n        try:\n            # Ensure target directory exists\n            os.makedirs(os.path.dirname(target_dir), exist_ok=True)\n\n            # If specific commit is requested, don't use shallow clone\n            if commit_id:\n                # Clone full repository to access specific commit\n                result = subprocess.run([\n                    'git', 'clone', clone_url, target_dir\n                ], capture_output=True, text=True, timeout=timeout)\n\n                if result.returncode != 0:\n                    print(f\"Error cloning repository: {result.stderr}\")\n                    return False\n\n                # Checkout specific commit\n                result = subprocess.run([\n                    'git', 'checkout', commit_id\n                ], cwd=target_dir, capture_output=True, text=True, timeout=30)\n\n                if result.returncode != 0:\n                    print(f\"Error checking out commit {commit_id}: {result.stderr}\")\n                    return False\n            else:\n                # Clone repository with shallow depth\n                result = subprocess.run([\n                    'git', 'clone', '--depth', str(depth), clone_url, target_dir\n                ], capture_output=True, text=True, timeout=timeout)\n\n                if result.returncode != 0:\n                    print(f\"Error cloning repository: {result.stderr}\")\n                    return False\n\n            return True\n        except Exception as e:\n            print(f\"Error cloning repository: {e}\")\n            return False\n\n    @classmethod\n    def _parse_url_components(cls, url: str) -> Dict[str, str]:\n        \"\"\"Parse URL into components for analysis.\"\"\"\n        parsed = urlparse(url)\n        return {\n            'scheme': parsed.scheme,\n            'netloc': parsed.netloc.lower(),\n            'path': parsed.path.strip('/'),\n            'query': parsed.query,\n            'fragment': parsed.fragment\n        }\n\n    @classmethod\n    def _extract_path_components(cls, path: str, min_parts: int = 2) -> List[str]:\n        \"\"\"Extract and validate path components.\"\"\"\n        path_parts = path.split('/')\n        if len(path_parts) < min_parts:\n            raise ValueError(f\"Invalid repository path: requires at least {min_parts} components\")\n        return [part for part in path_parts if part]  # Filter out empty parts",
    "start_line": 42,
    "end_line": 147,
    "has_docstring": true,
    "docstring": "Abstract base class for repository processors.",
    "parameters": null,
    "node_type": "class",
    "base_classes": [
      "ABC"
    ],
    "class_name": null,
    "display_name": "class BaseRepositoryProcessor",
    "component_id": "codewiki.src.fe.repository_processors.base_processor.BaseRepositoryProcessor"
  },
  "codewiki.src.fe.repository_processors.factory.RepositoryProcessorFactory": {
    "id": "codewiki.src.fe.repository_processors.factory.RepositoryProcessorFactory",
    "name": "RepositoryProcessorFactory",
    "component_type": "class",
    "file_path": "/Users/wufagang/project/aiopen/CodeWiki/codewiki/src/fe/repository_processors/factory.py",
    "relative_path": "codewiki/src/fe/repository_processors/factory.py",
    "depends_on": [],
    "source_code": "class RepositoryProcessorFactory:\n    \"\"\"Factory class for selecting appropriate repository processor.\"\"\"\n\n    # Registry of all supported processors\n    _processors: List[Type[BaseRepositoryProcessor]] = [\n        GitHubProcessor,\n        GiteeProcessor,\n        JDGitProcessor,\n    ]\n\n    @classmethod\n    def register_processor(cls, processor_class: Type[BaseRepositoryProcessor]):\n        \"\"\"Register a new repository processor.\"\"\"\n        if processor_class not in cls._processors:\n            cls._processors.append(processor_class)\n\n    @classmethod\n    def get_processor(cls, url: str) -> Optional[Type[BaseRepositoryProcessor]]:\n        \"\"\"\n        Get the appropriate processor for a given URL.\n\n        Args:\n            url: Repository URL\n\n        Returns:\n            Processor class that can handle the URL, or None if not supported\n        \"\"\"\n        for processor in cls._processors:\n            if processor.is_valid_url(url):\n                return processor\n        return None\n\n    @classmethod\n    def is_supported_url(cls, url: str) -> bool:\n        \"\"\"Check if the URL is supported by any processor.\"\"\"\n        return cls.get_processor(url) is not None\n\n    @classmethod\n    def get_supported_platforms(cls) -> List[str]:\n        \"\"\"Get list of all supported platform names.\"\"\"\n        return [processor.PLATFORM_NAME for processor in cls._processors]\n\n    @classmethod\n    def get_supported_domains(cls) -> List[str]:\n        \"\"\"Get list of all supported domain names.\"\"\"\n        domains = []\n        for processor in cls._processors:\n            domains.extend(processor.get_supported_domains())\n        return domains\n\n    @classmethod\n    def get_processor_by_platform(cls, platform_name: str) -> Optional[Type[BaseRepositoryProcessor]]:\n        \"\"\"Get processor by platform name.\"\"\"\n        for processor in cls._processors:\n            if processor.PLATFORM_NAME == platform_name:\n                return processor\n        return None",
    "start_line": 13,
    "end_line": 69,
    "has_docstring": true,
    "docstring": "Factory class for selecting appropriate repository processor.",
    "parameters": null,
    "node_type": "class",
    "base_classes": null,
    "class_name": null,
    "display_name": "class RepositoryProcessorFactory",
    "component_id": "codewiki.src.fe.repository_processors.factory.RepositoryProcessorFactory"
  },
  "codewiki.src.fe.repository_processors.gitee_processor.GiteeProcessor": {
    "id": "codewiki.src.fe.repository_processors.gitee_processor.GiteeProcessor",
    "name": "GiteeProcessor",
    "component_type": "class",
    "file_path": "/Users/wufagang/project/aiopen/CodeWiki/codewiki/src/fe/repository_processors/gitee_processor.py",
    "relative_path": "codewiki/src/fe/repository_processors/gitee_processor.py",
    "depends_on": [
      "codewiki.src.fe.repository_processors.base_processor.RepositoryInfo"
    ],
    "source_code": "class GiteeProcessor(BaseRepositoryProcessor):\n    \"\"\"Handles Gitee repository processing.\"\"\"\n\n    PLATFORM_NAME = \"Gitee\"\n\n    @classmethod\n    def get_supported_domains(cls) -> List[str]:\n        \"\"\"Return list of supported Gitee domain names.\"\"\"\n        return ['gitee.com', 'www.gitee.com']\n\n    @classmethod\n    def is_valid_url(cls, url: str) -> bool:\n        \"\"\"Validate if the URL is a valid Gitee repository URL.\"\"\"\n        try:\n            components = cls._parse_url_components(url)\n\n            # Check domain\n            if components['netloc'] not in cls.get_supported_domains():\n                return False\n\n            # Check path structure\n            path_parts = cls._extract_path_components(components['path'], min_parts=2)\n\n            # Basic validation: should have at least owner/repo\n            if len(path_parts) < 2:\n                return False\n\n            # Check if owner and repo are not empty\n            return bool(path_parts[0]) and bool(path_parts[1])\n\n        except Exception:\n            return False\n\n    @classmethod\n    def get_repo_info(cls, url: str) -> RepositoryInfo:\n        \"\"\"Extract repository information from Gitee URL.\"\"\"\n        components = cls._parse_url_components(url)\n        path_parts = cls._extract_path_components(components['path'])\n\n        owner = path_parts[0]\n        repo = path_parts[1]\n\n        # Remove .git suffix if present\n        if repo.endswith('.git'):\n            repo = repo[:-4]\n\n        full_name = f\"{owner}/{repo}\"\n        clone_url = f\"https://gitee.com/{owner}/{repo}.git\"\n        normalized_url = f\"https://gitee.com/{owner}/{repo}\"\n\n        return RepositoryInfo(\n            owner=owner,\n            repo=repo,\n            full_name=full_name,\n            clone_url=clone_url,\n            platform=cls.PLATFORM_NAME,\n            normalized_url=normalized_url\n        )\n\n    @classmethod\n    def normalize_url(cls, url: str) -> str:\n        \"\"\"Normalize Gitee URL to standard format.\"\"\"\n        repo_info = cls.get_repo_info(url)\n        return repo_info.normalized_url",
    "start_line": 10,
    "end_line": 73,
    "has_docstring": true,
    "docstring": "Handles Gitee repository processing.",
    "parameters": null,
    "node_type": "class",
    "base_classes": [
      "BaseRepositoryProcessor"
    ],
    "class_name": null,
    "display_name": "class GiteeProcessor",
    "component_id": "codewiki.src.fe.repository_processors.gitee_processor.GiteeProcessor"
  },
  "codewiki.src.fe.repository_processors.github_processor.GitHubProcessor": {
    "id": "codewiki.src.fe.repository_processors.github_processor.GitHubProcessor",
    "name": "GitHubProcessor",
    "component_type": "class",
    "file_path": "/Users/wufagang/project/aiopen/CodeWiki/codewiki/src/fe/repository_processors/github_processor.py",
    "relative_path": "codewiki/src/fe/repository_processors/github_processor.py",
    "depends_on": [
      "codewiki.src.fe.repository_processors.base_processor.RepositoryInfo"
    ],
    "source_code": "class GitHubProcessor(BaseRepositoryProcessor):\n    \"\"\"Handles GitHub repository processing.\"\"\"\n\n    PLATFORM_NAME = \"GitHub\"\n\n    @classmethod\n    def get_supported_domains(cls) -> List[str]:\n        \"\"\"Return list of supported GitHub domain names.\"\"\"\n        return ['github.com', 'www.github.com']\n\n    @classmethod\n    def is_valid_url(cls, url: str) -> bool:\n        \"\"\"Validate if the URL is a valid GitHub repository URL.\"\"\"\n        try:\n            components = cls._parse_url_components(url)\n\n            # Check domain\n            if components['netloc'] not in cls.get_supported_domains():\n                return False\n\n            # Check path structure\n            path_parts = cls._extract_path_components(components['path'], min_parts=2)\n\n            # Basic validation: should have at least owner/repo\n            if len(path_parts) < 2:\n                return False\n\n            # Check if owner and repo are not empty\n            return bool(path_parts[0]) and bool(path_parts[1])\n\n        except Exception:\n            return False\n\n    @classmethod\n    def get_repo_info(cls, url: str) -> RepositoryInfo:\n        \"\"\"Extract repository information from GitHub URL.\"\"\"\n        components = cls._parse_url_components(url)\n        path_parts = cls._extract_path_components(components['path'])\n\n        owner = path_parts[0]\n        repo = path_parts[1]\n\n        # Remove .git suffix if present\n        if repo.endswith('.git'):\n            repo = repo[:-4]\n\n        full_name = f\"{owner}/{repo}\"\n        clone_url = f\"https://github.com/{owner}/{repo}.git\"\n        normalized_url = f\"https://github.com/{owner}/{repo}\"\n\n        return RepositoryInfo(\n            owner=owner,\n            repo=repo,\n            full_name=full_name,\n            clone_url=clone_url,\n            platform=cls.PLATFORM_NAME,\n            normalized_url=normalized_url\n        )\n\n    @classmethod\n    def normalize_url(cls, url: str) -> str:\n        \"\"\"Normalize GitHub URL to standard format.\"\"\"\n        repo_info = cls.get_repo_info(url)\n        return repo_info.normalized_url",
    "start_line": 10,
    "end_line": 73,
    "has_docstring": true,
    "docstring": "Handles GitHub repository processing.",
    "parameters": null,
    "node_type": "class",
    "base_classes": [
      "BaseRepositoryProcessor"
    ],
    "class_name": null,
    "display_name": "class GitHubProcessor",
    "component_id": "codewiki.src.fe.repository_processors.github_processor.GitHubProcessor"
  },
  "codewiki.src.fe.repository_processors.jd_git_processor.JDGitProcessor": {
    "id": "codewiki.src.fe.repository_processors.jd_git_processor.JDGitProcessor",
    "name": "JDGitProcessor",
    "component_type": "class",
    "file_path": "/Users/wufagang/project/aiopen/CodeWiki/codewiki/src/fe/repository_processors/jd_git_processor.py",
    "relative_path": "codewiki/src/fe/repository_processors/jd_git_processor.py",
    "depends_on": [
      "codewiki.src.fe.repository_processors.base_processor.RepositoryInfo"
    ],
    "source_code": "class JDGitProcessor(BaseRepositoryProcessor):\n    \"\"\"Handles JD internal Git repository processing.\"\"\"\n\n    PLATFORM_NAME = \"JD Internal Git\"\n\n    @classmethod\n    def get_supported_domains(cls) -> List[str]:\n        \"\"\"Return list of supported JD Git domain names.\"\"\"\n        return ['coding.jd.com', 'xingyun.jd.com', 'www.xingyun.jd.com', 'www.coding.jd.com']\n\n    @classmethod\n    def is_valid_url(cls, url: str) -> bool:\n        \"\"\"Validate if the URL is a valid JD internal Git repository URL.\"\"\"\n        try:\n            # Handle SSH URLs (git@coding.jd.com:owner/repo.git)\n            if url.startswith('git@'):\n                return cls._is_valid_ssh_url(url)\n\n            # Handle SSH URLs (ssh://git@coding.jd.com/owner/repo.git)\n            if url.startswith('ssh://'):\n                return cls._is_valid_ssh_url(url)\n\n            # Handle HTTP/HTTPS URLs\n            components = cls._parse_url_components(url)\n\n            # Check domain\n            if components['netloc'] not in cls.get_supported_domains():\n                return False\n\n            # Check path structure\n            path_parts = cls._extract_path_components(components['path'], min_parts=2)\n\n            # For xingyun.jd.com: /codingRoot/owner/repo format\n            if components['netloc'] in ['xingyun.jd.com', 'www.xingyun.jd.com']:\n                if len(path_parts) >= 3 and path_parts[0] == 'codingRoot':\n                    return bool(path_parts[1]) and bool(path_parts[2])\n\n            # For coding.jd.com or general format: /owner/repo\n            if len(path_parts) >= 2:\n                return bool(path_parts[0]) and bool(path_parts[1])\n\n            return False\n\n        except Exception:\n            return False\n\n    @classmethod\n    def _is_valid_ssh_url(cls, url: str) -> bool:\n        \"\"\"Validate SSH URL formats for JD Git.\"\"\"\n        try:\n            if url.startswith('git@'):\n                # Format: git@coding.jd.com:owner/repo.git\n                if '@coding.jd.com:' in url or '@xingyun.jd.com:' in url:\n                    # Extract path part after ':'\n                    if '@coding.jd.com:' in url:\n                        path_part = url.split('@coding.jd.com:')[1]\n                    else:\n                        path_part = url.split('@xingyun.jd.com:')[1]\n\n                    path_components = path_part.rstrip('.git').split('/')\n                    # Should have at least owner/repo\n                    return len(path_components) >= 2\n\n            elif url.startswith('ssh://'):\n                # Format: ssh://git@coding.jd.com/owner/repo.git\n                if 'coding.jd.com' in url or 'xingyun.jd.com' in url:\n                    components = cls._parse_url_components(url)\n                    if 'git@' in components['netloc']:\n                        path_components = cls._extract_path_components(components['path'], min_parts=2)\n                        return len(path_components) >= 2\n\n            return False\n\n        except Exception:\n            return False\n\n    @classmethod\n    def get_repo_info(cls, url: str) -> RepositoryInfo:\n        \"\"\"Extract repository information from JD internal Git URL.\"\"\"\n        if url.startswith('git@') or url.startswith('ssh://'):\n            return cls._get_ssh_repo_info(url)\n        else:\n            return cls._get_http_repo_info(url)\n\n    @classmethod\n    def _get_ssh_repo_info(cls, url: str) -> RepositoryInfo:\n        \"\"\"Extract repo info from SSH URL.\"\"\"\n        if url.startswith('git@'):\n            # Determine which domain\n            if '@coding.jd.com:' in url:\n                domain = 'coding.jd.com'\n                path_part = url.split('@coding.jd.com:')[1].rstrip('.git')\n            elif '@xingyun.jd.com:' in url:\n                domain = 'xingyun.jd.com'\n                path_part = url.split('@xingyun.jd.com:')[1].rstrip('.git')\n            else:\n                raise ValueError(f\"Unsupported SSH domain in URL: {url}\")\n\n            path_components = path_part.split('/')\n\n            # For coding.jd.com: simple owner/repo format\n            if domain == 'coding.jd.com':\n                owner = path_components[0]\n                repo = path_components[1] if len(path_components) > 1 else path_components[0]\n                ssh_clone_url = f\"git@coding.jd.com:{owner}/{repo}.git\"\n                normalized_url = f\"https://coding.jd.com/{owner}/{repo}\"\n\n            # For xingyun.jd.com: might have codingRoot prefix\n            else:\n                if len(path_components) >= 3 and path_components[0] == 'codingRoot':\n                    owner = path_components[1]\n                    repo = path_components[2]\n                    ssh_clone_url = f\"git@xingyun.jd.com:codingRoot/{owner}/{repo}.git\"\n                    normalized_url = f\"http://xingyun.jd.com/codingRoot/{owner}/{repo}\"\n                else:\n                    owner = path_components[0]\n                    repo = path_components[1] if len(path_components) > 1 else path_components[0]\n                    ssh_clone_url = f\"git@xingyun.jd.com:{owner}/{repo}.git\"\n                    normalized_url = f\"http://xingyun.jd.com/{owner}/{repo}\"\n\n        elif url.startswith('ssh://'):\n            # Handle ssh:// format\n            components = cls._parse_url_components(url)\n            domain = components['netloc'].replace('git@', '')\n            path_parts = cls._extract_path_components(components['path'])\n\n            if domain == 'coding.jd.com':\n                owner = path_parts[0]\n                repo = path_parts[1] if len(path_parts) > 1 else path_parts[0]\n                ssh_clone_url = f\"git@coding.jd.com:{owner}/{repo}.git\"\n                normalized_url = f\"https://coding.jd.com/{owner}/{repo}\"\n            else:\n                owner = path_parts[0]\n                repo = path_parts[1] if len(path_parts) > 1 else path_parts[0]\n                ssh_clone_url = f\"git@{domain}:{owner}/{repo}.git\"\n                normalized_url = f\"http://{domain}/{owner}/{repo}\"\n\n        # Remove .git suffix if present\n        if repo.endswith('.git'):\n            repo = repo[:-4]\n\n        full_name = f\"{owner}/{repo}\"\n\n        return RepositoryInfo(\n            owner=owner,\n            repo=repo,\n            full_name=full_name,\n            clone_url=ssh_clone_url,\n            platform=cls.PLATFORM_NAME,\n            normalized_url=normalized_url\n        )\n\n    @classmethod\n    def _get_http_repo_info(cls, url: str) -> RepositoryInfo:\n        \"\"\"Extract repo info from HTTP URL.\"\"\"\n        components = cls._parse_url_components(url)\n        path_parts = cls._extract_path_components(components['path'])\n        domain = components['netloc']\n\n        # Handle different path structures based on domain\n        if domain in ['xingyun.jd.com', 'www.xingyun.jd.com']:\n            if len(path_parts) >= 3 and path_parts[0] == 'codingRoot':\n                # xingyun format: /codingRoot/owner/repo/\n                owner = path_parts[1]\n                repo = path_parts[2]\n                clone_url = f\"git@xingyun.jd.com:codingRoot/{owner}/{repo}.git\"\n                normalized_url = f\"http://xingyun.jd.com/codingRoot/{owner}/{repo}\"\n            else:\n                # Simple format: /owner/repo/\n                owner = path_parts[0]\n                repo = path_parts[1]\n                clone_url = f\"git@xingyun.jd.com:{owner}/{repo}.git\"\n                normalized_url = f\"http://xingyun.jd.com/{owner}/{repo}\"\n        else:\n            # coding.jd.com or other: /owner/repo format\n            owner = path_parts[0]\n            repo = path_parts[1]\n            clone_url = f\"git@coding.jd.com:{owner}/{repo}.git\"\n            normalized_url = f\"https://coding.jd.com/{owner}/{repo}\"\n\n        # Remove .git suffix if present\n        if repo.endswith('.git'):\n            repo = repo[:-4]\n\n        full_name = f\"{owner}/{repo}\"\n\n        return RepositoryInfo(\n            owner=owner,\n            repo=repo,\n            full_name=full_name,\n            clone_url=clone_url,\n            platform=cls.PLATFORM_NAME,\n            normalized_url=normalized_url\n        )\n\n    @classmethod\n    def normalize_url(cls, url: str) -> str:\n        \"\"\"Normalize JD internal Git URL to standard format.\"\"\"\n        repo_info = cls.get_repo_info(url)\n        return repo_info.normalized_url",
    "start_line": 10,
    "end_line": 209,
    "has_docstring": true,
    "docstring": "Handles JD internal Git repository processing.",
    "parameters": null,
    "node_type": "class",
    "base_classes": [
      "BaseRepositoryProcessor"
    ],
    "class_name": null,
    "display_name": "class JDGitProcessor",
    "component_id": "codewiki.src.fe.repository_processors.jd_git_processor.JDGitProcessor"
  },
  "codewiki.src.fe.repository_processors.manager.RepositoryManager": {
    "id": "codewiki.src.fe.repository_processors.manager.RepositoryManager",
    "name": "RepositoryManager",
    "component_type": "class",
    "file_path": "/Users/wufagang/project/aiopen/CodeWiki/codewiki/src/fe/repository_processors/manager.py",
    "relative_path": "codewiki/src/fe/repository_processors/manager.py",
    "depends_on": [
      "codewiki.src.be.dependency_analyzer.analysis.cloning.clone_repository"
    ],
    "source_code": "class RepositoryManager:\n    \"\"\"Unified interface for multi-platform repository operations.\"\"\"\n\n    @classmethod\n    def is_valid_repository_url(cls, url: str) -> bool:\n        \"\"\"Check if the URL is a valid repository URL from any supported platform.\"\"\"\n        return RepositoryProcessorFactory.is_supported_url(url)\n\n    @classmethod\n    def get_repository_info(cls, url: str) -> Optional[RepositoryInfo]:\n        \"\"\"Get repository information from URL.\"\"\"\n        processor = RepositoryProcessorFactory.get_processor(url)\n        if processor:\n            return processor.get_repo_info(url)\n        return None\n\n    @classmethod\n    def normalize_repository_url(cls, url: str) -> Optional[str]:\n        \"\"\"Normalize repository URL to standard format.\"\"\"\n        processor = RepositoryProcessorFactory.get_processor(url)\n        if processor:\n            return processor.normalize_url(url)\n        return None\n\n    @classmethod\n    def clone_repository(cls,\n                        url: str,\n                        target_dir: str,\n                        commit_id: Optional[str] = None,\n                        depth: int = 1,\n                        timeout: int = 300) -> bool:\n        \"\"\"Clone repository using appropriate processor.\"\"\"\n        processor = RepositoryProcessorFactory.get_processor(url)\n        if processor:\n            repo_info = processor.get_repo_info(url)\n            return processor.clone_repository(\n                clone_url=repo_info.clone_url,\n                target_dir=target_dir,\n                commit_id=commit_id,\n                depth=depth,\n                timeout=timeout\n            )\n        return False\n\n    @classmethod\n    def get_supported_platforms(cls) -> list:\n        \"\"\"Get list of all supported platforms.\"\"\"\n        return RepositoryProcessorFactory.get_supported_platforms()\n\n    @classmethod\n    def get_supported_domains(cls) -> list:\n        \"\"\"Get list of all supported domains.\"\"\"\n        return RepositoryProcessorFactory.get_supported_domains()\n\n    # Backwards compatibility methods\n    @classmethod\n    def is_valid_github_url(cls, url: str) -> bool:\n        \"\"\"Legacy method for backwards compatibility.\"\"\"\n        return cls.is_valid_repository_url(url)\n\n    @classmethod\n    def get_repo_info(cls, url: str) -> dict:\n        \"\"\"Legacy method for backwards compatibility.\"\"\"\n        repo_info = cls.get_repository_info(url)\n        if repo_info:\n            return repo_info.to_dict()\n        return {}",
    "start_line": 11,
    "end_line": 77,
    "has_docstring": true,
    "docstring": "Unified interface for multi-platform repository operations.",
    "parameters": null,
    "node_type": "class",
    "base_classes": null,
    "class_name": null,
    "display_name": "class RepositoryManager",
    "component_id": "codewiki.src.fe.repository_processors.manager.RepositoryManager"
  },
  "codewiki.src.fe.routes.WebRoutes": {
    "id": "codewiki.src.fe.routes.WebRoutes",
    "name": "WebRoutes",
    "component_type": "class",
    "file_path": "/Users/wufagang/project/aiopen/CodeWiki/codewiki/src/fe/routes.py",
    "relative_path": "codewiki/src/fe/routes.py",
    "depends_on": [
      "codewiki.src.fe.models.JobStatusResponse",
      "codewiki.src.fe.template_utils.render_template",
      "codewiki.src.fe.models.JobStatus",
      "codewiki.src.fe.visualise_docs.get_file_title",
      "codewiki.src.fe.web_app.get_job_status",
      "codewiki.src.fe.visualise_docs.markdown_to_html"
    ],
    "source_code": "class WebRoutes:\n    \"\"\"Handles all web routes for the application.\"\"\"\n    \n    def __init__(self, background_worker: BackgroundWorker, cache_manager: CacheManager):\n        self.background_worker = background_worker\n        self.cache_manager = cache_manager\n    \n    async def index_get(self, request: Request) -> HTMLResponse:\n        \"\"\"Main page with form for submitting GitHub repositories.\"\"\"\n        # Clean up old jobs before displaying\n        # self.cleanup_old_jobs()\n        \n        # Get recent jobs (last 10)\n        all_jobs = self.background_worker.get_all_jobs()\n        recent_jobs = sorted(\n            all_jobs.values(),\n            key=lambda x: x.created_at,\n            reverse=True\n        )[:100]\n        \n        context = {\n            \"message\": None,\n            \"message_type\": None,\n            \"repo_url\": \"\",\n            \"commit_id\": \"\",\n            \"recent_jobs\": recent_jobs\n        }\n        \n        return HTMLResponse(content=render_template(WEB_INTERFACE_TEMPLATE, context))\n    \n    async def index_post(self, request: Request, repo_url: str = Form(...), commit_id: str = Form(\"\")) -> HTMLResponse:\n        \"\"\"Handle repository submission.\"\"\"\n        # Clean up old jobs before processing\n        self.cleanup_old_jobs()\n        \n        message = None\n        message_type = None\n        \n        repo_url = repo_url.strip()\n        commit_id = commit_id.strip() if commit_id else \"\"\n        \n        if not repo_url:\n            message = \"Please enter a Git repository URL\"\n            message_type = \"error\"\n        elif not RepositoryManager.is_valid_repository_url(repo_url):\n            message = \"Please enter a valid Git repository URL (GitHub, Gitee, or JD internal Git)\"\n            message_type = \"error\"\n        else:\n            # Normalize the repo URL for comparison\n            normalized_repo_url = self._normalize_repository_url(repo_url)\n\n            # Get repo info for job ID generation\n            repo_info = RepositoryManager.get_repository_info(normalized_repo_url)\n            job_id = self._repo_full_name_to_job_id(repo_info.full_name)\n            \n            # Check if already in queue, processing, or recently failed\n            existing_job = self.background_worker.get_job_status(job_id)\n            recent_cutoff = datetime.now() - timedelta(minutes=WebAppConfig.RETRY_COOLDOWN_MINUTES)\n            \n            if existing_job:\n                if existing_job.status in ['queued', 'processing']:\n                    pass  # Will handle below\n                elif existing_job.status == 'failed' and existing_job.created_at > recent_cutoff:\n                    pass  # Will handle below\n                else:\n                    existing_job = None  # Job is old or completed, can reuse\n            \n            if existing_job:\n                if existing_job.status in ['queued', 'processing']:\n                    message = f\"Repository is already being processed (Job ID: {existing_job.job_id})\"\n                else:\n                    message = f\"Repository recently failed processing. Please wait a few minutes before retrying (Job ID: {existing_job.job_id})\"\n                message_type = \"error\"\n            else:\n                # Check cache\n                cached_docs = self.cache_manager.get_cached_docs(normalized_repo_url)\n                if cached_docs and Path(cached_docs).exists():\n                    message = \"Documentation found in cache! Redirecting to view...\"\n                    message_type = \"success\"\n                    # Create a dummy completed job for display\n                    job = JobStatus(\n                        job_id=job_id,\n                        repo_url=normalized_repo_url,  # Use normalized URL\n                        status='completed',\n                        created_at=datetime.now(),\n                        completed_at=datetime.now(),\n                        docs_path=cached_docs,\n                        progress=\"Retrieved from cache\",\n                        commit_id=commit_id if commit_id else None\n                    )\n                    self.background_worker.job_status[job_id] = job\n                else:\n                    # Add to queue\n                    try:\n                        job = JobStatus(\n                            job_id=job_id,\n                            repo_url=normalized_repo_url,  # Use normalized URL\n                            status='queued',\n                            created_at=datetime.now(),\n                            progress=\"Waiting in queue...\",\n                            commit_id=commit_id if commit_id else None\n                        )\n                        \n                        self.background_worker.add_job(job_id, job)\n                        message = f\"Repository added to processing queue! Job ID: {job_id}\"\n                        message_type = \"success\"\n                        repo_url = \"\"  # Clear form\n                        \n                    except Exception as e:\n                        message = f\"Failed to add repository to queue: {str(e)}\\n{format_exc()}\"\n                        message_type = \"error\"\n        \n        # Get recent jobs (last 10)\n        all_jobs = self.background_worker.get_all_jobs()\n        recent_jobs = sorted(\n            all_jobs.values(),\n            key=lambda x: x.created_at,\n            reverse=True\n        )\n        \n        context = {\n            \"message\": message,\n            \"message_type\": message_type,\n            \"repo_url\": repo_url or \"\",\n            \"commit_id\": commit_id or \"\",\n            \"recent_jobs\": recent_jobs\n        }\n        \n        return HTMLResponse(content=render_template(WEB_INTERFACE_TEMPLATE, context))\n    \n    async def get_job_status(self, job_id: str) -> JobStatusResponse:\n        \"\"\"API endpoint to get job status.\"\"\"\n        job = self.background_worker.get_job_status(job_id)\n        if not job:\n            raise HTTPException(status_code=404, detail=\"Job not found\")\n        \n        return JobStatusResponse(**asdict(job))\n    \n    async def view_docs(self, job_id: str) -> RedirectResponse:\n        \"\"\"View generated documentation.\"\"\"\n        job = self.background_worker.get_job_status(job_id)\n        if not job:\n            raise HTTPException(status_code=404, detail=\"Job not found\")\n        \n        if job.status != 'completed' or not job.docs_path:\n            raise HTTPException(status_code=404, detail=\"Documentation not available\")\n        \n        docs_path = Path(job.docs_path)\n        if not docs_path.exists():\n            raise HTTPException(status_code=404, detail=\"Documentation files not found\")\n        \n        # Redirect to the documentation viewer\n        return RedirectResponse(url=f\"/static-docs/{job_id}/\", status_code=status.HTTP_302_FOUND)\n    \n    async def serve_generated_docs(self, job_id: str, filename: str = \"overview.md\") -> HTMLResponse:\n        \"\"\"Serve generated documentation files.\"\"\"\n        job = self.background_worker.get_job_status(job_id)\n        docs_path = None\n        repo_url = None\n        \n        if job:\n            # Job status exists - use it\n            if job.status != 'completed' or not job.docs_path:\n                raise HTTPException(status_code=404, detail=\"Documentation not available\")\n            docs_path = Path(job.docs_path)\n            repo_url = job.repo_url\n        else:\n            # No job status - try to find documentation in cache by job_id\n            # Convert job_id back to repo full name and construct potential paths\n            repo_full_name = self._job_id_to_repo_full_name(job_id)\n            potential_repo_url = f\"https://github.com/{repo_full_name}\"\n            \n            # Check if documentation exists in cache\n            cached_docs = self.cache_manager.get_cached_docs(potential_repo_url)\n            if cached_docs and Path(cached_docs).exists():\n                docs_path = Path(cached_docs)\n                repo_url = potential_repo_url\n                \n                # Recreate job status for consistency\n                job = JobStatus(\n                    job_id=job_id,\n                    repo_url=potential_repo_url,\n                    status='completed',\n                    created_at=datetime.now(),\n                    completed_at=datetime.now(),\n                    docs_path=cached_docs,\n                    progress=\"Loaded from cache\",\n                    commit_id=None  # No commit info available from cache\n                )\n                self.background_worker.job_status[job_id] = job\n                self.background_worker.save_job_statuses()\n            else:\n                raise HTTPException(status_code=404, detail=\"Documentation not found\")\n        \n        if not docs_path or not docs_path.exists():\n            raise HTTPException(status_code=404, detail=\"Documentation files not found\")\n        \n        # Load module tree\n        module_tree = None\n        module_tree_file = docs_path / \"module_tree.json\"\n        if module_tree_file.exists():\n            try:\n                module_tree = file_manager.load_json(module_tree_file)\n            except Exception:\n                pass\n        \n        # Load metadata\n        metadata = None\n        metadata_file = docs_path / \"metadata.json\"\n        if metadata_file.exists():\n            try:\n                metadata = file_manager.load_json(metadata_file)\n            except Exception:\n                pass\n        \n        # Serve the requested file\n        file_path = docs_path / filename\n        if not file_path.exists():\n            raise HTTPException(status_code=404, detail=f\"File {filename} not found\")\n        \n        try:\n            content = file_manager.load_text(file_path)\n            \n            # Convert markdown to HTML (reuse from visualise_docs.py)\n            from .visualise_docs import markdown_to_html, get_file_title\n            from .templates import DOCS_VIEW_TEMPLATE\n            \n            html_content = markdown_to_html(content)\n            title = get_file_title(file_path)\n            \n            context = {\n                \"repo_name\": repo_url.split(\"/\")[-1],\n                \"title\": title,\n                \"content\": html_content,\n                \"navigation\": module_tree,\n                \"current_page\": filename,\n                \"job_id\": job_id,\n                \"metadata\": metadata\n            }\n            \n            return HTMLResponse(content=render_template(DOCS_VIEW_TEMPLATE, context))\n            \n        except Exception as e:\n            raise HTTPException(status_code=500, detail=f\"Error reading {filename}: {e}\\n{format_exc()}\")\n    \n    def _normalize_repository_url(self, url: str) -> str:\n        \"\"\"Normalize repository URL for consistent comparison.\"\"\"\n        try:\n            # Use RepositoryManager to get normalized URL\n            normalized = RepositoryManager.normalize_repository_url(url)\n            return normalized if normalized else url.rstrip('/').lower()\n        except Exception:\n            # Fallback to basic normalization\n            return url.rstrip('/').lower()\n    \n    def _repo_full_name_to_job_id(self, full_name: str) -> str:\n        \"\"\"Convert repo full name to URL-safe job ID.\"\"\"\n        return full_name.replace('/', '--')\n    \n    def _job_id_to_repo_full_name(self, job_id: str) -> str:\n        \"\"\"Convert job ID back to repo full name.\"\"\"\n        return job_id.replace('--', '/')\n    \n    def cleanup_old_jobs(self):\n        \"\"\"Clean up old job status entries.\"\"\"\n        cutoff = datetime.now() - timedelta(hours=WebAppConfig.JOB_CLEANUP_HOURS)\n        all_jobs = self.background_worker.get_all_jobs()\n        expired_jobs = [\n            job_id for job_id, job in all_jobs.items()\n            if job.created_at < cutoff and job.status in ['completed', 'failed']\n        ]\n        \n        for job_id in expired_jobs:\n            if job_id in self.background_worker.job_status:\n                del self.background_worker.job_status[job_id]",
    "start_line": 25,
    "end_line": 299,
    "has_docstring": true,
    "docstring": "Handles all web routes for the application.",
    "parameters": null,
    "node_type": "class",
    "base_classes": null,
    "class_name": null,
    "display_name": "class WebRoutes",
    "component_id": "codewiki.src.fe.routes.WebRoutes"
  },
  "codewiki.src.fe.template_utils.StringTemplateLoader": {
    "id": "codewiki.src.fe.template_utils.StringTemplateLoader",
    "name": "StringTemplateLoader",
    "component_type": "class",
    "file_path": "/Users/wufagang/project/aiopen/CodeWiki/codewiki/src/fe/template_utils.py",
    "relative_path": "codewiki/src/fe/template_utils.py",
    "depends_on": [],
    "source_code": "class StringTemplateLoader(BaseLoader):\n    \"\"\"Custom Jinja2 loader for string templates.\"\"\"\n    \n    def __init__(self, template_string: str):\n        self.template_string = template_string\n    \n    def get_source(self, environment, template):\n        return self.template_string, None, lambda: True",
    "start_line": 10,
    "end_line": 17,
    "has_docstring": true,
    "docstring": "Custom Jinja2 loader for string templates.",
    "parameters": null,
    "node_type": "class",
    "base_classes": [
      "BaseLoader"
    ],
    "class_name": null,
    "display_name": "class StringTemplateLoader",
    "component_id": "codewiki.src.fe.template_utils.StringTemplateLoader"
  },
  "codewiki.src.fe.template_utils.render_template": {
    "id": "codewiki.src.fe.template_utils.render_template",
    "name": "render_template",
    "component_type": "function",
    "file_path": "/Users/wufagang/project/aiopen/CodeWiki/codewiki/src/fe/template_utils.py",
    "relative_path": "codewiki/src/fe/template_utils.py",
    "depends_on": [
      "codewiki.src.fe.template_utils.StringTemplateLoader"
    ],
    "source_code": "def render_template(template: str, context: Dict[str, Any]) -> str:\n    \"\"\"\n    Render template using Jinja2.\n    \n    Args:\n        template: HTML template string with Jinja2 syntax\n        context: Dictionary of variables to substitute\n    \n    Returns:\n        Rendered HTML string\n    \"\"\"\n    # Create Jinja2 environment with string template\n    env = Environment(\n        loader=StringTemplateLoader(template),\n        autoescape=select_autoescape(['html', 'xml']),\n        trim_blocks=True,\n        lstrip_blocks=True\n    )\n    \n    # Get template and render\n    jinja_template = env.get_template('')\n    return jinja_template.render(**context)",
    "start_line": 20,
    "end_line": 41,
    "has_docstring": true,
    "docstring": "Render template using Jinja2.\n\nArgs:\n    template: HTML template string with Jinja2 syntax\n    context: Dictionary of variables to substitute\n\nReturns:\n    Rendered HTML string",
    "parameters": [
      "template",
      "context"
    ],
    "node_type": "function",
    "base_classes": null,
    "class_name": null,
    "display_name": "function render_template",
    "component_id": "codewiki.src.fe.template_utils.render_template"
  },
  "codewiki.src.fe.template_utils.render_navigation": {
    "id": "codewiki.src.fe.template_utils.render_navigation",
    "name": "render_navigation",
    "component_type": "function",
    "file_path": "/Users/wufagang/project/aiopen/CodeWiki/codewiki/src/fe/template_utils.py",
    "relative_path": "codewiki/src/fe/template_utils.py",
    "depends_on": [
      "codewiki.src.fe.template_utils.render_template"
    ],
    "source_code": "def render_navigation(module_tree: Dict[str, Any], current_page: str = \"\") -> str:\n    \"\"\"\n    Render navigation HTML from module tree structure.\n    \n    Args:\n        module_tree: Dictionary representing the module tree\n        current_page: Current page filename for highlighting\n    \n    Returns:\n        HTML string for navigation\n    \"\"\"\n    if not module_tree:\n        return \"\"\n    \n    nav_template = \"\"\"\n    {%- for section_key, section_data in module_tree.items() %}\n    <div class=\"nav-section\">\n        <h3>{{ section_key.replace('_', ' ').title() }}</h3>\n        {%- if section_data.get('components') %}\n        <a href=\"/{{ section_key }}.md\" class=\"nav-item {{ 'active' if current_page == section_key + '.md' else '' }}\">Overview</a>\n        {%- endif %}\n        {%- if section_data.get('children') %}\n        {%- for child_key, child_data in section_data['children'].items() %}\n        <div class=\"nav-subsection\">\n            <a href=\"/{{ child_key }}.md\" class=\"nav-item {{ 'active' if current_page == child_key + '.md' else '' }}\">{{ child_key.replace('_', ' ').title() }}</a>\n        </div>\n        {%- endfor %}\n        {%- endif %}\n    </div>\n    {%- endfor %}\n    \"\"\"\n    \n    return render_template(nav_template, {\n        'module_tree': module_tree,\n        'current_page': current_page\n    })",
    "start_line": 44,
    "end_line": 79,
    "has_docstring": true,
    "docstring": "Render navigation HTML from module tree structure.\n\nArgs:\n    module_tree: Dictionary representing the module tree\n    current_page: Current page filename for highlighting\n\nReturns:\n    HTML string for navigation",
    "parameters": [
      "module_tree",
      "current_page"
    ],
    "node_type": "function",
    "base_classes": null,
    "class_name": null,
    "display_name": "function render_navigation",
    "component_id": "codewiki.src.fe.template_utils.render_navigation"
  },
  "codewiki.src.fe.template_utils.render_job_list": {
    "id": "codewiki.src.fe.template_utils.render_job_list",
    "name": "render_job_list",
    "component_type": "function",
    "file_path": "/Users/wufagang/project/aiopen/CodeWiki/codewiki/src/fe/template_utils.py",
    "relative_path": "codewiki/src/fe/template_utils.py",
    "depends_on": [
      "codewiki.src.fe.template_utils.render_template"
    ],
    "source_code": "def render_job_list(jobs: list) -> str:\n    \"\"\"\n    Render job list HTML.\n    \n    Args:\n        jobs: List of job objects\n    \n    Returns:\n        HTML string for job list\n    \"\"\"\n    if not jobs:\n        return \"\"\n    \n    job_list_template = \"\"\"\n    {%- for job in jobs %}\n    <div class=\"job-item\">\n        <div class=\"job-header\">\n            <div class=\"job-url\">{{ job.repo_url }}</div>\n            <div class=\"job-status status-{{ job.status }}\">{{ job.status.title() }}</div>\n        </div>\n        {%- if job.progress %}\n        <div class=\"job-progress\">{{ job.progress }}</div>\n        {%- endif %}\n        {%- if job.status == 'completed' and job.docs_path %}\n        <div class=\"job-actions\">\n            <a href=\"/docs/{{ job.job_id }}\" class=\"btn btn-small\">View Documentation</a>\n        </div>\n        {%- endif %}\n    </div>\n    {%- endfor %}\n    \"\"\"\n    \n    return render_template(job_list_template, {'jobs': jobs})",
    "start_line": 82,
    "end_line": 114,
    "has_docstring": true,
    "docstring": "Render job list HTML.\n\nArgs:\n    jobs: List of job objects\n\nReturns:\n    HTML string for job list",
    "parameters": [
      "jobs"
    ],
    "node_type": "function",
    "base_classes": null,
    "class_name": null,
    "display_name": "function render_job_list",
    "component_id": "codewiki.src.fe.template_utils.render_job_list"
  },
  "codewiki.src.fe.visualise_docs.initialize_globals": {
    "id": "codewiki.src.fe.visualise_docs.initialize_globals",
    "name": "initialize_globals",
    "component_type": "function",
    "file_path": "/Users/wufagang/project/aiopen/CodeWiki/codewiki/src/fe/visualise_docs.py",
    "relative_path": "codewiki/src/fe/visualise_docs.py",
    "depends_on": [
      "codewiki.src.fe.visualise_docs.load_module_tree"
    ],
    "source_code": "def initialize_globals():\n    \"\"\"Initialize global variables from environment or command line args if not already set.\"\"\"\n    global DOCS_FOLDER, MODULE_TREE\n    \n    if DOCS_FOLDER is None:\n        # Try to get from environment variable or use a default\n        import os\n        docs_folder_path = os.environ.get('DOCS_FOLDER')\n        if docs_folder_path and Path(docs_folder_path).exists():\n            DOCS_FOLDER = docs_folder_path\n            MODULE_TREE = load_module_tree(Path(docs_folder_path))\n        else:\n            # If no environment variable, we need to handle this gracefully\n            # The FastAPI endpoints will need to check if DOCS_FOLDER is None\n            pass",
    "start_line": 34,
    "end_line": 48,
    "has_docstring": true,
    "docstring": "Initialize global variables from environment or command line args if not already set.",
    "parameters": [],
    "node_type": "function",
    "base_classes": null,
    "class_name": null,
    "display_name": "function initialize_globals",
    "component_id": "codewiki.src.fe.visualise_docs.initialize_globals"
  },
  "codewiki.src.fe.visualise_docs.load_module_tree": {
    "id": "codewiki.src.fe.visualise_docs.load_module_tree",
    "name": "load_module_tree",
    "component_type": "function",
    "file_path": "/Users/wufagang/project/aiopen/CodeWiki/codewiki/src/fe/visualise_docs.py",
    "relative_path": "codewiki/src/fe/visualise_docs.py",
    "depends_on": [],
    "source_code": "def load_module_tree(docs_folder: Path) -> Optional[Dict]:\n    \"\"\"Load the module tree structure from module_tree.json.\"\"\"\n    tree_file = docs_folder / \"module_tree.json\"\n    if not tree_file.exists():\n        print(f\"Warning: module_tree.json not found in {docs_folder}\")\n        return None\n    \n    try:\n        return file_manager.load_json(tree_file)\n    except Exception as e:\n        print(f\"Error loading module_tree.json: {e}\")\n        return None",
    "start_line": 54,
    "end_line": 65,
    "has_docstring": true,
    "docstring": "Load the module tree structure from module_tree.json.",
    "parameters": [
      "docs_folder"
    ],
    "node_type": "function",
    "base_classes": null,
    "class_name": null,
    "display_name": "function load_module_tree",
    "component_id": "codewiki.src.fe.visualise_docs.load_module_tree"
  },
  "codewiki.src.fe.visualise_docs.markdown_to_html": {
    "id": "codewiki.src.fe.visualise_docs.markdown_to_html",
    "name": "markdown_to_html",
    "component_type": "function",
    "file_path": "/Users/wufagang/project/aiopen/CodeWiki/codewiki/src/fe/visualise_docs.py",
    "relative_path": "codewiki/src/fe/visualise_docs.py",
    "depends_on": [],
    "source_code": "def markdown_to_html(content: str) -> str:\n    \"\"\"Convert markdown content to HTML, with special handling for mermaid diagrams.\"\"\"\n    # First, convert markdown to HTML\n    html = md.render(content)\n    \n    # Post-process to ensure mermaid code blocks are properly formatted\n    # Look for code blocks with language-mermaid class and convert them to mermaid divs\n    import re\n    \n    # Pattern to match mermaid code blocks\n    pattern = r'<pre><code class=\"language-mermaid\">(.*?)</code></pre>'\n    \n    def replace_mermaid(match):\n        mermaid_code = match.group(1)\n        # Decode HTML entities that might have been encoded\n        import html\n        mermaid_code = html.unescape(mermaid_code)\n        return f'<div class=\"mermaid\">{mermaid_code}</div>'\n    \n    # Replace mermaid code blocks with proper mermaid divs\n    html = re.sub(pattern, replace_mermaid, html, flags=re.DOTALL)\n    \n    return html",
    "start_line": 68,
    "end_line": 90,
    "has_docstring": true,
    "docstring": "Convert markdown content to HTML, with special handling for mermaid diagrams.",
    "parameters": [
      "content"
    ],
    "node_type": "function",
    "base_classes": null,
    "class_name": null,
    "display_name": "function markdown_to_html",
    "component_id": "codewiki.src.fe.visualise_docs.markdown_to_html"
  },
  "codewiki.src.fe.visualise_docs.replace_mermaid": {
    "id": "codewiki.src.fe.visualise_docs.replace_mermaid",
    "name": "replace_mermaid",
    "component_type": "function",
    "file_path": "/Users/wufagang/project/aiopen/CodeWiki/codewiki/src/fe/visualise_docs.py",
    "relative_path": "codewiki/src/fe/visualise_docs.py",
    "depends_on": [],
    "source_code": "    def replace_mermaid(match):\n        mermaid_code = match.group(1)\n        # Decode HTML entities that might have been encoded\n        import html\n        mermaid_code = html.unescape(mermaid_code)\n        return f'<div class=\"mermaid\">{mermaid_code}</div>'",
    "start_line": 80,
    "end_line": 85,
    "has_docstring": false,
    "docstring": "",
    "parameters": [
      "match"
    ],
    "node_type": "function",
    "base_classes": null,
    "class_name": null,
    "display_name": "function replace_mermaid",
    "component_id": "codewiki.src.fe.visualise_docs.replace_mermaid"
  },
  "codewiki.src.fe.visualise_docs.get_file_title": {
    "id": "codewiki.src.fe.visualise_docs.get_file_title",
    "name": "get_file_title",
    "component_type": "function",
    "file_path": "/Users/wufagang/project/aiopen/CodeWiki/codewiki/src/fe/visualise_docs.py",
    "relative_path": "codewiki/src/fe/visualise_docs.py",
    "depends_on": [],
    "source_code": "def get_file_title(file_path: Path) -> str:\n    \"\"\"Extract title from markdown file, fallback to filename.\"\"\"\n    try:\n        content = file_manager.load_text(file_path)\n        first_line = content.split('\\n')[0].strip()\n        if first_line.startswith('# '):\n            return first_line[2:].strip()\n    except Exception:\n        pass\n    \n    # Fallback to filename without extension\n    return file_path.stem.replace('_', ' ').title()",
    "start_line": 93,
    "end_line": 104,
    "has_docstring": true,
    "docstring": "Extract title from markdown file, fallback to filename.",
    "parameters": [
      "file_path"
    ],
    "node_type": "function",
    "base_classes": null,
    "class_name": null,
    "display_name": "function get_file_title",
    "component_id": "codewiki.src.fe.visualise_docs.get_file_title"
  },
  "codewiki.src.fe.visualise_docs.index": {
    "id": "codewiki.src.fe.visualise_docs.index",
    "name": "index",
    "component_type": "function",
    "file_path": "/Users/wufagang/project/aiopen/CodeWiki/codewiki/src/fe/visualise_docs.py",
    "relative_path": "codewiki/src/fe/visualise_docs.py",
    "depends_on": [
      "codewiki.src.fe.visualise_docs.initialize_globals",
      "codewiki.src.fe.visualise_docs.markdown_to_html",
      "codewiki.src.fe.visualise_docs.get_file_title",
      "codewiki.src.fe.template_utils.render_template"
    ],
    "source_code": "async def index():\n    \"\"\"Serve the overview page as the main page.\"\"\"\n    initialize_globals()\n    \n    if DOCS_FOLDER is None:\n        raise HTTPException(status_code=500, detail=\"Documentation folder not configured. Please set DOCS_FOLDER environment variable or run with --docs-folder argument.\")\n    \n    overview_file = Path(DOCS_FOLDER) / \"overview.md\"\n    \n    if not overview_file.exists():\n        raise HTTPException(status_code=404, detail=\"overview.md not found in the documentation folder\")\n    \n    try:\n        content = file_manager.load_text(overview_file)\n        \n        html_content = markdown_to_html(content)\n        title = get_file_title(overview_file)\n        \n        context = {\n            \"title\": title,\n            \"content\": html_content,\n            \"navigation\": MODULE_TREE,\n            \"current_page\": \"overview.md\"\n        }\n        \n        return HTMLResponse(content=render_template(DOCS_VIEW_TEMPLATE, context))\n        \n    except Exception as e:\n        raise HTTPException(status_code=500, detail=f\"Error reading overview.md: {e}\")",
    "start_line": 108,
    "end_line": 136,
    "has_docstring": true,
    "docstring": "Serve the overview page as the main page.",
    "parameters": [],
    "node_type": "function",
    "base_classes": null,
    "class_name": null,
    "display_name": "function index",
    "component_id": "codewiki.src.fe.visualise_docs.index"
  },
  "codewiki.src.fe.visualise_docs.serve_doc": {
    "id": "codewiki.src.fe.visualise_docs.serve_doc",
    "name": "serve_doc",
    "component_type": "function",
    "file_path": "/Users/wufagang/project/aiopen/CodeWiki/codewiki/src/fe/visualise_docs.py",
    "relative_path": "codewiki/src/fe/visualise_docs.py",
    "depends_on": [
      "codewiki.src.fe.visualise_docs.initialize_globals",
      "codewiki.src.fe.visualise_docs.markdown_to_html",
      "codewiki.src.fe.visualise_docs.get_file_title",
      "codewiki.src.fe.template_utils.render_template"
    ],
    "source_code": "async def serve_doc(filename: str):\n    \"\"\"Serve individual documentation files.\"\"\"\n    initialize_globals()\n    \n    if DOCS_FOLDER is None:\n        raise HTTPException(status_code=500, detail=\"Documentation folder not configured. Please set DOCS_FOLDER environment variable or run with --docs-folder argument.\")\n    \n    # Security check: ensure we're only serving .md files and they exist in the docs folder\n    if not filename.endswith('.md'):\n        raise HTTPException(status_code=404, detail=\"Only markdown files are supported\")\n    \n    file_path = Path(DOCS_FOLDER) / filename\n    \n    # Ensure the file is within the docs folder (prevent directory traversal)\n    try:\n        file_path = file_path.resolve()\n        docs_folder_resolved = Path(DOCS_FOLDER).resolve()\n        if not str(file_path).startswith(str(docs_folder_resolved)):\n            raise HTTPException(status_code=403, detail=\"Access denied\")\n    except Exception:\n        raise HTTPException(status_code=403, detail=\"Invalid file path\")\n    \n    if not file_path.exists():\n        raise HTTPException(status_code=404, detail=f\"File {filename} not found\")\n    \n    try:\n        content = file_manager.load_text(file_path)\n        \n        html_content = markdown_to_html(content)\n        title = get_file_title(file_path)\n        \n        context = {\n            \"title\": title,\n            \"content\": html_content,\n            \"navigation\": MODULE_TREE,\n            \"current_page\": filename\n        }\n        \n        return HTMLResponse(content=render_template(DOCS_VIEW_TEMPLATE, context))\n        \n    except Exception as e:\n        raise HTTPException(status_code=500, detail=f\"Error reading {filename}: {e}\")",
    "start_line": 140,
    "end_line": 181,
    "has_docstring": true,
    "docstring": "Serve individual documentation files.",
    "parameters": [
      "filename"
    ],
    "node_type": "function",
    "base_classes": null,
    "class_name": null,
    "display_name": "function serve_doc",
    "component_id": "codewiki.src.fe.visualise_docs.serve_doc"
  },
  "codewiki.src.fe.visualise_docs.main": {
    "id": "codewiki.src.fe.visualise_docs.main",
    "name": "main",
    "component_type": "function",
    "file_path": "/Users/wufagang/project/aiopen/CodeWiki/codewiki/src/fe/visualise_docs.py",
    "relative_path": "codewiki/src/fe/visualise_docs.py",
    "depends_on": [
      "codewiki.src.fe.visualise_docs.load_module_tree"
    ],
    "source_code": "def main():\n    \"\"\"Main function to run the documentation server.\"\"\"\n    parser = argparse.ArgumentParser(\n        description=\"Simple documentation server for hosting markdown documentation folders\"\n    )\n    parser.add_argument(\n        \"--docs-folder\",\n        type=str,\n        required=True,\n        help=\"Path to the documentation folder containing markdown files and module_tree.json\"\n    )\n    parser.add_argument(\n        \"--port\",\n        type=int,\n        default=8000,\n        help=\"Port to run the server on (default: 8000)\"\n    )\n    parser.add_argument(\n        \"--host\",\n        type=str,\n        default=\"127.0.0.1\",\n        help=\"Host to bind the server to (default: 127.0.0.1)\"\n    )\n    parser.add_argument(\n        \"--debug\",\n        action=\"store_true\",\n        help=\"Run the server in debug mode\"\n    )\n    \n    args = parser.parse_args()\n    \n    # Validate docs folder\n    docs_folder = Path(args.docs_folder)\n    if not docs_folder.exists():\n        print(f\"Error: Documentation folder '{docs_folder}' does not exist\")\n        sys.exit(1)\n    \n    if not docs_folder.is_dir():\n        print(f\"Error: '{docs_folder}' is not a directory\")\n        sys.exit(1)\n    \n    # Check for overview.md\n    overview_file = docs_folder / \"overview.md\"\n    if not overview_file.exists():\n        print(f\"Warning: overview.md not found in '{docs_folder}'\")\n    \n    # Set global variables and environment variable for uvicorn reload\n    global DOCS_FOLDER, MODULE_TREE\n    DOCS_FOLDER = str(docs_folder.resolve())\n    MODULE_TREE = load_module_tree(docs_folder)\n    \n    # Set environment variable so uvicorn reload can pick it up\n    import os\n    os.environ['DOCS_FOLDER'] = DOCS_FOLDER\n    \n    print(f\"📚 Starting documentation server...\")\n    print(f\"📁 Documentation folder: {DOCS_FOLDER}\")\n    print(f\"🌐 Server running at: http://{args.host}:{args.port}\")\n    print(f\"📖 Main page: overview.md\")\n    \n    if MODULE_TREE:\n        modules_count = len(MODULE_TREE)\n        print(f\"🗂️  Found {modules_count} main modules in module_tree.json\")\n    \n    print(\"\\nPress Ctrl+C to stop the server\")\n    \n    try:\n        import uvicorn\n        uvicorn.run(\n            \"visualise_docs:app\",\n            host=args.host,\n            port=args.port,\n            reload=args.debug,\n            log_level=\"debug\" if args.debug else \"info\"\n        )\n    except KeyboardInterrupt:\n        print(\"\\n👋 Server stopped\")",
    "start_line": 188,
    "end_line": 264,
    "has_docstring": true,
    "docstring": "Main function to run the documentation server.",
    "parameters": [],
    "node_type": "function",
    "base_classes": null,
    "class_name": null,
    "display_name": "function main",
    "component_id": "codewiki.src.fe.visualise_docs.main"
  },
  "codewiki.src.fe.web_app.index_get": {
    "id": "codewiki.src.fe.web_app.index_get",
    "name": "index_get",
    "component_type": "function",
    "file_path": "/Users/wufagang/project/aiopen/CodeWiki/codewiki/src/fe/web_app.py",
    "relative_path": "codewiki/src/fe/web_app.py",
    "depends_on": [
      "codewiki.src.fe.web_app.index_get"
    ],
    "source_code": "async def index_get(request: Request):\n    \"\"\"Main page with form for submitting GitHub repositories.\"\"\"\n    return await web_routes.index_get(request)",
    "start_line": 44,
    "end_line": 46,
    "has_docstring": true,
    "docstring": "Main page with form for submitting GitHub repositories.",
    "parameters": [
      "request"
    ],
    "node_type": "function",
    "base_classes": null,
    "class_name": null,
    "display_name": "function index_get",
    "component_id": "codewiki.src.fe.web_app.index_get"
  },
  "codewiki.src.fe.web_app.index_post": {
    "id": "codewiki.src.fe.web_app.index_post",
    "name": "index_post",
    "component_type": "function",
    "file_path": "/Users/wufagang/project/aiopen/CodeWiki/codewiki/src/fe/web_app.py",
    "relative_path": "codewiki/src/fe/web_app.py",
    "depends_on": [
      "codewiki.src.fe.web_app.index_post"
    ],
    "source_code": "async def index_post(request: Request, repo_url: str = Form(...), commit_id: str = Form(\"\")):\n    \"\"\"Handle repository submission.\"\"\"\n    return await web_routes.index_post(request, repo_url, commit_id)",
    "start_line": 50,
    "end_line": 52,
    "has_docstring": true,
    "docstring": "Handle repository submission.",
    "parameters": [
      "request",
      "repo_url",
      "commit_id"
    ],
    "node_type": "function",
    "base_classes": null,
    "class_name": null,
    "display_name": "function index_post",
    "component_id": "codewiki.src.fe.web_app.index_post"
  },
  "codewiki.src.fe.web_app.get_job_status": {
    "id": "codewiki.src.fe.web_app.get_job_status",
    "name": "get_job_status",
    "component_type": "function",
    "file_path": "/Users/wufagang/project/aiopen/CodeWiki/codewiki/src/fe/web_app.py",
    "relative_path": "codewiki/src/fe/web_app.py",
    "depends_on": [
      "codewiki.src.fe.web_app.get_job_status"
    ],
    "source_code": "async def get_job_status(job_id: str):\n    \"\"\"API endpoint to get job status.\"\"\"\n    return await web_routes.get_job_status(job_id)",
    "start_line": 56,
    "end_line": 58,
    "has_docstring": true,
    "docstring": "API endpoint to get job status.",
    "parameters": [
      "job_id"
    ],
    "node_type": "function",
    "base_classes": null,
    "class_name": null,
    "display_name": "function get_job_status",
    "component_id": "codewiki.src.fe.web_app.get_job_status"
  },
  "codewiki.src.fe.web_app.view_docs": {
    "id": "codewiki.src.fe.web_app.view_docs",
    "name": "view_docs",
    "component_type": "function",
    "file_path": "/Users/wufagang/project/aiopen/CodeWiki/codewiki/src/fe/web_app.py",
    "relative_path": "codewiki/src/fe/web_app.py",
    "depends_on": [
      "codewiki.src.fe.web_app.view_docs"
    ],
    "source_code": "async def view_docs(job_id: str):\n    \"\"\"View generated documentation.\"\"\"\n    return await web_routes.view_docs(job_id)",
    "start_line": 62,
    "end_line": 64,
    "has_docstring": true,
    "docstring": "View generated documentation.",
    "parameters": [
      "job_id"
    ],
    "node_type": "function",
    "base_classes": null,
    "class_name": null,
    "display_name": "function view_docs",
    "component_id": "codewiki.src.fe.web_app.view_docs"
  },
  "codewiki.src.fe.web_app.serve_generated_docs": {
    "id": "codewiki.src.fe.web_app.serve_generated_docs",
    "name": "serve_generated_docs",
    "component_type": "function",
    "file_path": "/Users/wufagang/project/aiopen/CodeWiki/codewiki/src/fe/web_app.py",
    "relative_path": "codewiki/src/fe/web_app.py",
    "depends_on": [
      "codewiki.src.fe.web_app.serve_generated_docs"
    ],
    "source_code": "async def serve_generated_docs(job_id: str, filename: str = \"overview.md\"):\n    \"\"\"Serve generated documentation files.\"\"\"\n    if not filename: \n        filename = \"overview.md\"\n    return await web_routes.serve_generated_docs(job_id, filename)",
    "start_line": 69,
    "end_line": 73,
    "has_docstring": true,
    "docstring": "Serve generated documentation files.",
    "parameters": [
      "job_id",
      "filename"
    ],
    "node_type": "function",
    "base_classes": null,
    "class_name": null,
    "display_name": "function serve_generated_docs",
    "component_id": "codewiki.src.fe.web_app.serve_generated_docs"
  },
  "codewiki.src.fe.web_app.main": {
    "id": "codewiki.src.fe.web_app.main",
    "name": "main",
    "component_type": "function",
    "file_path": "/Users/wufagang/project/aiopen/CodeWiki/codewiki/src/fe/web_app.py",
    "relative_path": "codewiki/src/fe/web_app.py",
    "depends_on": [],
    "source_code": "def main():\n    \"\"\"Main function to run the web application.\"\"\"\n    import uvicorn\n\n    parser = argparse.ArgumentParser(\n        description=\"CodeWiki Web Application - Generate documentation for GitHub repositories\"\n    )\n    parser.add_argument(\n        \"--host\",\n        type=str,\n        default=WebAppConfig.DEFAULT_HOST,\n        help=f\"Host to bind the server to (default: {WebAppConfig.DEFAULT_HOST})\"\n    )\n    parser.add_argument(\n        \"--port\",\n        type=int,\n        default=WebAppConfig.DEFAULT_PORT,\n        help=f\"Port to run the server on (default: {WebAppConfig.DEFAULT_PORT})\"\n    )\n    parser.add_argument(\n        \"--debug\",\n        action=\"store_true\",\n        help=\"Run the server in debug mode\"\n    )\n    parser.add_argument(\n        \"--reload\",\n        action=\"store_true\",\n        help=\"Enable auto-reload for development\"\n    )\n\n    args = parser.parse_args()\n\n    # 设置日志配置 - 让日志信息显示在控制台\n    log_level = logging.DEBUG if args.debug else logging.INFO\n    logging.basicConfig(\n        level=log_level,\n        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n        handlers=[\n            logging.StreamHandler(),  # 输出到控制台\n        ]\n    )\n\n    print(f\"🔧 日志级别设置为: {logging.getLevelName(log_level)}\")\n\n    # Ensure required directories exist\n    WebAppConfig.ensure_directories()\n\n    # Start background worker\n    background_worker.start()\n\n    print(f\"🚀 CodeWiki Web Application starting...\")\n    print(f\"🌐 Server running at: http://{args.host}:{args.port}\")\n    print(f\"📁 Cache directory: {WebAppConfig.get_absolute_path(WebAppConfig.CACHE_DIR)}\")\n    print(f\"🗂️  Temp directory: {WebAppConfig.get_absolute_path(WebAppConfig.TEMP_DIR)}\")\n    print(\"\\nPress Ctrl+C to stop the server\")\n\n    try:\n        uvicorn.run(\n            \"fe.web_app:app\",\n            host=args.host,\n            port=args.port,\n            reload=args.reload,\n            log_level=\"debug\" if args.debug else \"info\"\n        )\n    except KeyboardInterrupt:\n        print(\"\\n👋 Server stopped\")\n        background_worker.stop()",
    "start_line": 76,
    "end_line": 142,
    "has_docstring": true,
    "docstring": "Main function to run the web application.",
    "parameters": [],
    "node_type": "function",
    "base_classes": null,
    "class_name": null,
    "display_name": "function main",
    "component_id": "codewiki.src.fe.web_app.main"
  },
  "codewiki.src.utils.FileManager": {
    "id": "codewiki.src.utils.FileManager",
    "name": "FileManager",
    "component_type": "class",
    "file_path": "/Users/wufagang/project/aiopen/CodeWiki/codewiki/src/utils.py",
    "relative_path": "codewiki/src/utils.py",
    "depends_on": [],
    "source_code": "class FileManager:\n    \"\"\"Handles file I/O operations.\"\"\"\n    \n    @staticmethod\n    def ensure_directory(path: str) -> None:\n        \"\"\"Create directory if it doesn't exist.\"\"\"\n        os.makedirs(path, exist_ok=True)\n    \n    @staticmethod\n    def save_json(data: Any, filepath: str) -> None:\n        \"\"\"Save data as JSON to file.\"\"\"\n        with open(filepath, 'w') as f:\n            json.dump(data, f, indent=4)\n    \n    @staticmethod\n    def load_json(filepath: str) -> Optional[Dict[str, Any]]:\n        \"\"\"Load JSON from file, return None if file doesn't exist.\"\"\"\n        if not os.path.exists(filepath):\n            return None\n        \n        with open(filepath, 'r') as f:\n            return json.load(f)\n    \n    @staticmethod\n    def save_text(content: str, filepath: str) -> None:\n        \"\"\"Save text content to file.\"\"\"\n        with open(filepath, 'w') as f:\n            f.write(content)\n    \n    @staticmethod\n    def load_text(filepath: str) -> str:\n        \"\"\"Load text content from file.\"\"\"\n        with open(filepath, 'r') as f:\n            return f.read()",
    "start_line": 10,
    "end_line": 43,
    "has_docstring": true,
    "docstring": "Handles file I/O operations.",
    "parameters": null,
    "node_type": "class",
    "base_classes": null,
    "class_name": null,
    "display_name": "class FileManager",
    "component_id": "codewiki.src.utils.FileManager"
  }
}